{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "full_data = [train_data, test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if we have some missing values in data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan in the data sets\n",
      "          Train Dataset  Test Dataset\n",
      "Age                 177          86.0\n",
      "Fare                  0           1.0\n",
      "Cabin               687         327.0\n",
      "Embarked              2           0.0\n"
     ]
    }
   ],
   "source": [
    "datasetHasNan = False\n",
    "if train_data.count().min() == train_data.shape[0] and test_data.count().min() == test_data.shape[0] :\n",
    "    pass\n",
    "else:\n",
    "    datasetHasNan = True\n",
    "    \n",
    "#Check for missing data & list them \n",
    "if datasetHasNan == True:\n",
    "    nas = pd.concat([train_data.isnull().sum(), test_data.isnull().sum()], axis=1, \n",
    "                    keys=['Train Dataset', 'Test Dataset'], sort=False) \n",
    "    print('Nan in the data sets')\n",
    "    print(nas[nas.sum(axis=1) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.742038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived\n",
       "Sex          \n",
       "0    0.188908\n",
       "1    0.742038"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in full_data:\n",
    "    data['Sex'] = data['Sex'].map( {'female': 1, 'male': 0} )\n",
    "    \n",
    "sex_pivot = train_data.pivot_table(index='Sex',values='Survived')\n",
    "sex_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.472826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Survived\n",
       "Pclass          \n",
       "1       0.629630\n",
       "2       0.472826\n",
       "3       0.242363"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pclass_pivot = train_data.pivot_table(index='Pclass',values='Survived')\n",
    "pclass_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SibSp, Parch columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sibsp_chart = train_data.pivot_table(index='SibSp',values='Survived')\n",
    "sibsp_chart.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parch_chart = train_data.pivot_table(index='Parch',values='Survived')\n",
    "parch_chart.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in full_data:\n",
    "    data['FamilySize'] = data['Parch'] + data['SibSp'] + 1\n",
    "\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for data in full_data:\n",
    "    data['IsAlone'] = 0\n",
    "    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "family_chart = train_data.pivot_table(index='FamilySize',values='Survived')\n",
    "family_chart.plot.bar()\n",
    "plt.show()\n",
    "\n",
    "for data in full_data:\n",
    "    data['FamilySizeGroup'] = 'Small'\n",
    "    data.loc[data['FamilySize'] == 1, 'FamilySizeGroup'] = 'Alone'\n",
    "    data.loc[data['FamilySize'] >= 5, 'FamilySizeGroup'] = 'Big'\n",
    "\n",
    "for data in full_data:\n",
    "    data['FamilySizeGroup'] = data['FamilySizeGroup'].map({'Small': 0, 'Alone': 1, 'Big': 2})\n",
    "\n",
    "famSize_cat_pivot = train_data.pivot_table(index='FamilySizeGroup',values='Survived')\n",
    "famSize_cat_pivot.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embarked column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     889\n",
       "unique      3\n",
       "top         S\n",
       "freq      644\n",
       "Name: Embarked, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Embarked'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Embarked'].fillna('S', inplace=True)\n",
    "test_data['Embarked'].fillna('S', inplace=True)\n",
    "\n",
    "for data in full_data:\n",
    "    data['Embarked'] = data['Embarked'].map( {'S': 1, 'Q': 2, 'C': 3} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embarked_chart = train_data.pivot_table(index='Embarked',values='Survived')\n",
    "embarked_chart.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    714.000000\n",
       "mean      29.699118\n",
       "std       14.526497\n",
       "min        0.420000\n",
       "25%       20.125000\n",
       "50%       28.000000\n",
       "75%       38.000000\n",
       "max       80.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.shape[0])\n",
    "train_data['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "survived = train_data[train_data['Survived'] == 1]\n",
    "died = train_data[train_data['Survived'] == 0]\n",
    "survived['Age'].plot.hist(alpha=0.5,color='red',bins=50)\n",
    "died['Age'].plot.hist(alpha=0.5,color='blue',bins=50)\n",
    "plt.legend(['Survived','Died'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.364948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.358871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived\n",
       "Age          \n",
       "0    0.576923\n",
       "1    0.364948\n",
       "2    0.358871\n",
       "3    0.434783\n",
       "4    0.090909"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_random_ages = np.random.randint(train_data['Age'].mean() - train_data['Age'].std(),\n",
    "                                          train_data['Age'].mean() + train_data['Age'].std(),\n",
    "                                          size = train_data['Age'].isnull().sum())\n",
    "\n",
    "test_random_ages = np.random.randint(test_data['Age'].mean() - test_data['Age'].std(),\n",
    "                                          test_data['Age'].mean() + test_data['Age'].std(),\n",
    "                                          size = test_data['Age'].isnull().sum())\n",
    "\n",
    "train_data['Age'][np.isnan(train_data['Age'])] = train_random_ages\n",
    "test_data['Age'][np.isnan(test_data['Age'])] = test_random_ages\n",
    "train_data['Age'] = train_data['Age'].astype(int)\n",
    "test_data['Age'] = test_data['Age'].astype(int)\n",
    "\n",
    "for data in full_data:    \n",
    "    data.loc[ data['Age'] <= 14, 'Age'] = 0\n",
    "    data.loc[(data['Age'] > 14) & (data['Age'] <= 32), 'Age'] = 1\n",
    "    data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n",
    "    data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n",
    "    data.loc[ data['Age'] > 64, 'Age'] = 4\n",
    "\n",
    "pivot_ages = train_data.pivot_table(index = 'Age', values = 'Survived')\n",
    "pivot_ages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>FamilySizeGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0    1      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1    2      1      0   \n",
       "2                             Heikkinen, Miss. Laina    1    1      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1    2      1      0   \n",
       "4                           Allen, Mr. William Henry    0    2      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin  Embarked  FamilySize  IsAlone  \\\n",
       "0         A/5 21171   7.2500   NaN         1           2        0   \n",
       "1          PC 17599  71.2833   C85         3           2        0   \n",
       "2  STON/O2. 3101282   7.9250   NaN         1           1        1   \n",
       "3            113803  53.1000  C123         1           2        0   \n",
       "4            373450   8.0500   NaN         1           1        1   \n",
       "\n",
       "   FamilySizeGroup  \n",
       "0                0  \n",
       "1                0  \n",
       "2                1  \n",
       "3                0  \n",
       "4                1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    891.000000\n",
       "mean      32.204208\n",
       "std       49.693429\n",
       "min        0.000000\n",
       "25%        7.910400\n",
       "50%       14.454200\n",
       "75%       31.000000\n",
       "max      512.329200\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Fare'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGApJREFUeJzt3X+QVfWZ5/H3I7CCYqJCkxjQNE7U1VkRtUVTTrIdMqPGcjDZiKBTShlmSPyRwCaVHU3GCakKqUyKxKjZciUTS81qEJdkJK470TgSa5JMhFaCKLrij9UOlCBJMCIq6LN/3NM9VzzQt5u+fW93v19Vt+4533vOvc+3vd4P55zvOScyE0mSdrdfowuQJDUnA0KSVMqAkCSVMiAkSaUMCElSKQNCklTKgJAklTIgJEmlDAhJUqmRjS5gX4wfPz5bW1sbXYYkDSodHR0vZWZLT8sN6oBobW1l9erVjS5DkgaViPh/tSznLiZJUikDQpJUyoCQJJUa1McgJA19O3fupLOzk9dee63RpQw6o0ePZtKkSYwaNapP6xsQkppaZ2cnBx10EK2trUREo8sZNDKTrVu30tnZyeTJk/v0Hu5iktTUXnvtNcaNG2c49FJEMG7cuH3a8jIgJDU9w6Fv9vXvZkBIkkp5DELS4LJwYUPeb9GiRdx+++2MGDGC/fbbjxtvvJFTTz11nz56xYoVPP7441x55ZX79D4AY8eO5ZVXXtnn96lmQBSqvyP9/f2TNLj96le/4u677+bhhx9m//3356WXXuKNN96oad1du3YxcmT5T+2MGTOYMWNGf5bar9zFJEk92LRpE+PHj2f//fcHYPz48bzvfe+jtbWVl156CYDVq1fT3t4OwMKFC5k3bx5nnHEGF198MaeeeiqPPfZY9/u1t7fT0dHBzTffzBVXXMG2bdtobW3lrbfeAuDVV1/l8MMPZ+fOnTz99NOcddZZnHzyyXzoQx/iiSeeAODZZ5/lgx/8IKeccgpXX311XfptQEhSD8444wxeeOEFjj76aC677DJ+/vOf97hOR0cHd911F7fffjuzZ89m2bJlQCVsNm7cyMknn9y97Lvf/W5OOOGE7vf9yU9+wplnnsmoUaOYN28e119/PR0dHSxevJjLLrsMgPnz53PppZeyatUq3vve99ah1waEJPVo7NixdHR0sGTJElpaWpg1axY333zzXteZMWMGY8aMAeD888/nzjvvBGDZsmXMnDnzHcvPmjWLO+64A4ClS5cya9YsXnnlFX75y18yc+ZMpk6dyqc//Wk2bdoEwC9+8QsuuOACAC666KL+6urbeAxCkmowYsQI2tvbaW9v5/jjj+eWW25h5MiR3buFdj/f4MADD+yenjhxIuPGjWPt2rXccccd3Hjjje94/xkzZnDVVVfxu9/9jo6ODqZPn8727ds5+OCDWbNmTWlN9R7+6xaEJPXgySef5KmnnuqeX7NmDe9///tpbW2lo6MDgOXLl+/1PWbPns03v/lNtm3bxvHHH/+O18eOHcu0adOYP38+55xzDiNGjOBd73oXkydP7t76yEx+85vfAHD66aezdOlSAG677bZ+6efu3IKQNLg0YJjhK6+8wmc/+1n+8Ic/MHLkSD7wgQ+wZMkS1q9fz9y5c/n617/e45DX8847j/nz5+/1gPKsWbOYOXMmK1eu7G677bbbuPTSS/na177Gzp07mT17NieccALXXnstF154Iddeey2f/OQn+6urbxOZWZc3HghtbW3ZXzcMcpir1JzWr1/Pscce2+gyBq2yv19EdGRmW0/ruotJklTKgJAklTIgJEmlDAhJUikDQpJUqm4BERGHR8QDEbE+Ih6LiPlF+8KI+G1ErCkeZ1etc1VEbIiIJyPizHrVJknqWT3Pg9gFfCEzH46Ig4COiLiveO2azFxcvXBEHAfMBv4UeB/ws4g4OjPfrGONkgaZRlzte8SIERx//PHs3LmTkSNHMmfOHBYsWMB+++3H6tWrufXWW7nuuutq/sz29nYWL15MW1uPI00bqm4BkZmbgE3F9B8jYj0wcS+rnAsszczXgWcjYgMwDfhVvWqUpFqMGTOm+3IXmzdv5sILL2Tbtm189atfpa2trel/6PtqQI5BREQrcCLw66LpiohYGxE3RcQhRdtE4IWq1TopCZSImBcRqyNi9ZYtW+pYtSS904QJE1iyZAnf/e53yUxWrlzJOeecA8D27dv51Kc+xSmnnMKJJ57IXXfdBcCOHTuYPXs2U6ZMYdasWezYsaORXahZ3QMiIsYCy4EFmfkycAPwJ8BUKlsY3+patGT1d5zmnZlLMrMtM9taWlrqVLUk7dmRRx7JW2+9xebNm9/WvmjRIqZPn86qVat44IEH+OIXv8j27du54YYbOOCAA1i7di1f/vKXu6/f1Ozqei2miBhFJRxuy8wfAWTmi1Wvfw+4u5jtBA6vWn0SsLGe9UlSX5Vdpujee+9lxYoVLF5cOcT62muv8fzzz/Pggw/yuc99DoApU6YwZcqUAa21r+oWEFG5Du33gfWZ+e2q9sOK4xMAnwDWFdMrgNsj4ttUDlIfBTxUr/okqa+eeeYZRowYwYQJE1i/fn13e2ayfPlyjjnmmHesU+9Lc9dDPXcxnQ5cBEzfbUjrNyPi0YhYC3wE+K8AmfkYsAx4HPhn4HJHMElqNlu2bOEzn/kMV1xxxTt+9M8880yuv/767q2LRx55BIAPf/jD3ZfkXrduHWvXrh3YovuonqOY/pXy4wr37GWdRcCietUkafBrxNWWd+zYwdSpU7uHuV500UV8/vOff8dyV199NQsWLGDKlClkJq2trdx9991ceumlXHLJJUyZMoWpU6cybdq0ge9EH3g/CEnqwZtv7nlnRtdd5qAyHLbsbnFjxozpvrnPYOKlNiRJpQwISVIpA0JS0xvMd75spH39uxkQkpra6NGj2bp1qyHRS5nJ1q1bGT16dJ/fw4PUkprapEmT6OzsxEvr9N7o0aOZNGlSn9c3ICQ1tVGjRjF58uRGlzEsuYtJklTKgJAklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSKQNCklTKgJAklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVqltARMThEfFARKyPiMciYn7RfmhE3BcRTxXPhxTtERHXRcSGiFgbESfVqzZJUs/quQWxC/hCZh4LnAZcHhHHAVcC92fmUcD9xTzAx4Cjisc84IY61iZJ6kHdAiIzN2Xmw8X0H4H1wETgXOCWYrFbgI8X0+cCt2bFvwEHR8Rh9apPkrR3A3IMIiJagROBXwPvycxNUAkRYEKx2ETgharVOos2SVID1D0gImIssBxYkJkv723RkrYseb95EbE6IlZv2bKlv8qUJO2mrgEREaOohMNtmfmjovnFrl1HxfPmor0TOLxq9UnAxt3fMzOXZGZbZra1tLTUr3hJGubqOYopgO8D6zPz21UvrQDmFNNzgLuq2i8uRjOdBmzr2hUlSRp4I+v43qcDFwGPRsSaou1LwDeAZRExF3gemFm8dg9wNrABeBW4pI61SZJ6ULeAyMx/pfy4AsBHS5ZP4PJ61SNJ6h3PpJYklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSKQNCklTKgJAklTIgJEmlagqIiPhP9S5EktRcat2C+B8R8VBEXBYRB9e1IklSU6gpIDLzz4C/onLP6NURcXtE/EVdK5MkNVTNxyAy8yng74C/Bf4zcF1EPBER/6VexUmSGqfWYxBTIuIaYD0wHfjLzDy2mL6mjvVJkhqk1ntSfxf4HvClzNzR1ZiZGyPi7+pSmSSpoWoNiLOBHZn5JkBE7AeMzsxXM/MHdatOktQwtR6D+Bkwpmr+gKJNkjRE1RoQozPzla6ZYvqA+pQkSWoGtQbE9og4qWsmIk4GduxleUnSIFfrMYgFwJ0RsbGYPwyYVZ+SJEnNoKaAyMxVEfEfgWOAAJ7IzJ11rUyS1FC1bkEAnAK0FuucGBFk5q11qUqS1HA1BURE/AD4E2AN8GbRnIABIUlDVK1bEG3AcZmZ9SxGktQ8ah3FtA54b2/eOCJuiojNEbGuqm1hRPw2ItYUj7OrXrsqIjZExJMRcWZvPkuS1P9q3YIYDzweEQ8Br3c1ZuaMvaxzM5VLdOy+G+qazFxc3RARxwGzgT8F3gf8LCKO7jpzW5I08GoNiIW9fePMfDAiWmtc/FxgaWa+DjwbERuAacCvevu5kqT+Uev9IH4OPAeMKqZXAQ/38TOviIi1xS6oQ4q2icALVct0Fm2SpAap9XLffwP8L+DGomki8E99+LwbqIyGmgpsAr7V9REly5YeEI+IeRGxOiJWb9mypQ8lSJJqUetB6suB04GXofvmQRN6+2GZ+WJmvpmZb1G5fPi04qVOKner6zIJ2Lj7+sV7LMnMtsxsa2lp6W0JkqQa1RoQr2fmG10zETGSPfwLf28i4rCq2U9QGR0FsAKYHRH7R8Rk4Cjgod6+vySp/9R6kPrnEfElYExxL+rLgJ/sbYWI+CHQDoyPiE7gK0B7REylEi7PAZ8GyMzHImIZ8DiwC7jcEUyS1Fi1BsSVwFzgUSo/6vcA/7i3FTLzgpLm7+9l+UXAohrrkSTVWa0X6+s6ZvC9+pYjSWoWtV6L6VlKjjlk5pH9XpEkqSn05lpMXUYDM4FD+78cSVKzqPVEua1Vj99m5neA6XWuTZLUQLXuYjqpanY/KlsUB9WlIklSU6h1F9O3qqZ3URmien6/VyNJahq1jmL6SL0LkSQ1l1p3MX1+b69n5rf7pxxJUrPozSimU6hcEgPgL4EHefsVWCVJQ0hvbhh0Umb+ESp3hgPuzMy/rldhkqTGqvVifUcAb1TNvwG09ns1kqSmUesWxA+AhyLix1TOqP4E77yVqCRpCKl1FNOiiPg/wIeKpksy85H6lSVJarRadzEBHAC8nJnXAp3FfRskSUNUrbcc/Qrwt8BVRdMo4H/WqyhJUuPVugXxCWAGsB0gMzfipTYkaUirNSDeyMykuOR3RBxYv5IkSc2g1oBYFhE3AgdHxN8AP8ObB0nSkFbrKKbFxb2oXwaOAf4+M++ra2WSpIbqMSAiYgTw08z8c8BQkKRhosddTJn5JvBqRLx7AOqRJDWJWs+kfg14NCLuoxjJBJCZn6tLVZKkhqs1IP538ZAkDRN7DYiIOCIzn8/MWwaqIElSc+jpGMQ/dU1ExPI61yJJaiI9BURUTR9Zz0IkSc2lp4DIPUxLkoa4ng5SnxARL1PZkhhTTFPMZ2a+q67VSZIaZq8BkZkjBqoQSVJz6c39IHolIm6KiM0Rsa6q7dCIuC8iniqeDynaIyKui4gNEbE2Ik6qV12SpNrULSCAm4Gzdmu7Erg/M48C7i/mAT4GHFU85gE31LEuSVIN6hYQmfkg8Lvdms8Fus6puAX4eFX7rVnxb1SuGntYvWqTJPWsnlsQZd6TmZsAiucJRftE4IWq5TqLNklSgwx0QOxJlLSVDquNiHkRsToiVm/ZsqXOZUnS8DXQAfFi166j4nlz0d4JHF613CRgY9kbZOaSzGzLzLaWlpa6FitJw9lAB8QKYE4xPQe4q6r94mI002nAtq5dUZKkxqj1aq69FhE/BNqB8RHRCXwF+AaV25fOBZ4HZhaL3wOcDWwAXgUuqVddkqTa1C0gMvOCPbz00ZJlE7i8XrVIknqvWQ5SS5KajAEhSSplQEiSShkQkqRSBoQkqZQBIUkqZUBIkkoZEJKkUgaEJKmUASFJKmVASJJKGRCSpFJ1u1jfoLZwYe/aJWkIcgtCklTKgJAklTIgJEmlDAhJUikDQpJUyoCQJJUyICRJpYbveRC7n9Owsv3fX6Jqun3lntfpqV2SBjG3ICRJpQwISVIpA0KSVMqAkCSVMiAkSaUMCElSKQNCklTKgJAklWrIiXIR8RzwR+BNYFdmtkXEocAdQCvwHHB+Zv6+EfVJkhq7BfGRzJyamW3F/JXA/Zl5FHB/MS9JapBm2sV0LnBLMX0L8PEG1iJJw16jAiKBeyOiIyLmFW3vycxNAMXzhAbVJkmicRfrOz0zN0bEBOC+iHii1hWLQJkHcMQRR9SrPkka9hqyBZGZG4vnzcCPgWnAixFxGEDxvHkP6y7JzLbMbGtpaRmokiVp2BnwgIiIAyPioK5p4AxgHbACmFMsNge4a6BrkyT9u0bsYnoP8OOI6Pr82zPznyNiFbAsIuYCzwMzG1CbJKkw4AGRmc8AJ5S0bwU+OtD1SJLKDd87yu2jhW+7A50kDT3NdB6EJKmJuAXRC9VbDZI01LkFIUkqZUBIkkoZEJKkUgaEJKmUASFJKmVASJJKGRCSpFIGhCSplAEhSSrlmdSNsHBh79olqQEMiB54eQ1Jw5W7mCRJpQwISVIpA0KSVMqAkCSVMiAkSaUcxdQf6j1s1WGxkhrALQhJUqlhuwUxJM5vcMtCUh25BSFJKjVstyAGg+4NgZXtLGxf2cBKJA1HBkQ/2NPuqoUDWoUk9S93MUmSShkQkqRS7mKqoz0OJtrTMYXdVxgKI60kDVoGhPo2XLa3Q2kdeisNOk0XEBFxFnAtMAL4x8z8RoNLqovqA9u9HaHU53X9kZbUC00VEBExAvjvwF8AncCqiFiRmY83trI+Wrmy0RWokTyRUYNcUwUEMA3YkJnPAETEUuBcYHAGRI1q2SLo1ZnfPfwA7XFYbj+ea1HzVk6zXceqn+vZly1FCWjoPzSaLSAmAi9UzXcCpzaolobY10uADMQP0sKFdB9Ar+UzFq5s7z4ppJH/eH7b33bh22upPinxbevU0r+qZQwBDSWRmY2uoVtEzATOzMy/LuYvAqZl5merlpkHzCtmjwGe7OPHjQde2odyB5vh1N/h1FcYXv21r/3j/ZnZ0tNCzbYF0QkcXjU/CdhYvUBmLgGW7OsHRcTqzGzb1/cZLIZTf4dTX2F49de+DqxmO1FuFXBUREyOiP8AzAZWNLgmSRqWmmoLIjN3RcQVwE+pDHO9KTMfa3BZkjQsNVVAAGTmPcA9A/BR+7ybapAZTv0dTn2F4dVf+zqAmuogtSSpeTTbMQhJUpMYlgEREWdFxJMRsSEirmx0Pf0hIm6KiM0Rsa6q7dCIuC8iniqeDynaIyKuK/q/NiJOalzlvRcRh0fEAxGxPiIei4j5RfuQ629EjI6IhyLiN0Vfv1q0T46IXxd9vaMY1EFE7F/Mbyheb21k/X0RESMi4pGIuLuYH8p9fS4iHo2INRGxumhrmu/xsAuIqst5fAw4DrggIo5rbFX94mbgrN3argTuz8yjgPuLeaj0/ajiMQ+4YYBq7C+7gC9k5rHAacDlxX/Dodjf14HpmXkCMBU4KyJOA/4BuKbo6++BucXyc4HfZ+YHgGuK5Qab+cD6qvmh3FeAj2Tm1Kohrc3zPc7MYfUAPgj8tGr+KuCqRtfVT31rBdZVzT8JHFZMHwY8WUzfCFxQttxgfAB3Ubl+15DuL3AA8DCVqwu8BIws2ru/01RGAH6wmB5ZLBeNrr0XfZxE5UdxOnA3EEO1r0XdzwHjd2trmu/xsNuCoPxyHhMbVEu9vSczNwEUzxOK9iHzNyh2K5wI/Joh2t9il8saYDNwH/A08IfM3FUsUt2f7r4Wr28Dxg1sxfvkO8B/A94q5scxdPsKkMC9EdFRXCUCmuh73HTDXAdAlLQNt6FcQ+JvEBFjgeXAgsx8OaKsW5VFS9oGTX8z801gakQcDPwYOLZsseJ50PY1Is4BNmdmR0S0dzWXLDro+1rl9MzcGBETgPsi4om9LDvg/R2OWxA9Xs5jCHkxIg4DKJ43F+2D/m8QEaOohMNtmfmjonnI9hcgM/8ArKRy3OXgiOj6B151f7r7Wrz+buB3A1tpn50OzIiI54ClVHYzfYeh2VcAMnNj8byZSvhPo4m+x8MxIIbT5TxWAHOK6TlU9tV3tV9cjIo4DdjWtUk7GERlU+H7wPrM/HbVS0OuvxHRUmw5EBFjgD+ncgD3AeC8YrHd+9r1NzgP+Jcsdlg3u8y8KjMnZWYrlf8v/yUz/4oh2FeAiDgwIg7qmgbOANbRTN/jRh+kadCBobOB/0tlX+6XG11PP/Xph8AmYCeVf2nMpbI/9n7gqeL50GLZoDKS62ngUaCt0fX3sq9/RmXTei2wpnicPRT7C0wBHin6ug74+6L9SOAhYANwJ7B/0T66mN9QvH5ko/vQx363A3cP5b4W/fpN8Xis67eomb7HnkktSSo1HHcxSZJqYEBIkkoZEJKkUgaEJKmUASFJKmVASJJKGRCSpFIGhCSp1P8H7SS+Ex9lJn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "survived = train_data[train_data['Survived'] == 1]\n",
    "died = train_data[train_data['Survived'] == 0]\n",
    "survived['Fare'].plot.hist(alpha=0.5,color='red',bins=50)\n",
    "died['Fare'].plot.hist(alpha=0.5,color='blue',bins=50)\n",
    "plt.legend(['Survived','Died'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.197309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.308756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.597156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Survived\n",
       "Fare          \n",
       "0.0   0.197309\n",
       "1.0   0.308756\n",
       "2.0   0.437500\n",
       "3.0   0.597156"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in full_data:\n",
    "    data.loc[ data['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 32), 'Fare']   = 2\n",
    "    data.loc[ data['Fare'] > 32, 'Fare'] = 3\n",
    "#     data['Fare'] = data['Fare'].astype(int)\n",
    "\n",
    "pivot_fare = train_data.pivot_table(index = 'Fare', values = 'Survived')\n",
    "pivot_fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Title from Name column\n",
    "Do not forget about title Donna that is without . and do not fit the pattern bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different titles found on the dataset:\n",
      "17 : ['Capt', 'Col', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Master', 'Miss', 'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms', 'Rev', 'Sir', 'the Countess']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data = [train_data, test_data]\n",
    "\n",
    "for data in full_data:\n",
    "    data['Title'] = 'Missing'\n",
    "    \n",
    "def get_title(name):\n",
    "    if '.' in name:\n",
    "        return name.split(',')[1].split('.')[0].strip()\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "titles = sorted(set([x for x in train_data.Name.map(lambda x: get_title(x))]))\n",
    "print('Different titles found on the dataset:')\n",
    "print(len(titles), ':', titles)\n",
    "print()\n",
    "\n",
    "def replace_titles(x):\n",
    "    title = x['Title']\n",
    "    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n",
    "        return 'Mr'\n",
    "    elif title in ['the Countess', 'Mme', 'Lady', 'Dona']:\n",
    "        return 'Mrs'\n",
    "    elif title in ['Mlle', 'Ms']:\n",
    "        return 'Miss'\n",
    "    elif title =='Dr':\n",
    "        if x['Sex']=='male':\n",
    "            return 'Mr'\n",
    "        else:\n",
    "            return 'Mrs'\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "train_data['Title'] = train_data['Name'].map(lambda x: get_title(x))\n",
    "train_data['Title'] = train_data.apply(replace_titles, axis=1)\n",
    "\n",
    "test_data['Title'] = test_data['Name'].map(lambda x: get_title(x))\n",
    "test_data['Title'] = test_data.apply(replace_titles, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.158192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived\n",
       "Title          \n",
       "1      0.575000\n",
       "2      0.702703\n",
       "3      0.158192\n",
       "4      0.777778"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Title'] = train_data['Title'].map({'Master': 1, 'Miss': 2, 'Mr': 3, 'Mrs': 4})\n",
    "test_data['Title'] = test_data['Title'].map({'Master': 1, 'Miss': 2, 'Mr': 3, 'Mrs': 4})\n",
    "\n",
    "title_cat_pivot = train_data.pivot_table(index='Title',values='Survived')\n",
    "title_cat_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.691358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Survived\n",
       "Cabin          \n",
       "0      0.299854\n",
       "1      0.588235\n",
       "2      0.691358\n",
       "3      0.660377"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in full_data:\n",
    "    # classify Cabin by fare\n",
    "    data['Cabin'] = data['Cabin'].fillna('X')\n",
    "    data['Cabin'] = data['Cabin'].apply(lambda x: str(x)[0])\n",
    "    data['Cabin'] = data['Cabin'].replace(['A', 'D', 'E', 'T'], 'M')\n",
    "    data['Cabin'] = data['Cabin'].replace(['B', 'C'], 'H')\n",
    "    data['Cabin'] = data['Cabin'].replace(['F', 'G'], 'L')\n",
    "    data['Cabin'] = data['Cabin'].map({'X': 0, 'L': 1, 'M': 2, 'H': 3}).astype(int) \n",
    "\n",
    "cabin_pivot = train_data.pivot_table(index = 'Cabin', values = 'Survived')\n",
    "cabin_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>FamilySizeGroup</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name  Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    0   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)    1   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    0   \n",
       "3          895       3                              Wirz, Mr. Albert    0   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)    1   \n",
       "\n",
       "   Age  SibSp  Parch   Ticket  Fare  Cabin  Embarked  FamilySize  IsAlone  \\\n",
       "0    2      0      0   330911   0.0      0         2           1        1   \n",
       "1    2      1      0   363272   0.0      0         1           2        0   \n",
       "2    3      0      0   240276   1.0      0         2           1        1   \n",
       "3    1      0      0   315154   1.0      0         1           1        1   \n",
       "4    1      1      1  3101298   1.0      0         1           3        0   \n",
       "\n",
       "   FamilySizeGroup  Title  \n",
       "0                1      3  \n",
       "1                0      4  \n",
       "2                1      3  \n",
       "3                1      3  \n",
       "4                0      4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>FamilySizeGroup</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0    1      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1    2      1      0   \n",
       "2                             Heikkinen, Miss. Laina    1    1      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1    2      1      0   \n",
       "4                           Allen, Mr. William Henry    0    2      0      0   \n",
       "\n",
       "             Ticket  Fare  Cabin  Embarked  FamilySize  IsAlone  \\\n",
       "0         A/5 21171   0.0      0         1           2        0   \n",
       "1          PC 17599   3.0      3         3           2        0   \n",
       "2  STON/O2. 3101282   1.0      0         1           1        1   \n",
       "3            113803   3.0      3         1           2        0   \n",
       "4            373450   1.0      0         1           1        1   \n",
       "\n",
       "   FamilySizeGroup  Title  \n",
       "0                0      3  \n",
       "1                0      4  \n",
       "2                1      2  \n",
       "3                0      4  \n",
       "4                1      3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save PassengerId for evaluation\n",
    "train_passenger_id = train_data['PassengerId']\n",
    "test_passenger_id=test_data['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>FamilySizeGroup</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex  Age  Fare  Cabin  Embarked  IsAlone  FamilySizeGroup  Title\n",
       "0       3    0    2   0.0      0         2        1                1      3\n",
       "1       3    1    2   0.0      0         1        0                0      4\n",
       "2       2    0    3   1.0      0         2        1                1      3\n",
       "3       3    0    1   1.0      0         1        1                1      3\n",
       "4       3    1    1   1.0      0         1        0                0      4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_not_concerned(data, columns):\n",
    "    return data.drop(columns, axis = 1)\n",
    "\n",
    "not_concerned_columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'FamilySize']\n",
    "train_data = drop_not_concerned(train_data, not_concerned_columns)\n",
    "test_data = drop_not_concerned(test_data, not_concerned_columns)\n",
    "\n",
    "train_data.head()\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make additional columns to indicate the category for each current column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_date(data, columns):\n",
    "    for column in columns:\n",
    "        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n",
    "        data = data.drop(column, axis=1)\n",
    "    return data\n",
    "\n",
    "dummy_columns = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin', 'Embarked', 'IsAlone', 'FamilySizeGroup', 'Title']\n",
    "\n",
    "train_data = dummy_date(train_data, dummy_columns)\n",
    "test_data = dummy_date(test_data, dummy_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_0</th>\n",
       "      <th>Sex_1</th>\n",
       "      <th>Age_0</th>\n",
       "      <th>Age_1</th>\n",
       "      <th>Age_2</th>\n",
       "      <th>Age_3</th>\n",
       "      <th>Age_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Embarked_3</th>\n",
       "      <th>IsAlone_0</th>\n",
       "      <th>IsAlone_1</th>\n",
       "      <th>FamilySizeGroup_0</th>\n",
       "      <th>FamilySizeGroup_1</th>\n",
       "      <th>FamilySizeGroup_2</th>\n",
       "      <th>Title_1</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Title_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass_1  Pclass_2  Pclass_3  Sex_0  Sex_1  Age_0  Age_1  Age_2  Age_3  \\\n",
       "0         0         0         1      1      0      0      0      1      0   \n",
       "1         0         0         1      0      1      0      0      1      0   \n",
       "2         0         1         0      1      0      0      0      0      1   \n",
       "3         0         0         1      1      0      0      1      0      0   \n",
       "4         0         0         1      0      1      0      1      0      0   \n",
       "\n",
       "   Age_4  ...  Embarked_3  IsAlone_0  IsAlone_1  FamilySizeGroup_0  \\\n",
       "0      0  ...           0          0          1                  0   \n",
       "1      0  ...           0          1          0                  1   \n",
       "2      0  ...           0          0          1                  0   \n",
       "3      0  ...           0          0          1                  0   \n",
       "4      0  ...           0          1          0                  1   \n",
       "\n",
       "   FamilySizeGroup_1  FamilySizeGroup_2  Title_1  Title_2  Title_3  Title_4  \n",
       "0                  1                  0        0        0        1        0  \n",
       "1                  0                  0        0        0        0        1  \n",
       "2                  1                  0        0        0        1        0  \n",
       "3                  1                  0        0        0        1        0  \n",
       "4                  0                  0        0        0        0        1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_0</th>\n",
       "      <th>Sex_1</th>\n",
       "      <th>Age_0</th>\n",
       "      <th>Age_1</th>\n",
       "      <th>Age_2</th>\n",
       "      <th>Age_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Embarked_3</th>\n",
       "      <th>IsAlone_0</th>\n",
       "      <th>IsAlone_1</th>\n",
       "      <th>FamilySizeGroup_0</th>\n",
       "      <th>FamilySizeGroup_1</th>\n",
       "      <th>FamilySizeGroup_2</th>\n",
       "      <th>Title_1</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Title_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass_1  Pclass_2  Pclass_3  Sex_0  Sex_1  Age_0  Age_1  Age_2  \\\n",
       "0         0         0         0         1      1      0      0      1      0   \n",
       "1         1         1         0         0      0      1      0      0      1   \n",
       "2         1         0         0         1      0      1      0      1      0   \n",
       "3         1         1         0         0      0      1      0      0      1   \n",
       "4         0         0         0         1      1      0      0      0      1   \n",
       "\n",
       "   Age_3  ...  Embarked_3  IsAlone_0  IsAlone_1  FamilySizeGroup_0  \\\n",
       "0      0  ...           0          1          0                  1   \n",
       "1      0  ...           1          1          0                  1   \n",
       "2      0  ...           0          0          1                  0   \n",
       "3      0  ...           0          1          0                  1   \n",
       "4      0  ...           0          0          1                  0   \n",
       "\n",
       "   FamilySizeGroup_1  FamilySizeGroup_2  Title_1  Title_2  Title_3  Title_4  \n",
       "0                  0                  0        0        0        1        0  \n",
       "1                  0                  0        0        0        0        1  \n",
       "2                  1                  0        0        1        0        0  \n",
       "3                  0                  0        0        0        0        1  \n",
       "4                  1                  0        0        0        1        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_valid_test_data(data, fraction=(0.2)):\n",
    "    data_y = data[\"Survived\"]\n",
    "    lb = LabelBinarizer()\n",
    "    data_y = lb.fit_transform(data_y)\n",
    "    data_x = data.drop([\"Survived\"], axis=1)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=fraction)\n",
    "    return train_x, train_y, valid_x, valid_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just to see what it does\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit_transform(['yes', 'no', 'no', 'yes', 'maybe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x:(712, 30)\n",
      "train_y:(712, 1)\n",
      "valid_x:(179, 30)\n",
      "valid_y:(179, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = split_valid_test_data(train_data)\n",
    "print(\"train_x:{}\".format(train_x.shape))\n",
    "print(\"train_y:{}\".format(train_y.shape))\n",
    "\n",
    "print(\"valid_x:{}\".format(valid_x.shape))\n",
    "print(\"valid_y:{}\".format(valid_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data_x,data_y,batch_size=32):\n",
    "    batch_n=len(data_x)//batch_size\n",
    "    for i in range(batch_n):\n",
    "        batch_x=data_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y=data_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        yield batch_x,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/2000 Train Loss: 0.8470 Train Acc: 0.3652\n",
      "Epoch: 3/2000 Validation Loss: 0.7989 Validation Acc: 0.3520\n",
      "Epoch: 5/2000 Train Loss: 0.8442 Train Acc: 0.3652\n",
      "Epoch: 5/2000 Validation Loss: 0.7975 Validation Acc: 0.3520\n",
      "Epoch: 7/2000 Train Loss: 0.8414 Train Acc: 0.3652\n",
      "Epoch: 7/2000 Validation Loss: 0.7962 Validation Acc: 0.3520\n",
      "Epoch: 10/2000 Train Loss: 0.8386 Train Acc: 0.3652\n",
      "Epoch: 10/2000 Validation Loss: 0.7949 Validation Acc: 0.3520\n",
      "Epoch: 12/2000 Train Loss: 0.8359 Train Acc: 0.3750\n",
      "Epoch: 12/2000 Validation Loss: 0.7936 Validation Acc: 0.3520\n",
      "Epoch: 14/2000 Train Loss: 0.8332 Train Acc: 0.3750\n",
      "Epoch: 14/2000 Validation Loss: 0.7923 Validation Acc: 0.3520\n",
      "Epoch: 16/2000 Train Loss: 0.8306 Train Acc: 0.3750\n",
      "Epoch: 16/2000 Validation Loss: 0.7910 Validation Acc: 0.3520\n",
      "Epoch: 19/2000 Train Loss: 0.8279 Train Acc: 0.3736\n",
      "Epoch: 19/2000 Validation Loss: 0.7897 Validation Acc: 0.3520\n",
      "Epoch: 21/2000 Train Loss: 0.8253 Train Acc: 0.3736\n",
      "Epoch: 21/2000 Validation Loss: 0.7885 Validation Acc: 0.3520\n",
      "Epoch: 23/2000 Train Loss: 0.8228 Train Acc: 0.3736\n",
      "Epoch: 23/2000 Validation Loss: 0.7873 Validation Acc: 0.3520\n",
      "Epoch: 25/2000 Train Loss: 0.8202 Train Acc: 0.3736\n",
      "Epoch: 25/2000 Validation Loss: 0.7860 Validation Acc: 0.3520\n",
      "Epoch: 28/2000 Train Loss: 0.8177 Train Acc: 0.3722\n",
      "Epoch: 28/2000 Validation Loss: 0.7848 Validation Acc: 0.3520\n",
      "Epoch: 30/2000 Train Loss: 0.8152 Train Acc: 0.3722\n",
      "Epoch: 30/2000 Validation Loss: 0.7836 Validation Acc: 0.3520\n",
      "Epoch: 32/2000 Train Loss: 0.8127 Train Acc: 0.3722\n",
      "Epoch: 32/2000 Validation Loss: 0.7824 Validation Acc: 0.3575\n",
      "Epoch: 35/2000 Train Loss: 0.8103 Train Acc: 0.3722\n",
      "Epoch: 35/2000 Validation Loss: 0.7812 Validation Acc: 0.3575\n",
      "Epoch: 37/2000 Train Loss: 0.8079 Train Acc: 0.3750\n",
      "Epoch: 37/2000 Validation Loss: 0.7801 Validation Acc: 0.3575\n",
      "Epoch: 39/2000 Train Loss: 0.8055 Train Acc: 0.3750\n",
      "Epoch: 39/2000 Validation Loss: 0.7789 Validation Acc: 0.3575\n",
      "Epoch: 41/2000 Train Loss: 0.8031 Train Acc: 0.3750\n",
      "Epoch: 41/2000 Validation Loss: 0.7778 Validation Acc: 0.3575\n",
      "Epoch: 44/2000 Train Loss: 0.8008 Train Acc: 0.3750\n",
      "Epoch: 44/2000 Validation Loss: 0.7767 Validation Acc: 0.3575\n",
      "Epoch: 46/2000 Train Loss: 0.7985 Train Acc: 0.3750\n",
      "Epoch: 46/2000 Validation Loss: 0.7756 Validation Acc: 0.3575\n",
      "Epoch: 48/2000 Train Loss: 0.7962 Train Acc: 0.3750\n",
      "Epoch: 48/2000 Validation Loss: 0.7745 Validation Acc: 0.3631\n",
      "Epoch: 50/2000 Train Loss: 0.7939 Train Acc: 0.4326\n",
      "Epoch: 50/2000 Validation Loss: 0.7734 Validation Acc: 0.3631\n",
      "Epoch: 53/2000 Train Loss: 0.7916 Train Acc: 0.4326\n",
      "Epoch: 53/2000 Validation Loss: 0.7723 Validation Acc: 0.3631\n",
      "Epoch: 55/2000 Train Loss: 0.7893 Train Acc: 0.4382\n",
      "Epoch: 55/2000 Validation Loss: 0.7712 Validation Acc: 0.3631\n",
      "Epoch: 57/2000 Train Loss: 0.7870 Train Acc: 0.4410\n",
      "Epoch: 57/2000 Validation Loss: 0.7702 Validation Acc: 0.3464\n",
      "Epoch: 60/2000 Train Loss: 0.7848 Train Acc: 0.4410\n",
      "Epoch: 60/2000 Validation Loss: 0.7691 Validation Acc: 0.3520\n",
      "Epoch: 62/2000 Train Loss: 0.7826 Train Acc: 0.4480\n",
      "Epoch: 62/2000 Validation Loss: 0.7681 Validation Acc: 0.3520\n",
      "Epoch: 64/2000 Train Loss: 0.7804 Train Acc: 0.4621\n",
      "Epoch: 64/2000 Validation Loss: 0.7670 Validation Acc: 0.3520\n",
      "Epoch: 66/2000 Train Loss: 0.7782 Train Acc: 0.4621\n",
      "Epoch: 66/2000 Validation Loss: 0.7659 Validation Acc: 0.3520\n",
      "Epoch: 69/2000 Train Loss: 0.7760 Train Acc: 0.4621\n",
      "Epoch: 69/2000 Validation Loss: 0.7648 Validation Acc: 0.3520\n",
      "Epoch: 71/2000 Train Loss: 0.7739 Train Acc: 0.4944\n",
      "Epoch: 71/2000 Validation Loss: 0.7637 Validation Acc: 0.3520\n",
      "Epoch: 73/2000 Train Loss: 0.7718 Train Acc: 0.5000\n",
      "Epoch: 73/2000 Validation Loss: 0.7627 Validation Acc: 0.3575\n",
      "Epoch: 75/2000 Train Loss: 0.7697 Train Acc: 0.5000\n",
      "Epoch: 75/2000 Validation Loss: 0.7616 Validation Acc: 0.3575\n",
      "Epoch: 78/2000 Train Loss: 0.7676 Train Acc: 0.5000\n",
      "Epoch: 78/2000 Validation Loss: 0.7606 Validation Acc: 0.3575\n",
      "Epoch: 80/2000 Train Loss: 0.7656 Train Acc: 0.5014\n",
      "Epoch: 80/2000 Validation Loss: 0.7595 Validation Acc: 0.3575\n",
      "Epoch: 82/2000 Train Loss: 0.7636 Train Acc: 0.5028\n",
      "Epoch: 82/2000 Validation Loss: 0.7585 Validation Acc: 0.3575\n",
      "Epoch: 85/2000 Train Loss: 0.7616 Train Acc: 0.5028\n",
      "Epoch: 85/2000 Validation Loss: 0.7575 Validation Acc: 0.3687\n",
      "Epoch: 87/2000 Train Loss: 0.7597 Train Acc: 0.5028\n",
      "Epoch: 87/2000 Validation Loss: 0.7565 Validation Acc: 0.3687\n",
      "Epoch: 89/2000 Train Loss: 0.7577 Train Acc: 0.5056\n",
      "Epoch: 89/2000 Validation Loss: 0.7555 Validation Acc: 0.3687\n",
      "Epoch: 91/2000 Train Loss: 0.7558 Train Acc: 0.5056\n",
      "Epoch: 91/2000 Validation Loss: 0.7545 Validation Acc: 0.3687\n",
      "Epoch: 94/2000 Train Loss: 0.7539 Train Acc: 0.5056\n",
      "Epoch: 94/2000 Validation Loss: 0.7535 Validation Acc: 0.3687\n",
      "Epoch: 96/2000 Train Loss: 0.7520 Train Acc: 0.5070\n",
      "Epoch: 96/2000 Validation Loss: 0.7525 Validation Acc: 0.3687\n",
      "Epoch: 98/2000 Train Loss: 0.7501 Train Acc: 0.5323\n",
      "Epoch: 98/2000 Validation Loss: 0.7515 Validation Acc: 0.3687\n",
      "Epoch: 100/2000 Train Loss: 0.7482 Train Acc: 0.5281\n",
      "Epoch: 100/2000 Validation Loss: 0.7506 Validation Acc: 0.3687\n",
      "Epoch: 103/2000 Train Loss: 0.7464 Train Acc: 0.5295\n",
      "Epoch: 103/2000 Validation Loss: 0.7496 Validation Acc: 0.3631\n",
      "Epoch: 105/2000 Train Loss: 0.7446 Train Acc: 0.5295\n",
      "Epoch: 105/2000 Validation Loss: 0.7486 Validation Acc: 0.3687\n",
      "Epoch: 107/2000 Train Loss: 0.7428 Train Acc: 0.5309\n",
      "Epoch: 107/2000 Validation Loss: 0.7477 Validation Acc: 0.3687\n",
      "Epoch: 110/2000 Train Loss: 0.7410 Train Acc: 0.5351\n",
      "Epoch: 110/2000 Validation Loss: 0.7468 Validation Acc: 0.3687\n",
      "Epoch: 112/2000 Train Loss: 0.7392 Train Acc: 0.5351\n",
      "Epoch: 112/2000 Validation Loss: 0.7459 Validation Acc: 0.3687\n",
      "Epoch: 114/2000 Train Loss: 0.7375 Train Acc: 0.5351\n",
      "Epoch: 114/2000 Validation Loss: 0.7450 Validation Acc: 0.3687\n",
      "Epoch: 116/2000 Train Loss: 0.7358 Train Acc: 0.5351\n",
      "Epoch: 116/2000 Validation Loss: 0.7440 Validation Acc: 0.3631\n",
      "Epoch: 119/2000 Train Loss: 0.7340 Train Acc: 0.5351\n",
      "Epoch: 119/2000 Validation Loss: 0.7431 Validation Acc: 0.3687\n",
      "Epoch: 121/2000 Train Loss: 0.7324 Train Acc: 0.5435\n",
      "Epoch: 121/2000 Validation Loss: 0.7422 Validation Acc: 0.3687\n",
      "Epoch: 123/2000 Train Loss: 0.7307 Train Acc: 0.5421\n",
      "Epoch: 123/2000 Validation Loss: 0.7413 Validation Acc: 0.3799\n",
      "Epoch: 125/2000 Train Loss: 0.7290 Train Acc: 0.5435\n",
      "Epoch: 125/2000 Validation Loss: 0.7404 Validation Acc: 0.3799\n",
      "Epoch: 128/2000 Train Loss: 0.7273 Train Acc: 0.5562\n",
      "Epoch: 128/2000 Validation Loss: 0.7395 Validation Acc: 0.3799\n",
      "Epoch: 130/2000 Train Loss: 0.7257 Train Acc: 0.5604\n",
      "Epoch: 130/2000 Validation Loss: 0.7386 Validation Acc: 0.3911\n",
      "Epoch: 132/2000 Train Loss: 0.7241 Train Acc: 0.5660\n",
      "Epoch: 132/2000 Validation Loss: 0.7377 Validation Acc: 0.3966\n",
      "Epoch: 135/2000 Train Loss: 0.7225 Train Acc: 0.5730\n",
      "Epoch: 135/2000 Validation Loss: 0.7368 Validation Acc: 0.3966\n",
      "Epoch: 137/2000 Train Loss: 0.7209 Train Acc: 0.5730\n",
      "Epoch: 137/2000 Validation Loss: 0.7359 Validation Acc: 0.3966\n",
      "Epoch: 139/2000 Train Loss: 0.7193 Train Acc: 0.5744\n",
      "Epoch: 139/2000 Validation Loss: 0.7351 Validation Acc: 0.3966\n",
      "Epoch: 141/2000 Train Loss: 0.7177 Train Acc: 0.5772\n",
      "Epoch: 141/2000 Validation Loss: 0.7342 Validation Acc: 0.3966\n",
      "Epoch: 144/2000 Train Loss: 0.7161 Train Acc: 0.5772\n",
      "Epoch: 144/2000 Validation Loss: 0.7333 Validation Acc: 0.3966\n",
      "Epoch: 146/2000 Train Loss: 0.7145 Train Acc: 0.5772\n",
      "Epoch: 146/2000 Validation Loss: 0.7325 Validation Acc: 0.3966\n",
      "Epoch: 148/2000 Train Loss: 0.7130 Train Acc: 0.5758\n",
      "Epoch: 148/2000 Validation Loss: 0.7316 Validation Acc: 0.4022\n",
      "Epoch: 150/2000 Train Loss: 0.7114 Train Acc: 0.5787\n",
      "Epoch: 150/2000 Validation Loss: 0.7308 Validation Acc: 0.4022\n",
      "Epoch: 153/2000 Train Loss: 0.7099 Train Acc: 0.5815\n",
      "Epoch: 153/2000 Validation Loss: 0.7299 Validation Acc: 0.4078\n",
      "Epoch: 155/2000 Train Loss: 0.7084 Train Acc: 0.5815\n",
      "Epoch: 155/2000 Validation Loss: 0.7291 Validation Acc: 0.4134\n",
      "Epoch: 157/2000 Train Loss: 0.7069 Train Acc: 0.5787\n",
      "Epoch: 157/2000 Validation Loss: 0.7283 Validation Acc: 0.4134\n",
      "Epoch: 160/2000 Train Loss: 0.7054 Train Acc: 0.5787\n",
      "Epoch: 160/2000 Validation Loss: 0.7274 Validation Acc: 0.4190\n",
      "Epoch: 162/2000 Train Loss: 0.7039 Train Acc: 0.5787\n",
      "Epoch: 162/2000 Validation Loss: 0.7266 Validation Acc: 0.4190\n",
      "Epoch: 164/2000 Train Loss: 0.7024 Train Acc: 0.5787\n",
      "Epoch: 164/2000 Validation Loss: 0.7258 Validation Acc: 0.4190\n",
      "Epoch: 166/2000 Train Loss: 0.7009 Train Acc: 0.5787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166/2000 Validation Loss: 0.7249 Validation Acc: 0.4246\n",
      "Epoch: 169/2000 Train Loss: 0.6995 Train Acc: 0.6292\n",
      "Epoch: 169/2000 Validation Loss: 0.7241 Validation Acc: 0.4246\n",
      "Epoch: 171/2000 Train Loss: 0.6980 Train Acc: 0.6292\n",
      "Epoch: 171/2000 Validation Loss: 0.7233 Validation Acc: 0.4302\n",
      "Epoch: 173/2000 Train Loss: 0.6966 Train Acc: 0.6320\n",
      "Epoch: 173/2000 Validation Loss: 0.7225 Validation Acc: 0.4358\n",
      "Epoch: 175/2000 Train Loss: 0.6952 Train Acc: 0.6390\n",
      "Epoch: 175/2000 Validation Loss: 0.7216 Validation Acc: 0.4358\n",
      "Epoch: 178/2000 Train Loss: 0.6937 Train Acc: 0.6390\n",
      "Epoch: 178/2000 Validation Loss: 0.7208 Validation Acc: 0.4358\n",
      "Epoch: 180/2000 Train Loss: 0.6923 Train Acc: 0.6419\n",
      "Epoch: 180/2000 Validation Loss: 0.7200 Validation Acc: 0.4302\n",
      "Epoch: 182/2000 Train Loss: 0.6909 Train Acc: 0.6419\n",
      "Epoch: 182/2000 Validation Loss: 0.7192 Validation Acc: 0.4302\n",
      "Epoch: 185/2000 Train Loss: 0.6896 Train Acc: 0.6419\n",
      "Epoch: 185/2000 Validation Loss: 0.7184 Validation Acc: 0.4302\n",
      "Epoch: 187/2000 Train Loss: 0.6882 Train Acc: 0.6419\n",
      "Epoch: 187/2000 Validation Loss: 0.7176 Validation Acc: 0.4246\n",
      "Epoch: 189/2000 Train Loss: 0.6868 Train Acc: 0.6419\n",
      "Epoch: 189/2000 Validation Loss: 0.7168 Validation Acc: 0.4246\n",
      "Epoch: 191/2000 Train Loss: 0.6855 Train Acc: 0.6419\n",
      "Epoch: 191/2000 Validation Loss: 0.7160 Validation Acc: 0.4246\n",
      "Epoch: 194/2000 Train Loss: 0.6841 Train Acc: 0.6404\n",
      "Epoch: 194/2000 Validation Loss: 0.7152 Validation Acc: 0.4302\n",
      "Epoch: 196/2000 Train Loss: 0.6828 Train Acc: 0.6404\n",
      "Epoch: 196/2000 Validation Loss: 0.7145 Validation Acc: 0.4358\n",
      "Epoch: 198/2000 Train Loss: 0.6815 Train Acc: 0.6404\n",
      "Epoch: 198/2000 Validation Loss: 0.7137 Validation Acc: 0.4358\n",
      "Epoch: 200/2000 Train Loss: 0.6802 Train Acc: 0.6404\n",
      "Epoch: 200/2000 Validation Loss: 0.7129 Validation Acc: 0.4358\n",
      "Epoch: 203/2000 Train Loss: 0.6789 Train Acc: 0.6419\n",
      "Epoch: 203/2000 Validation Loss: 0.7121 Validation Acc: 0.4358\n",
      "Epoch: 205/2000 Train Loss: 0.6776 Train Acc: 0.6419\n",
      "Epoch: 205/2000 Validation Loss: 0.7114 Validation Acc: 0.4358\n",
      "Epoch: 207/2000 Train Loss: 0.6763 Train Acc: 0.6419\n",
      "Epoch: 207/2000 Validation Loss: 0.7106 Validation Acc: 0.4358\n",
      "Epoch: 210/2000 Train Loss: 0.6750 Train Acc: 0.6404\n",
      "Epoch: 210/2000 Validation Loss: 0.7099 Validation Acc: 0.4358\n",
      "Epoch: 212/2000 Train Loss: 0.6738 Train Acc: 0.6376\n",
      "Epoch: 212/2000 Validation Loss: 0.7091 Validation Acc: 0.4358\n",
      "Epoch: 214/2000 Train Loss: 0.6725 Train Acc: 0.6376\n",
      "Epoch: 214/2000 Validation Loss: 0.7084 Validation Acc: 0.4358\n",
      "Epoch: 216/2000 Train Loss: 0.6713 Train Acc: 0.6404\n",
      "Epoch: 216/2000 Validation Loss: 0.7076 Validation Acc: 0.4358\n",
      "Epoch: 219/2000 Train Loss: 0.6700 Train Acc: 0.6404\n",
      "Epoch: 219/2000 Validation Loss: 0.7069 Validation Acc: 0.4358\n",
      "Epoch: 221/2000 Train Loss: 0.6688 Train Acc: 0.6419\n",
      "Epoch: 221/2000 Validation Loss: 0.7061 Validation Acc: 0.4469\n",
      "Epoch: 223/2000 Train Loss: 0.6675 Train Acc: 0.6461\n",
      "Epoch: 223/2000 Validation Loss: 0.7054 Validation Acc: 0.4469\n",
      "Epoch: 225/2000 Train Loss: 0.6662 Train Acc: 0.6531\n",
      "Epoch: 225/2000 Validation Loss: 0.7046 Validation Acc: 0.4469\n",
      "Epoch: 228/2000 Train Loss: 0.6650 Train Acc: 0.6531\n",
      "Epoch: 228/2000 Validation Loss: 0.7039 Validation Acc: 0.4525\n",
      "Epoch: 230/2000 Train Loss: 0.6638 Train Acc: 0.6531\n",
      "Epoch: 230/2000 Validation Loss: 0.7032 Validation Acc: 0.4469\n",
      "Epoch: 232/2000 Train Loss: 0.6625 Train Acc: 0.6531\n",
      "Epoch: 232/2000 Validation Loss: 0.7024 Validation Acc: 0.4525\n",
      "Epoch: 235/2000 Train Loss: 0.6613 Train Acc: 0.6531\n",
      "Epoch: 235/2000 Validation Loss: 0.7017 Validation Acc: 0.4637\n",
      "Epoch: 237/2000 Train Loss: 0.6601 Train Acc: 0.6531\n",
      "Epoch: 237/2000 Validation Loss: 0.7009 Validation Acc: 0.4637\n",
      "Epoch: 239/2000 Train Loss: 0.6589 Train Acc: 0.6517\n",
      "Epoch: 239/2000 Validation Loss: 0.7002 Validation Acc: 0.4581\n",
      "Epoch: 241/2000 Train Loss: 0.6576 Train Acc: 0.6531\n",
      "Epoch: 241/2000 Validation Loss: 0.6994 Validation Acc: 0.4581\n",
      "Epoch: 244/2000 Train Loss: 0.6564 Train Acc: 0.6531\n",
      "Epoch: 244/2000 Validation Loss: 0.6987 Validation Acc: 0.4637\n",
      "Epoch: 246/2000 Train Loss: 0.6552 Train Acc: 0.6531\n",
      "Epoch: 246/2000 Validation Loss: 0.6980 Validation Acc: 0.4693\n",
      "Epoch: 248/2000 Train Loss: 0.6541 Train Acc: 0.6531\n",
      "Epoch: 248/2000 Validation Loss: 0.6972 Validation Acc: 0.4637\n",
      "Epoch: 250/2000 Train Loss: 0.6529 Train Acc: 0.6531\n",
      "Epoch: 250/2000 Validation Loss: 0.6965 Validation Acc: 0.4637\n",
      "Epoch: 253/2000 Train Loss: 0.6517 Train Acc: 0.6531\n",
      "Epoch: 253/2000 Validation Loss: 0.6958 Validation Acc: 0.4637\n",
      "Epoch: 255/2000 Train Loss: 0.6505 Train Acc: 0.6587\n",
      "Epoch: 255/2000 Validation Loss: 0.6951 Validation Acc: 0.4637\n",
      "Epoch: 257/2000 Train Loss: 0.6494 Train Acc: 0.6643\n",
      "Epoch: 257/2000 Validation Loss: 0.6944 Validation Acc: 0.4637\n",
      "Epoch: 260/2000 Train Loss: 0.6482 Train Acc: 0.6840\n",
      "Epoch: 260/2000 Validation Loss: 0.6937 Validation Acc: 0.4637\n",
      "Epoch: 262/2000 Train Loss: 0.6471 Train Acc: 0.6840\n",
      "Epoch: 262/2000 Validation Loss: 0.6930 Validation Acc: 0.4637\n",
      "Epoch: 264/2000 Train Loss: 0.6460 Train Acc: 0.6896\n",
      "Epoch: 264/2000 Validation Loss: 0.6923 Validation Acc: 0.4637\n",
      "Epoch: 266/2000 Train Loss: 0.6448 Train Acc: 0.6896\n",
      "Epoch: 266/2000 Validation Loss: 0.6916 Validation Acc: 0.4916\n",
      "Epoch: 269/2000 Train Loss: 0.6437 Train Acc: 0.6896\n",
      "Epoch: 269/2000 Validation Loss: 0.6909 Validation Acc: 0.5028\n",
      "Epoch: 271/2000 Train Loss: 0.6426 Train Acc: 0.6896\n",
      "Epoch: 271/2000 Validation Loss: 0.6902 Validation Acc: 0.5028\n",
      "Epoch: 273/2000 Train Loss: 0.6416 Train Acc: 0.6896\n",
      "Epoch: 273/2000 Validation Loss: 0.6895 Validation Acc: 0.5028\n",
      "Epoch: 275/2000 Train Loss: 0.6405 Train Acc: 0.6896\n",
      "Epoch: 275/2000 Validation Loss: 0.6888 Validation Acc: 0.5028\n",
      "Epoch: 278/2000 Train Loss: 0.6394 Train Acc: 0.6896\n",
      "Epoch: 278/2000 Validation Loss: 0.6881 Validation Acc: 0.5084\n",
      "Epoch: 280/2000 Train Loss: 0.6383 Train Acc: 0.6938\n",
      "Epoch: 280/2000 Validation Loss: 0.6875 Validation Acc: 0.5084\n",
      "Epoch: 282/2000 Train Loss: 0.6373 Train Acc: 0.6938\n",
      "Epoch: 282/2000 Validation Loss: 0.6868 Validation Acc: 0.5140\n",
      "Epoch: 285/2000 Train Loss: 0.6362 Train Acc: 0.7022\n",
      "Epoch: 285/2000 Validation Loss: 0.6861 Validation Acc: 0.5140\n",
      "Epoch: 287/2000 Train Loss: 0.6352 Train Acc: 0.7051\n",
      "Epoch: 287/2000 Validation Loss: 0.6855 Validation Acc: 0.5140\n",
      "Epoch: 289/2000 Train Loss: 0.6341 Train Acc: 0.7051\n",
      "Epoch: 289/2000 Validation Loss: 0.6848 Validation Acc: 0.5140\n",
      "Epoch: 291/2000 Train Loss: 0.6331 Train Acc: 0.7051\n",
      "Epoch: 291/2000 Validation Loss: 0.6841 Validation Acc: 0.5140\n",
      "Epoch: 294/2000 Train Loss: 0.6321 Train Acc: 0.7051\n",
      "Epoch: 294/2000 Validation Loss: 0.6835 Validation Acc: 0.5251\n",
      "Epoch: 296/2000 Train Loss: 0.6311 Train Acc: 0.7051\n",
      "Epoch: 296/2000 Validation Loss: 0.6828 Validation Acc: 0.5251\n",
      "Epoch: 298/2000 Train Loss: 0.6301 Train Acc: 0.7051\n",
      "Epoch: 298/2000 Validation Loss: 0.6822 Validation Acc: 0.5307\n",
      "Epoch: 300/2000 Train Loss: 0.6290 Train Acc: 0.7051\n",
      "Epoch: 300/2000 Validation Loss: 0.6815 Validation Acc: 0.5307\n",
      "Epoch: 303/2000 Train Loss: 0.6280 Train Acc: 0.7065\n",
      "Epoch: 303/2000 Validation Loss: 0.6809 Validation Acc: 0.5307\n",
      "Epoch: 305/2000 Train Loss: 0.6271 Train Acc: 0.7065\n",
      "Epoch: 305/2000 Validation Loss: 0.6802 Validation Acc: 0.5419\n",
      "Epoch: 307/2000 Train Loss: 0.6261 Train Acc: 0.7065\n",
      "Epoch: 307/2000 Validation Loss: 0.6796 Validation Acc: 0.5419\n",
      "Epoch: 310/2000 Train Loss: 0.6251 Train Acc: 0.7065\n",
      "Epoch: 310/2000 Validation Loss: 0.6789 Validation Acc: 0.5419\n",
      "Epoch: 312/2000 Train Loss: 0.6241 Train Acc: 0.7079\n",
      "Epoch: 312/2000 Validation Loss: 0.6783 Validation Acc: 0.5419\n",
      "Epoch: 314/2000 Train Loss: 0.6231 Train Acc: 0.7079\n",
      "Epoch: 314/2000 Validation Loss: 0.6777 Validation Acc: 0.5587\n",
      "Epoch: 316/2000 Train Loss: 0.6221 Train Acc: 0.7079\n",
      "Epoch: 316/2000 Validation Loss: 0.6770 Validation Acc: 0.5531\n",
      "Epoch: 319/2000 Train Loss: 0.6211 Train Acc: 0.7079\n",
      "Epoch: 319/2000 Validation Loss: 0.6764 Validation Acc: 0.5531\n",
      "Epoch: 321/2000 Train Loss: 0.6201 Train Acc: 0.7079\n",
      "Epoch: 321/2000 Validation Loss: 0.6757 Validation Acc: 0.5642\n",
      "Epoch: 323/2000 Train Loss: 0.6192 Train Acc: 0.7093\n",
      "Epoch: 323/2000 Validation Loss: 0.6751 Validation Acc: 0.5642\n",
      "Epoch: 325/2000 Train Loss: 0.6182 Train Acc: 0.7093\n",
      "Epoch: 325/2000 Validation Loss: 0.6745 Validation Acc: 0.5642\n",
      "Epoch: 328/2000 Train Loss: 0.6172 Train Acc: 0.7149\n",
      "Epoch: 328/2000 Validation Loss: 0.6738 Validation Acc: 0.5642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330/2000 Train Loss: 0.6163 Train Acc: 0.7177\n",
      "Epoch: 330/2000 Validation Loss: 0.6732 Validation Acc: 0.6480\n",
      "Epoch: 332/2000 Train Loss: 0.6153 Train Acc: 0.7177\n",
      "Epoch: 332/2000 Validation Loss: 0.6726 Validation Acc: 0.6480\n",
      "Epoch: 335/2000 Train Loss: 0.6144 Train Acc: 0.7289\n",
      "Epoch: 335/2000 Validation Loss: 0.6720 Validation Acc: 0.6480\n",
      "Epoch: 337/2000 Train Loss: 0.6135 Train Acc: 0.7289\n",
      "Epoch: 337/2000 Validation Loss: 0.6713 Validation Acc: 0.6480\n",
      "Epoch: 339/2000 Train Loss: 0.6125 Train Acc: 0.7289\n",
      "Epoch: 339/2000 Validation Loss: 0.6707 Validation Acc: 0.6480\n",
      "Epoch: 341/2000 Train Loss: 0.6116 Train Acc: 0.7289\n",
      "Epoch: 341/2000 Validation Loss: 0.6701 Validation Acc: 0.6480\n",
      "Epoch: 344/2000 Train Loss: 0.6107 Train Acc: 0.7303\n",
      "Epoch: 344/2000 Validation Loss: 0.6695 Validation Acc: 0.6536\n",
      "Epoch: 346/2000 Train Loss: 0.6098 Train Acc: 0.7303\n",
      "Epoch: 346/2000 Validation Loss: 0.6689 Validation Acc: 0.6536\n",
      "Epoch: 348/2000 Train Loss: 0.6089 Train Acc: 0.7303\n",
      "Epoch: 348/2000 Validation Loss: 0.6683 Validation Acc: 0.6536\n",
      "Epoch: 350/2000 Train Loss: 0.6079 Train Acc: 0.7303\n",
      "Epoch: 350/2000 Validation Loss: 0.6676 Validation Acc: 0.6536\n",
      "Epoch: 353/2000 Train Loss: 0.6070 Train Acc: 0.7303\n",
      "Epoch: 353/2000 Validation Loss: 0.6670 Validation Acc: 0.6536\n",
      "Epoch: 355/2000 Train Loss: 0.6061 Train Acc: 0.7303\n",
      "Epoch: 355/2000 Validation Loss: 0.6664 Validation Acc: 0.6536\n",
      "Epoch: 357/2000 Train Loss: 0.6052 Train Acc: 0.7303\n",
      "Epoch: 357/2000 Validation Loss: 0.6658 Validation Acc: 0.6536\n",
      "Epoch: 360/2000 Train Loss: 0.6043 Train Acc: 0.7303\n",
      "Epoch: 360/2000 Validation Loss: 0.6652 Validation Acc: 0.6536\n",
      "Epoch: 362/2000 Train Loss: 0.6035 Train Acc: 0.7303\n",
      "Epoch: 362/2000 Validation Loss: 0.6646 Validation Acc: 0.6536\n",
      "Epoch: 364/2000 Train Loss: 0.6026 Train Acc: 0.7303\n",
      "Epoch: 364/2000 Validation Loss: 0.6640 Validation Acc: 0.6592\n",
      "Epoch: 366/2000 Train Loss: 0.6017 Train Acc: 0.7346\n",
      "Epoch: 366/2000 Validation Loss: 0.6635 Validation Acc: 0.6592\n",
      "Epoch: 369/2000 Train Loss: 0.6008 Train Acc: 0.7346\n",
      "Epoch: 369/2000 Validation Loss: 0.6629 Validation Acc: 0.6592\n",
      "Epoch: 371/2000 Train Loss: 0.5999 Train Acc: 0.7346\n",
      "Epoch: 371/2000 Validation Loss: 0.6623 Validation Acc: 0.6592\n",
      "Epoch: 373/2000 Train Loss: 0.5991 Train Acc: 0.7346\n",
      "Epoch: 373/2000 Validation Loss: 0.6617 Validation Acc: 0.6592\n",
      "Epoch: 375/2000 Train Loss: 0.5982 Train Acc: 0.7388\n",
      "Epoch: 375/2000 Validation Loss: 0.6611 Validation Acc: 0.6592\n",
      "Epoch: 378/2000 Train Loss: 0.5973 Train Acc: 0.7388\n",
      "Epoch: 378/2000 Validation Loss: 0.6605 Validation Acc: 0.6592\n",
      "Epoch: 380/2000 Train Loss: 0.5965 Train Acc: 0.7388\n",
      "Epoch: 380/2000 Validation Loss: 0.6600 Validation Acc: 0.6592\n",
      "Epoch: 382/2000 Train Loss: 0.5956 Train Acc: 0.7388\n",
      "Epoch: 382/2000 Validation Loss: 0.6594 Validation Acc: 0.6648\n",
      "Epoch: 385/2000 Train Loss: 0.5948 Train Acc: 0.7388\n",
      "Epoch: 385/2000 Validation Loss: 0.6588 Validation Acc: 0.6648\n",
      "Epoch: 387/2000 Train Loss: 0.5939 Train Acc: 0.7388\n",
      "Epoch: 387/2000 Validation Loss: 0.6583 Validation Acc: 0.6648\n",
      "Epoch: 389/2000 Train Loss: 0.5931 Train Acc: 0.7402\n",
      "Epoch: 389/2000 Validation Loss: 0.6577 Validation Acc: 0.6648\n",
      "Epoch: 391/2000 Train Loss: 0.5922 Train Acc: 0.7402\n",
      "Epoch: 391/2000 Validation Loss: 0.6571 Validation Acc: 0.6648\n",
      "Epoch: 394/2000 Train Loss: 0.5914 Train Acc: 0.7416\n",
      "Epoch: 394/2000 Validation Loss: 0.6566 Validation Acc: 0.6648\n",
      "Epoch: 396/2000 Train Loss: 0.5906 Train Acc: 0.7444\n",
      "Epoch: 396/2000 Validation Loss: 0.6560 Validation Acc: 0.6648\n",
      "Epoch: 398/2000 Train Loss: 0.5897 Train Acc: 0.7458\n",
      "Epoch: 398/2000 Validation Loss: 0.6555 Validation Acc: 0.6648\n",
      "Epoch: 400/2000 Train Loss: 0.5889 Train Acc: 0.7458\n",
      "Epoch: 400/2000 Validation Loss: 0.6549 Validation Acc: 0.6704\n",
      "Epoch: 403/2000 Train Loss: 0.5881 Train Acc: 0.7458\n",
      "Epoch: 403/2000 Validation Loss: 0.6543 Validation Acc: 0.6704\n",
      "Epoch: 405/2000 Train Loss: 0.5873 Train Acc: 0.7430\n",
      "Epoch: 405/2000 Validation Loss: 0.6538 Validation Acc: 0.6704\n",
      "Epoch: 407/2000 Train Loss: 0.5865 Train Acc: 0.7430\n",
      "Epoch: 407/2000 Validation Loss: 0.6532 Validation Acc: 0.6704\n",
      "Epoch: 410/2000 Train Loss: 0.5857 Train Acc: 0.7430\n",
      "Epoch: 410/2000 Validation Loss: 0.6527 Validation Acc: 0.6704\n",
      "Epoch: 412/2000 Train Loss: 0.5849 Train Acc: 0.7430\n",
      "Epoch: 412/2000 Validation Loss: 0.6521 Validation Acc: 0.6704\n",
      "Epoch: 414/2000 Train Loss: 0.5841 Train Acc: 0.7430\n",
      "Epoch: 414/2000 Validation Loss: 0.6515 Validation Acc: 0.6760\n",
      "Epoch: 416/2000 Train Loss: 0.5833 Train Acc: 0.7444\n",
      "Epoch: 416/2000 Validation Loss: 0.6510 Validation Acc: 0.6760\n",
      "Epoch: 419/2000 Train Loss: 0.5825 Train Acc: 0.7430\n",
      "Epoch: 419/2000 Validation Loss: 0.6504 Validation Acc: 0.6760\n",
      "Epoch: 421/2000 Train Loss: 0.5818 Train Acc: 0.7444\n",
      "Epoch: 421/2000 Validation Loss: 0.6499 Validation Acc: 0.6760\n",
      "Epoch: 423/2000 Train Loss: 0.5810 Train Acc: 0.7486\n",
      "Epoch: 423/2000 Validation Loss: 0.6493 Validation Acc: 0.6760\n",
      "Epoch: 425/2000 Train Loss: 0.5802 Train Acc: 0.7486\n",
      "Epoch: 425/2000 Validation Loss: 0.6488 Validation Acc: 0.6760\n",
      "Epoch: 428/2000 Train Loss: 0.5795 Train Acc: 0.7486\n",
      "Epoch: 428/2000 Validation Loss: 0.6482 Validation Acc: 0.6816\n",
      "Epoch: 430/2000 Train Loss: 0.5787 Train Acc: 0.7528\n",
      "Epoch: 430/2000 Validation Loss: 0.6477 Validation Acc: 0.6816\n",
      "Epoch: 432/2000 Train Loss: 0.5780 Train Acc: 0.7528\n",
      "Epoch: 432/2000 Validation Loss: 0.6471 Validation Acc: 0.6816\n",
      "Epoch: 435/2000 Train Loss: 0.5772 Train Acc: 0.7528\n",
      "Epoch: 435/2000 Validation Loss: 0.6466 Validation Acc: 0.6816\n",
      "Epoch: 437/2000 Train Loss: 0.5765 Train Acc: 0.7542\n",
      "Epoch: 437/2000 Validation Loss: 0.6461 Validation Acc: 0.6816\n",
      "Epoch: 439/2000 Train Loss: 0.5757 Train Acc: 0.7542\n",
      "Epoch: 439/2000 Validation Loss: 0.6455 Validation Acc: 0.6872\n",
      "Epoch: 441/2000 Train Loss: 0.5750 Train Acc: 0.7542\n",
      "Epoch: 441/2000 Validation Loss: 0.6450 Validation Acc: 0.6872\n",
      "Epoch: 444/2000 Train Loss: 0.5743 Train Acc: 0.7542\n",
      "Epoch: 444/2000 Validation Loss: 0.6445 Validation Acc: 0.6872\n",
      "Epoch: 446/2000 Train Loss: 0.5736 Train Acc: 0.7542\n",
      "Epoch: 446/2000 Validation Loss: 0.6440 Validation Acc: 0.6872\n",
      "Epoch: 448/2000 Train Loss: 0.5728 Train Acc: 0.7542\n",
      "Epoch: 448/2000 Validation Loss: 0.6434 Validation Acc: 0.6872\n",
      "Epoch: 450/2000 Train Loss: 0.5721 Train Acc: 0.7542\n",
      "Epoch: 450/2000 Validation Loss: 0.6429 Validation Acc: 0.6872\n",
      "Epoch: 453/2000 Train Loss: 0.5714 Train Acc: 0.7542\n",
      "Epoch: 453/2000 Validation Loss: 0.6424 Validation Acc: 0.6872\n",
      "Epoch: 455/2000 Train Loss: 0.5707 Train Acc: 0.7542\n",
      "Epoch: 455/2000 Validation Loss: 0.6419 Validation Acc: 0.6872\n",
      "Epoch: 457/2000 Train Loss: 0.5699 Train Acc: 0.7556\n",
      "Epoch: 457/2000 Validation Loss: 0.6413 Validation Acc: 0.6872\n",
      "Epoch: 460/2000 Train Loss: 0.5692 Train Acc: 0.7556\n",
      "Epoch: 460/2000 Validation Loss: 0.6408 Validation Acc: 0.6872\n",
      "Epoch: 462/2000 Train Loss: 0.5685 Train Acc: 0.7570\n",
      "Epoch: 462/2000 Validation Loss: 0.6403 Validation Acc: 0.6872\n",
      "Epoch: 464/2000 Train Loss: 0.5678 Train Acc: 0.7612\n",
      "Epoch: 464/2000 Validation Loss: 0.6398 Validation Acc: 0.6872\n",
      "Epoch: 466/2000 Train Loss: 0.5671 Train Acc: 0.7612\n",
      "Epoch: 466/2000 Validation Loss: 0.6393 Validation Acc: 0.6872\n",
      "Epoch: 469/2000 Train Loss: 0.5664 Train Acc: 0.7612\n",
      "Epoch: 469/2000 Validation Loss: 0.6388 Validation Acc: 0.6872\n",
      "Epoch: 471/2000 Train Loss: 0.5657 Train Acc: 0.7612\n",
      "Epoch: 471/2000 Validation Loss: 0.6383 Validation Acc: 0.6816\n",
      "Epoch: 473/2000 Train Loss: 0.5651 Train Acc: 0.7612\n",
      "Epoch: 473/2000 Validation Loss: 0.6378 Validation Acc: 0.6816\n",
      "Epoch: 475/2000 Train Loss: 0.5644 Train Acc: 0.7612\n",
      "Epoch: 475/2000 Validation Loss: 0.6373 Validation Acc: 0.6816\n",
      "Epoch: 478/2000 Train Loss: 0.5637 Train Acc: 0.7612\n",
      "Epoch: 478/2000 Validation Loss: 0.6368 Validation Acc: 0.6816\n",
      "Epoch: 480/2000 Train Loss: 0.5630 Train Acc: 0.7612\n",
      "Epoch: 480/2000 Validation Loss: 0.6363 Validation Acc: 0.6816\n",
      "Epoch: 482/2000 Train Loss: 0.5623 Train Acc: 0.7612\n",
      "Epoch: 482/2000 Validation Loss: 0.6358 Validation Acc: 0.6816\n",
      "Epoch: 485/2000 Train Loss: 0.5617 Train Acc: 0.7612\n",
      "Epoch: 485/2000 Validation Loss: 0.6353 Validation Acc: 0.6816\n",
      "Epoch: 487/2000 Train Loss: 0.5610 Train Acc: 0.7612\n",
      "Epoch: 487/2000 Validation Loss: 0.6348 Validation Acc: 0.6816\n",
      "Epoch: 489/2000 Train Loss: 0.5603 Train Acc: 0.7612\n",
      "Epoch: 489/2000 Validation Loss: 0.6343 Validation Acc: 0.6816\n",
      "Epoch: 491/2000 Train Loss: 0.5597 Train Acc: 0.7612\n",
      "Epoch: 491/2000 Validation Loss: 0.6338 Validation Acc: 0.6816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 494/2000 Train Loss: 0.5590 Train Acc: 0.7612\n",
      "Epoch: 494/2000 Validation Loss: 0.6333 Validation Acc: 0.6816\n",
      "Epoch: 496/2000 Train Loss: 0.5583 Train Acc: 0.7612\n",
      "Epoch: 496/2000 Validation Loss: 0.6329 Validation Acc: 0.6816\n",
      "Epoch: 498/2000 Train Loss: 0.5577 Train Acc: 0.7626\n",
      "Epoch: 498/2000 Validation Loss: 0.6324 Validation Acc: 0.6816\n",
      "Epoch: 500/2000 Train Loss: 0.5570 Train Acc: 0.7626\n",
      "Epoch: 500/2000 Validation Loss: 0.6319 Validation Acc: 0.6816\n",
      "Epoch: 503/2000 Train Loss: 0.5564 Train Acc: 0.7640\n",
      "Epoch: 503/2000 Validation Loss: 0.6314 Validation Acc: 0.6816\n",
      "Epoch: 505/2000 Train Loss: 0.5558 Train Acc: 0.7640\n",
      "Epoch: 505/2000 Validation Loss: 0.6310 Validation Acc: 0.6816\n",
      "Epoch: 507/2000 Train Loss: 0.5551 Train Acc: 0.7640\n",
      "Epoch: 507/2000 Validation Loss: 0.6305 Validation Acc: 0.6816\n",
      "Epoch: 510/2000 Train Loss: 0.5545 Train Acc: 0.7640\n",
      "Epoch: 510/2000 Validation Loss: 0.6300 Validation Acc: 0.6816\n",
      "Epoch: 512/2000 Train Loss: 0.5539 Train Acc: 0.7640\n",
      "Epoch: 512/2000 Validation Loss: 0.6296 Validation Acc: 0.6816\n",
      "Epoch: 514/2000 Train Loss: 0.5533 Train Acc: 0.7640\n",
      "Epoch: 514/2000 Validation Loss: 0.6291 Validation Acc: 0.6816\n",
      "Epoch: 516/2000 Train Loss: 0.5527 Train Acc: 0.7640\n",
      "Epoch: 516/2000 Validation Loss: 0.6286 Validation Acc: 0.6816\n",
      "Epoch: 519/2000 Train Loss: 0.5521 Train Acc: 0.7640\n",
      "Epoch: 519/2000 Validation Loss: 0.6282 Validation Acc: 0.6816\n",
      "Epoch: 521/2000 Train Loss: 0.5515 Train Acc: 0.7669\n",
      "Epoch: 521/2000 Validation Loss: 0.6277 Validation Acc: 0.6872\n",
      "Epoch: 523/2000 Train Loss: 0.5509 Train Acc: 0.7669\n",
      "Epoch: 523/2000 Validation Loss: 0.6273 Validation Acc: 0.6872\n",
      "Epoch: 525/2000 Train Loss: 0.5503 Train Acc: 0.7669\n",
      "Epoch: 525/2000 Validation Loss: 0.6268 Validation Acc: 0.6927\n",
      "Epoch: 528/2000 Train Loss: 0.5497 Train Acc: 0.7669\n",
      "Epoch: 528/2000 Validation Loss: 0.6264 Validation Acc: 0.6927\n",
      "Epoch: 530/2000 Train Loss: 0.5491 Train Acc: 0.7669\n",
      "Epoch: 530/2000 Validation Loss: 0.6259 Validation Acc: 0.6927\n",
      "Epoch: 532/2000 Train Loss: 0.5485 Train Acc: 0.7669\n",
      "Epoch: 532/2000 Validation Loss: 0.6255 Validation Acc: 0.6927\n",
      "Epoch: 535/2000 Train Loss: 0.5479 Train Acc: 0.7669\n",
      "Epoch: 535/2000 Validation Loss: 0.6250 Validation Acc: 0.6927\n",
      "Epoch: 537/2000 Train Loss: 0.5473 Train Acc: 0.7669\n",
      "Epoch: 537/2000 Validation Loss: 0.6246 Validation Acc: 0.6927\n",
      "Epoch: 539/2000 Train Loss: 0.5467 Train Acc: 0.7669\n",
      "Epoch: 539/2000 Validation Loss: 0.6241 Validation Acc: 0.6927\n",
      "Epoch: 541/2000 Train Loss: 0.5461 Train Acc: 0.7669\n",
      "Epoch: 541/2000 Validation Loss: 0.6237 Validation Acc: 0.6927\n",
      "Epoch: 544/2000 Train Loss: 0.5456 Train Acc: 0.7669\n",
      "Epoch: 544/2000 Validation Loss: 0.6232 Validation Acc: 0.6927\n",
      "Epoch: 546/2000 Train Loss: 0.5450 Train Acc: 0.7654\n",
      "Epoch: 546/2000 Validation Loss: 0.6228 Validation Acc: 0.6927\n",
      "Epoch: 548/2000 Train Loss: 0.5444 Train Acc: 0.7654\n",
      "Epoch: 548/2000 Validation Loss: 0.6224 Validation Acc: 0.6927\n",
      "Epoch: 550/2000 Train Loss: 0.5438 Train Acc: 0.7654\n",
      "Epoch: 550/2000 Validation Loss: 0.6219 Validation Acc: 0.6983\n",
      "Epoch: 553/2000 Train Loss: 0.5433 Train Acc: 0.7654\n",
      "Epoch: 553/2000 Validation Loss: 0.6215 Validation Acc: 0.6983\n",
      "Epoch: 555/2000 Train Loss: 0.5427 Train Acc: 0.7654\n",
      "Epoch: 555/2000 Validation Loss: 0.6211 Validation Acc: 0.6983\n",
      "Epoch: 557/2000 Train Loss: 0.5421 Train Acc: 0.7654\n",
      "Epoch: 557/2000 Validation Loss: 0.6206 Validation Acc: 0.6983\n",
      "Epoch: 560/2000 Train Loss: 0.5416 Train Acc: 0.7654\n",
      "Epoch: 560/2000 Validation Loss: 0.6202 Validation Acc: 0.6983\n",
      "Epoch: 562/2000 Train Loss: 0.5410 Train Acc: 0.7739\n",
      "Epoch: 562/2000 Validation Loss: 0.6198 Validation Acc: 0.6983\n",
      "Epoch: 564/2000 Train Loss: 0.5405 Train Acc: 0.7739\n",
      "Epoch: 564/2000 Validation Loss: 0.6193 Validation Acc: 0.6983\n",
      "Epoch: 566/2000 Train Loss: 0.5399 Train Acc: 0.7739\n",
      "Epoch: 566/2000 Validation Loss: 0.6189 Validation Acc: 0.6983\n",
      "Epoch: 569/2000 Train Loss: 0.5394 Train Acc: 0.7739\n",
      "Epoch: 569/2000 Validation Loss: 0.6185 Validation Acc: 0.6983\n",
      "Epoch: 571/2000 Train Loss: 0.5389 Train Acc: 0.7739\n",
      "Epoch: 571/2000 Validation Loss: 0.6181 Validation Acc: 0.6983\n",
      "Epoch: 573/2000 Train Loss: 0.5383 Train Acc: 0.7739\n",
      "Epoch: 573/2000 Validation Loss: 0.6176 Validation Acc: 0.6983\n",
      "Epoch: 575/2000 Train Loss: 0.5378 Train Acc: 0.7739\n",
      "Epoch: 575/2000 Validation Loss: 0.6172 Validation Acc: 0.6983\n",
      "Epoch: 578/2000 Train Loss: 0.5372 Train Acc: 0.7739\n",
      "Epoch: 578/2000 Validation Loss: 0.6168 Validation Acc: 0.6983\n",
      "Epoch: 580/2000 Train Loss: 0.5367 Train Acc: 0.7739\n",
      "Epoch: 580/2000 Validation Loss: 0.6164 Validation Acc: 0.6983\n",
      "Epoch: 582/2000 Train Loss: 0.5362 Train Acc: 0.7739\n",
      "Epoch: 582/2000 Validation Loss: 0.6159 Validation Acc: 0.6983\n",
      "Epoch: 585/2000 Train Loss: 0.5356 Train Acc: 0.7739\n",
      "Epoch: 585/2000 Validation Loss: 0.6155 Validation Acc: 0.6983\n",
      "Epoch: 587/2000 Train Loss: 0.5351 Train Acc: 0.7739\n",
      "Epoch: 587/2000 Validation Loss: 0.6151 Validation Acc: 0.6983\n",
      "Epoch: 589/2000 Train Loss: 0.5346 Train Acc: 0.7739\n",
      "Epoch: 589/2000 Validation Loss: 0.6147 Validation Acc: 0.6983\n",
      "Epoch: 591/2000 Train Loss: 0.5341 Train Acc: 0.7781\n",
      "Epoch: 591/2000 Validation Loss: 0.6143 Validation Acc: 0.6983\n",
      "Epoch: 594/2000 Train Loss: 0.5335 Train Acc: 0.7781\n",
      "Epoch: 594/2000 Validation Loss: 0.6139 Validation Acc: 0.6983\n",
      "Epoch: 596/2000 Train Loss: 0.5330 Train Acc: 0.7781\n",
      "Epoch: 596/2000 Validation Loss: 0.6135 Validation Acc: 0.6983\n",
      "Epoch: 598/2000 Train Loss: 0.5325 Train Acc: 0.7781\n",
      "Epoch: 598/2000 Validation Loss: 0.6131 Validation Acc: 0.6983\n",
      "Epoch: 600/2000 Train Loss: 0.5320 Train Acc: 0.7781\n",
      "Epoch: 600/2000 Validation Loss: 0.6126 Validation Acc: 0.6983\n",
      "Epoch: 603/2000 Train Loss: 0.5314 Train Acc: 0.7781\n",
      "Epoch: 603/2000 Validation Loss: 0.6122 Validation Acc: 0.6983\n",
      "Epoch: 605/2000 Train Loss: 0.5309 Train Acc: 0.7781\n",
      "Epoch: 605/2000 Validation Loss: 0.6118 Validation Acc: 0.6983\n",
      "Epoch: 607/2000 Train Loss: 0.5304 Train Acc: 0.7781\n",
      "Epoch: 607/2000 Validation Loss: 0.6114 Validation Acc: 0.6983\n",
      "Epoch: 610/2000 Train Loss: 0.5299 Train Acc: 0.7795\n",
      "Epoch: 610/2000 Validation Loss: 0.6109 Validation Acc: 0.6983\n",
      "Epoch: 612/2000 Train Loss: 0.5294 Train Acc: 0.7795\n",
      "Epoch: 612/2000 Validation Loss: 0.6105 Validation Acc: 0.6983\n",
      "Epoch: 614/2000 Train Loss: 0.5289 Train Acc: 0.7795\n",
      "Epoch: 614/2000 Validation Loss: 0.6101 Validation Acc: 0.6983\n",
      "Epoch: 616/2000 Train Loss: 0.5284 Train Acc: 0.7795\n",
      "Epoch: 616/2000 Validation Loss: 0.6097 Validation Acc: 0.6983\n",
      "Epoch: 619/2000 Train Loss: 0.5279 Train Acc: 0.7809\n",
      "Epoch: 619/2000 Validation Loss: 0.6093 Validation Acc: 0.6983\n",
      "Epoch: 621/2000 Train Loss: 0.5274 Train Acc: 0.7823\n",
      "Epoch: 621/2000 Validation Loss: 0.6088 Validation Acc: 0.6983\n",
      "Epoch: 623/2000 Train Loss: 0.5269 Train Acc: 0.7823\n",
      "Epoch: 623/2000 Validation Loss: 0.6084 Validation Acc: 0.6983\n",
      "Epoch: 625/2000 Train Loss: 0.5264 Train Acc: 0.7823\n",
      "Epoch: 625/2000 Validation Loss: 0.6080 Validation Acc: 0.6983\n",
      "Epoch: 628/2000 Train Loss: 0.5259 Train Acc: 0.7837\n",
      "Epoch: 628/2000 Validation Loss: 0.6076 Validation Acc: 0.6983\n",
      "Epoch: 630/2000 Train Loss: 0.5254 Train Acc: 0.7837\n",
      "Epoch: 630/2000 Validation Loss: 0.6072 Validation Acc: 0.6983\n",
      "Epoch: 632/2000 Train Loss: 0.5249 Train Acc: 0.7837\n",
      "Epoch: 632/2000 Validation Loss: 0.6068 Validation Acc: 0.6983\n",
      "Epoch: 635/2000 Train Loss: 0.5244 Train Acc: 0.7851\n",
      "Epoch: 635/2000 Validation Loss: 0.6064 Validation Acc: 0.6983\n",
      "Epoch: 637/2000 Train Loss: 0.5239 Train Acc: 0.7851\n",
      "Epoch: 637/2000 Validation Loss: 0.6060 Validation Acc: 0.6983\n",
      "Epoch: 639/2000 Train Loss: 0.5234 Train Acc: 0.7851\n",
      "Epoch: 639/2000 Validation Loss: 0.6056 Validation Acc: 0.6983\n",
      "Epoch: 641/2000 Train Loss: 0.5230 Train Acc: 0.7851\n",
      "Epoch: 641/2000 Validation Loss: 0.6052 Validation Acc: 0.6983\n",
      "Epoch: 644/2000 Train Loss: 0.5225 Train Acc: 0.7851\n",
      "Epoch: 644/2000 Validation Loss: 0.6048 Validation Acc: 0.6983\n",
      "Epoch: 646/2000 Train Loss: 0.5220 Train Acc: 0.7851\n",
      "Epoch: 646/2000 Validation Loss: 0.6044 Validation Acc: 0.6983\n",
      "Epoch: 648/2000 Train Loss: 0.5215 Train Acc: 0.7851\n",
      "Epoch: 648/2000 Validation Loss: 0.6040 Validation Acc: 0.6983\n",
      "Epoch: 650/2000 Train Loss: 0.5210 Train Acc: 0.7851\n",
      "Epoch: 650/2000 Validation Loss: 0.6036 Validation Acc: 0.7039\n",
      "Epoch: 653/2000 Train Loss: 0.5206 Train Acc: 0.7851\n",
      "Epoch: 653/2000 Validation Loss: 0.6032 Validation Acc: 0.7039\n",
      "Epoch: 655/2000 Train Loss: 0.5201 Train Acc: 0.7851\n",
      "Epoch: 655/2000 Validation Loss: 0.6028 Validation Acc: 0.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 657/2000 Train Loss: 0.5196 Train Acc: 0.7851\n",
      "Epoch: 657/2000 Validation Loss: 0.6024 Validation Acc: 0.7039\n",
      "Epoch: 660/2000 Train Loss: 0.5192 Train Acc: 0.7851\n",
      "Epoch: 660/2000 Validation Loss: 0.6020 Validation Acc: 0.7039\n",
      "Epoch: 662/2000 Train Loss: 0.5187 Train Acc: 0.7851\n",
      "Epoch: 662/2000 Validation Loss: 0.6016 Validation Acc: 0.7039\n",
      "Epoch: 664/2000 Train Loss: 0.5182 Train Acc: 0.7851\n",
      "Epoch: 664/2000 Validation Loss: 0.6012 Validation Acc: 0.7039\n",
      "Epoch: 666/2000 Train Loss: 0.5178 Train Acc: 0.7851\n",
      "Epoch: 666/2000 Validation Loss: 0.6008 Validation Acc: 0.7039\n",
      "Epoch: 669/2000 Train Loss: 0.5173 Train Acc: 0.7851\n",
      "Epoch: 669/2000 Validation Loss: 0.6004 Validation Acc: 0.7039\n",
      "Epoch: 671/2000 Train Loss: 0.5168 Train Acc: 0.7921\n",
      "Epoch: 671/2000 Validation Loss: 0.6000 Validation Acc: 0.7039\n",
      "Epoch: 673/2000 Train Loss: 0.5164 Train Acc: 0.7907\n",
      "Epoch: 673/2000 Validation Loss: 0.5997 Validation Acc: 0.7039\n",
      "Epoch: 675/2000 Train Loss: 0.5159 Train Acc: 0.7921\n",
      "Epoch: 675/2000 Validation Loss: 0.5993 Validation Acc: 0.7039\n",
      "Epoch: 678/2000 Train Loss: 0.5155 Train Acc: 0.7921\n",
      "Epoch: 678/2000 Validation Loss: 0.5989 Validation Acc: 0.6983\n",
      "Epoch: 680/2000 Train Loss: 0.5150 Train Acc: 0.7921\n",
      "Epoch: 680/2000 Validation Loss: 0.5985 Validation Acc: 0.6983\n",
      "Epoch: 682/2000 Train Loss: 0.5145 Train Acc: 0.7921\n",
      "Epoch: 682/2000 Validation Loss: 0.5981 Validation Acc: 0.6983\n",
      "Epoch: 685/2000 Train Loss: 0.5141 Train Acc: 0.7921\n",
      "Epoch: 685/2000 Validation Loss: 0.5977 Validation Acc: 0.6983\n",
      "Epoch: 687/2000 Train Loss: 0.5136 Train Acc: 0.7921\n",
      "Epoch: 687/2000 Validation Loss: 0.5973 Validation Acc: 0.6983\n",
      "Epoch: 689/2000 Train Loss: 0.5132 Train Acc: 0.7921\n",
      "Epoch: 689/2000 Validation Loss: 0.5970 Validation Acc: 0.6983\n",
      "Epoch: 691/2000 Train Loss: 0.5127 Train Acc: 0.7921\n",
      "Epoch: 691/2000 Validation Loss: 0.5966 Validation Acc: 0.6983\n",
      "Epoch: 694/2000 Train Loss: 0.5123 Train Acc: 0.7921\n",
      "Epoch: 694/2000 Validation Loss: 0.5962 Validation Acc: 0.6983\n",
      "Epoch: 696/2000 Train Loss: 0.5118 Train Acc: 0.7935\n",
      "Epoch: 696/2000 Validation Loss: 0.5958 Validation Acc: 0.6983\n",
      "Epoch: 698/2000 Train Loss: 0.5114 Train Acc: 0.7978\n",
      "Epoch: 698/2000 Validation Loss: 0.5954 Validation Acc: 0.6983\n",
      "Epoch: 700/2000 Train Loss: 0.5109 Train Acc: 0.7978\n",
      "Epoch: 700/2000 Validation Loss: 0.5950 Validation Acc: 0.6983\n",
      "Epoch: 703/2000 Train Loss: 0.5105 Train Acc: 0.7978\n",
      "Epoch: 703/2000 Validation Loss: 0.5947 Validation Acc: 0.6983\n",
      "Epoch: 705/2000 Train Loss: 0.5100 Train Acc: 0.7978\n",
      "Epoch: 705/2000 Validation Loss: 0.5943 Validation Acc: 0.6983\n",
      "Epoch: 707/2000 Train Loss: 0.5096 Train Acc: 0.7978\n",
      "Epoch: 707/2000 Validation Loss: 0.5939 Validation Acc: 0.6983\n",
      "Epoch: 710/2000 Train Loss: 0.5091 Train Acc: 0.7978\n",
      "Epoch: 710/2000 Validation Loss: 0.5935 Validation Acc: 0.6983\n",
      "Epoch: 712/2000 Train Loss: 0.5087 Train Acc: 0.7978\n",
      "Epoch: 712/2000 Validation Loss: 0.5932 Validation Acc: 0.6983\n",
      "Epoch: 714/2000 Train Loss: 0.5083 Train Acc: 0.7978\n",
      "Epoch: 714/2000 Validation Loss: 0.5928 Validation Acc: 0.6983\n",
      "Epoch: 716/2000 Train Loss: 0.5078 Train Acc: 0.7978\n",
      "Epoch: 716/2000 Validation Loss: 0.5924 Validation Acc: 0.6983\n",
      "Epoch: 719/2000 Train Loss: 0.5074 Train Acc: 0.8048\n",
      "Epoch: 719/2000 Validation Loss: 0.5921 Validation Acc: 0.6983\n",
      "Epoch: 721/2000 Train Loss: 0.5070 Train Acc: 0.8048\n",
      "Epoch: 721/2000 Validation Loss: 0.5917 Validation Acc: 0.6983\n",
      "Epoch: 723/2000 Train Loss: 0.5066 Train Acc: 0.8048\n",
      "Epoch: 723/2000 Validation Loss: 0.5913 Validation Acc: 0.6983\n",
      "Epoch: 725/2000 Train Loss: 0.5061 Train Acc: 0.8048\n",
      "Epoch: 725/2000 Validation Loss: 0.5910 Validation Acc: 0.6983\n",
      "Epoch: 728/2000 Train Loss: 0.5057 Train Acc: 0.8048\n",
      "Epoch: 728/2000 Validation Loss: 0.5906 Validation Acc: 0.6983\n",
      "Epoch: 730/2000 Train Loss: 0.5053 Train Acc: 0.8048\n",
      "Epoch: 730/2000 Validation Loss: 0.5903 Validation Acc: 0.7039\n",
      "Epoch: 732/2000 Train Loss: 0.5049 Train Acc: 0.8048\n",
      "Epoch: 732/2000 Validation Loss: 0.5899 Validation Acc: 0.6983\n",
      "Epoch: 735/2000 Train Loss: 0.5044 Train Acc: 0.8118\n",
      "Epoch: 735/2000 Validation Loss: 0.5895 Validation Acc: 0.6983\n",
      "Epoch: 737/2000 Train Loss: 0.5040 Train Acc: 0.8118\n",
      "Epoch: 737/2000 Validation Loss: 0.5892 Validation Acc: 0.6983\n",
      "Epoch: 739/2000 Train Loss: 0.5036 Train Acc: 0.8118\n",
      "Epoch: 739/2000 Validation Loss: 0.5888 Validation Acc: 0.6983\n",
      "Epoch: 741/2000 Train Loss: 0.5032 Train Acc: 0.8118\n",
      "Epoch: 741/2000 Validation Loss: 0.5885 Validation Acc: 0.6983\n",
      "Epoch: 744/2000 Train Loss: 0.5028 Train Acc: 0.8118\n",
      "Epoch: 744/2000 Validation Loss: 0.5881 Validation Acc: 0.7039\n",
      "Epoch: 746/2000 Train Loss: 0.5023 Train Acc: 0.8118\n",
      "Epoch: 746/2000 Validation Loss: 0.5878 Validation Acc: 0.7039\n",
      "Epoch: 748/2000 Train Loss: 0.5019 Train Acc: 0.8118\n",
      "Epoch: 748/2000 Validation Loss: 0.5874 Validation Acc: 0.7039\n",
      "Epoch: 750/2000 Train Loss: 0.5015 Train Acc: 0.8118\n",
      "Epoch: 750/2000 Validation Loss: 0.5871 Validation Acc: 0.7039\n",
      "Epoch: 753/2000 Train Loss: 0.5011 Train Acc: 0.8118\n",
      "Epoch: 753/2000 Validation Loss: 0.5867 Validation Acc: 0.7039\n",
      "Epoch: 755/2000 Train Loss: 0.5007 Train Acc: 0.8118\n",
      "Epoch: 755/2000 Validation Loss: 0.5864 Validation Acc: 0.7039\n",
      "Epoch: 757/2000 Train Loss: 0.5003 Train Acc: 0.8118\n",
      "Epoch: 757/2000 Validation Loss: 0.5860 Validation Acc: 0.7039\n",
      "Epoch: 760/2000 Train Loss: 0.4999 Train Acc: 0.8118\n",
      "Epoch: 760/2000 Validation Loss: 0.5857 Validation Acc: 0.7039\n",
      "Epoch: 762/2000 Train Loss: 0.4995 Train Acc: 0.8118\n",
      "Epoch: 762/2000 Validation Loss: 0.5853 Validation Acc: 0.7039\n",
      "Epoch: 764/2000 Train Loss: 0.4991 Train Acc: 0.8132\n",
      "Epoch: 764/2000 Validation Loss: 0.5850 Validation Acc: 0.7039\n",
      "Epoch: 766/2000 Train Loss: 0.4987 Train Acc: 0.8132\n",
      "Epoch: 766/2000 Validation Loss: 0.5847 Validation Acc: 0.7039\n",
      "Epoch: 769/2000 Train Loss: 0.4983 Train Acc: 0.8132\n",
      "Epoch: 769/2000 Validation Loss: 0.5843 Validation Acc: 0.7095\n",
      "Epoch: 771/2000 Train Loss: 0.4979 Train Acc: 0.8132\n",
      "Epoch: 771/2000 Validation Loss: 0.5840 Validation Acc: 0.7095\n",
      "Epoch: 773/2000 Train Loss: 0.4976 Train Acc: 0.8132\n",
      "Epoch: 773/2000 Validation Loss: 0.5837 Validation Acc: 0.7095\n",
      "Epoch: 775/2000 Train Loss: 0.4972 Train Acc: 0.8132\n",
      "Epoch: 775/2000 Validation Loss: 0.5834 Validation Acc: 0.7095\n",
      "Epoch: 778/2000 Train Loss: 0.4968 Train Acc: 0.8132\n",
      "Epoch: 778/2000 Validation Loss: 0.5830 Validation Acc: 0.7095\n",
      "Epoch: 780/2000 Train Loss: 0.4964 Train Acc: 0.8132\n",
      "Epoch: 780/2000 Validation Loss: 0.5827 Validation Acc: 0.7095\n",
      "Epoch: 782/2000 Train Loss: 0.4961 Train Acc: 0.8132\n",
      "Epoch: 782/2000 Validation Loss: 0.5824 Validation Acc: 0.7095\n",
      "Epoch: 785/2000 Train Loss: 0.4957 Train Acc: 0.8146\n",
      "Epoch: 785/2000 Validation Loss: 0.5821 Validation Acc: 0.7095\n",
      "Epoch: 787/2000 Train Loss: 0.4953 Train Acc: 0.8146\n",
      "Epoch: 787/2000 Validation Loss: 0.5817 Validation Acc: 0.7095\n",
      "Epoch: 789/2000 Train Loss: 0.4949 Train Acc: 0.8146\n",
      "Epoch: 789/2000 Validation Loss: 0.5814 Validation Acc: 0.7095\n",
      "Epoch: 791/2000 Train Loss: 0.4946 Train Acc: 0.8146\n",
      "Epoch: 791/2000 Validation Loss: 0.5811 Validation Acc: 0.7095\n",
      "Epoch: 794/2000 Train Loss: 0.4942 Train Acc: 0.8146\n",
      "Epoch: 794/2000 Validation Loss: 0.5808 Validation Acc: 0.7095\n",
      "Epoch: 796/2000 Train Loss: 0.4938 Train Acc: 0.8146\n",
      "Epoch: 796/2000 Validation Loss: 0.5805 Validation Acc: 0.7095\n",
      "Epoch: 798/2000 Train Loss: 0.4935 Train Acc: 0.8146\n",
      "Epoch: 798/2000 Validation Loss: 0.5802 Validation Acc: 0.7095\n",
      "Epoch: 800/2000 Train Loss: 0.4931 Train Acc: 0.8146\n",
      "Epoch: 800/2000 Validation Loss: 0.5799 Validation Acc: 0.7095\n",
      "Epoch: 803/2000 Train Loss: 0.4928 Train Acc: 0.8146\n",
      "Epoch: 803/2000 Validation Loss: 0.5795 Validation Acc: 0.7095\n",
      "Epoch: 805/2000 Train Loss: 0.4924 Train Acc: 0.8146\n",
      "Epoch: 805/2000 Validation Loss: 0.5792 Validation Acc: 0.7095\n",
      "Epoch: 807/2000 Train Loss: 0.4920 Train Acc: 0.8146\n",
      "Epoch: 807/2000 Validation Loss: 0.5789 Validation Acc: 0.7095\n",
      "Epoch: 810/2000 Train Loss: 0.4917 Train Acc: 0.8146\n",
      "Epoch: 810/2000 Validation Loss: 0.5786 Validation Acc: 0.7095\n",
      "Epoch: 812/2000 Train Loss: 0.4913 Train Acc: 0.8174\n",
      "Epoch: 812/2000 Validation Loss: 0.5783 Validation Acc: 0.7095\n",
      "Epoch: 814/2000 Train Loss: 0.4910 Train Acc: 0.8174\n",
      "Epoch: 814/2000 Validation Loss: 0.5780 Validation Acc: 0.7095\n",
      "Epoch: 816/2000 Train Loss: 0.4906 Train Acc: 0.8174\n",
      "Epoch: 816/2000 Validation Loss: 0.5777 Validation Acc: 0.7095\n",
      "Epoch: 819/2000 Train Loss: 0.4903 Train Acc: 0.8174\n",
      "Epoch: 819/2000 Validation Loss: 0.5774 Validation Acc: 0.7095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 821/2000 Train Loss: 0.4899 Train Acc: 0.8174\n",
      "Epoch: 821/2000 Validation Loss: 0.5771 Validation Acc: 0.7095\n",
      "Epoch: 823/2000 Train Loss: 0.4896 Train Acc: 0.8174\n",
      "Epoch: 823/2000 Validation Loss: 0.5768 Validation Acc: 0.7095\n",
      "Epoch: 825/2000 Train Loss: 0.4892 Train Acc: 0.8174\n",
      "Epoch: 825/2000 Validation Loss: 0.5765 Validation Acc: 0.7095\n",
      "Epoch: 828/2000 Train Loss: 0.4889 Train Acc: 0.8174\n",
      "Epoch: 828/2000 Validation Loss: 0.5762 Validation Acc: 0.7095\n",
      "Epoch: 830/2000 Train Loss: 0.4885 Train Acc: 0.8174\n",
      "Epoch: 830/2000 Validation Loss: 0.5759 Validation Acc: 0.7095\n",
      "Epoch: 832/2000 Train Loss: 0.4882 Train Acc: 0.8174\n",
      "Epoch: 832/2000 Validation Loss: 0.5756 Validation Acc: 0.7095\n",
      "Epoch: 835/2000 Train Loss: 0.4878 Train Acc: 0.8174\n",
      "Epoch: 835/2000 Validation Loss: 0.5753 Validation Acc: 0.7095\n",
      "Epoch: 837/2000 Train Loss: 0.4875 Train Acc: 0.8174\n",
      "Epoch: 837/2000 Validation Loss: 0.5750 Validation Acc: 0.7095\n",
      "Epoch: 839/2000 Train Loss: 0.4872 Train Acc: 0.8174\n",
      "Epoch: 839/2000 Validation Loss: 0.5747 Validation Acc: 0.7095\n",
      "Epoch: 841/2000 Train Loss: 0.4868 Train Acc: 0.8174\n",
      "Epoch: 841/2000 Validation Loss: 0.5744 Validation Acc: 0.7095\n",
      "Epoch: 844/2000 Train Loss: 0.4865 Train Acc: 0.8174\n",
      "Epoch: 844/2000 Validation Loss: 0.5741 Validation Acc: 0.7095\n",
      "Epoch: 846/2000 Train Loss: 0.4862 Train Acc: 0.8174\n",
      "Epoch: 846/2000 Validation Loss: 0.5738 Validation Acc: 0.7095\n",
      "Epoch: 848/2000 Train Loss: 0.4858 Train Acc: 0.8174\n",
      "Epoch: 848/2000 Validation Loss: 0.5735 Validation Acc: 0.7095\n",
      "Epoch: 850/2000 Train Loss: 0.4855 Train Acc: 0.8174\n",
      "Epoch: 850/2000 Validation Loss: 0.5733 Validation Acc: 0.7095\n",
      "Epoch: 853/2000 Train Loss: 0.4852 Train Acc: 0.8174\n",
      "Epoch: 853/2000 Validation Loss: 0.5730 Validation Acc: 0.7095\n",
      "Epoch: 855/2000 Train Loss: 0.4849 Train Acc: 0.8174\n",
      "Epoch: 855/2000 Validation Loss: 0.5727 Validation Acc: 0.7095\n",
      "Epoch: 857/2000 Train Loss: 0.4845 Train Acc: 0.8174\n",
      "Epoch: 857/2000 Validation Loss: 0.5724 Validation Acc: 0.7095\n",
      "Epoch: 860/2000 Train Loss: 0.4842 Train Acc: 0.8174\n",
      "Epoch: 860/2000 Validation Loss: 0.5721 Validation Acc: 0.7151\n",
      "Epoch: 862/2000 Train Loss: 0.4839 Train Acc: 0.8216\n",
      "Epoch: 862/2000 Validation Loss: 0.5718 Validation Acc: 0.7151\n",
      "Epoch: 864/2000 Train Loss: 0.4835 Train Acc: 0.8216\n",
      "Epoch: 864/2000 Validation Loss: 0.5716 Validation Acc: 0.7151\n",
      "Epoch: 866/2000 Train Loss: 0.4832 Train Acc: 0.8216\n",
      "Epoch: 866/2000 Validation Loss: 0.5713 Validation Acc: 0.7151\n",
      "Epoch: 869/2000 Train Loss: 0.4829 Train Acc: 0.8216\n",
      "Epoch: 869/2000 Validation Loss: 0.5710 Validation Acc: 0.7151\n",
      "Epoch: 871/2000 Train Loss: 0.4825 Train Acc: 0.8230\n",
      "Epoch: 871/2000 Validation Loss: 0.5707 Validation Acc: 0.7151\n",
      "Epoch: 873/2000 Train Loss: 0.4822 Train Acc: 0.8230\n",
      "Epoch: 873/2000 Validation Loss: 0.5704 Validation Acc: 0.7151\n",
      "Epoch: 875/2000 Train Loss: 0.4819 Train Acc: 0.8230\n",
      "Epoch: 875/2000 Validation Loss: 0.5702 Validation Acc: 0.7151\n",
      "Epoch: 878/2000 Train Loss: 0.4816 Train Acc: 0.8230\n",
      "Epoch: 878/2000 Validation Loss: 0.5699 Validation Acc: 0.7151\n",
      "Epoch: 880/2000 Train Loss: 0.4813 Train Acc: 0.8244\n",
      "Epoch: 880/2000 Validation Loss: 0.5696 Validation Acc: 0.7151\n",
      "Epoch: 882/2000 Train Loss: 0.4809 Train Acc: 0.8244\n",
      "Epoch: 882/2000 Validation Loss: 0.5693 Validation Acc: 0.7151\n",
      "Epoch: 885/2000 Train Loss: 0.4806 Train Acc: 0.8244\n",
      "Epoch: 885/2000 Validation Loss: 0.5691 Validation Acc: 0.7151\n",
      "Epoch: 887/2000 Train Loss: 0.4803 Train Acc: 0.8244\n",
      "Epoch: 887/2000 Validation Loss: 0.5688 Validation Acc: 0.7151\n",
      "Epoch: 889/2000 Train Loss: 0.4800 Train Acc: 0.8244\n",
      "Epoch: 889/2000 Validation Loss: 0.5685 Validation Acc: 0.7151\n",
      "Epoch: 891/2000 Train Loss: 0.4797 Train Acc: 0.8244\n",
      "Epoch: 891/2000 Validation Loss: 0.5683 Validation Acc: 0.7151\n",
      "Epoch: 894/2000 Train Loss: 0.4794 Train Acc: 0.8244\n",
      "Epoch: 894/2000 Validation Loss: 0.5680 Validation Acc: 0.7151\n",
      "Epoch: 896/2000 Train Loss: 0.4790 Train Acc: 0.8244\n",
      "Epoch: 896/2000 Validation Loss: 0.5677 Validation Acc: 0.7151\n",
      "Epoch: 898/2000 Train Loss: 0.4787 Train Acc: 0.8244\n",
      "Epoch: 898/2000 Validation Loss: 0.5674 Validation Acc: 0.7151\n",
      "Epoch: 900/2000 Train Loss: 0.4784 Train Acc: 0.8244\n",
      "Epoch: 900/2000 Validation Loss: 0.5672 Validation Acc: 0.7151\n",
      "Epoch: 903/2000 Train Loss: 0.4781 Train Acc: 0.8244\n",
      "Epoch: 903/2000 Validation Loss: 0.5669 Validation Acc: 0.7151\n",
      "Epoch: 905/2000 Train Loss: 0.4778 Train Acc: 0.8244\n",
      "Epoch: 905/2000 Validation Loss: 0.5667 Validation Acc: 0.7151\n",
      "Epoch: 907/2000 Train Loss: 0.4775 Train Acc: 0.8188\n",
      "Epoch: 907/2000 Validation Loss: 0.5664 Validation Acc: 0.7151\n",
      "Epoch: 910/2000 Train Loss: 0.4772 Train Acc: 0.8188\n",
      "Epoch: 910/2000 Validation Loss: 0.5661 Validation Acc: 0.7151\n",
      "Epoch: 912/2000 Train Loss: 0.4769 Train Acc: 0.8188\n",
      "Epoch: 912/2000 Validation Loss: 0.5659 Validation Acc: 0.7151\n",
      "Epoch: 914/2000 Train Loss: 0.4766 Train Acc: 0.8188\n",
      "Epoch: 914/2000 Validation Loss: 0.5656 Validation Acc: 0.7151\n",
      "Epoch: 916/2000 Train Loss: 0.4763 Train Acc: 0.8188\n",
      "Epoch: 916/2000 Validation Loss: 0.5654 Validation Acc: 0.7151\n",
      "Epoch: 919/2000 Train Loss: 0.4760 Train Acc: 0.8188\n",
      "Epoch: 919/2000 Validation Loss: 0.5651 Validation Acc: 0.7151\n",
      "Epoch: 921/2000 Train Loss: 0.4757 Train Acc: 0.8188\n",
      "Epoch: 921/2000 Validation Loss: 0.5648 Validation Acc: 0.7095\n",
      "Epoch: 923/2000 Train Loss: 0.4754 Train Acc: 0.8188\n",
      "Epoch: 923/2000 Validation Loss: 0.5646 Validation Acc: 0.7095\n",
      "Epoch: 925/2000 Train Loss: 0.4751 Train Acc: 0.8188\n",
      "Epoch: 925/2000 Validation Loss: 0.5643 Validation Acc: 0.7095\n",
      "Epoch: 928/2000 Train Loss: 0.4748 Train Acc: 0.8188\n",
      "Epoch: 928/2000 Validation Loss: 0.5641 Validation Acc: 0.7151\n",
      "Epoch: 930/2000 Train Loss: 0.4746 Train Acc: 0.8174\n",
      "Epoch: 930/2000 Validation Loss: 0.5638 Validation Acc: 0.7151\n",
      "Epoch: 932/2000 Train Loss: 0.4743 Train Acc: 0.8174\n",
      "Epoch: 932/2000 Validation Loss: 0.5636 Validation Acc: 0.7151\n",
      "Epoch: 935/2000 Train Loss: 0.4740 Train Acc: 0.8174\n",
      "Epoch: 935/2000 Validation Loss: 0.5633 Validation Acc: 0.7151\n",
      "Epoch: 937/2000 Train Loss: 0.4737 Train Acc: 0.8174\n",
      "Epoch: 937/2000 Validation Loss: 0.5631 Validation Acc: 0.7151\n",
      "Epoch: 939/2000 Train Loss: 0.4734 Train Acc: 0.8174\n",
      "Epoch: 939/2000 Validation Loss: 0.5629 Validation Acc: 0.7151\n",
      "Epoch: 941/2000 Train Loss: 0.4731 Train Acc: 0.8174\n",
      "Epoch: 941/2000 Validation Loss: 0.5626 Validation Acc: 0.7151\n",
      "Epoch: 944/2000 Train Loss: 0.4728 Train Acc: 0.8174\n",
      "Epoch: 944/2000 Validation Loss: 0.5624 Validation Acc: 0.7151\n",
      "Epoch: 946/2000 Train Loss: 0.4726 Train Acc: 0.8174\n",
      "Epoch: 946/2000 Validation Loss: 0.5621 Validation Acc: 0.7151\n",
      "Epoch: 948/2000 Train Loss: 0.4723 Train Acc: 0.8174\n",
      "Epoch: 948/2000 Validation Loss: 0.5619 Validation Acc: 0.7151\n",
      "Epoch: 950/2000 Train Loss: 0.4720 Train Acc: 0.8174\n",
      "Epoch: 950/2000 Validation Loss: 0.5616 Validation Acc: 0.7151\n",
      "Epoch: 953/2000 Train Loss: 0.4717 Train Acc: 0.8174\n",
      "Epoch: 953/2000 Validation Loss: 0.5614 Validation Acc: 0.7151\n",
      "Epoch: 955/2000 Train Loss: 0.4715 Train Acc: 0.8174\n",
      "Epoch: 955/2000 Validation Loss: 0.5612 Validation Acc: 0.7151\n",
      "Epoch: 957/2000 Train Loss: 0.4712 Train Acc: 0.8174\n",
      "Epoch: 957/2000 Validation Loss: 0.5609 Validation Acc: 0.7151\n",
      "Epoch: 960/2000 Train Loss: 0.4709 Train Acc: 0.8174\n",
      "Epoch: 960/2000 Validation Loss: 0.5607 Validation Acc: 0.7151\n",
      "Epoch: 962/2000 Train Loss: 0.4706 Train Acc: 0.8174\n",
      "Epoch: 962/2000 Validation Loss: 0.5605 Validation Acc: 0.7151\n",
      "Epoch: 964/2000 Train Loss: 0.4704 Train Acc: 0.8174\n",
      "Epoch: 964/2000 Validation Loss: 0.5602 Validation Acc: 0.7151\n",
      "Epoch: 966/2000 Train Loss: 0.4701 Train Acc: 0.8174\n",
      "Epoch: 966/2000 Validation Loss: 0.5600 Validation Acc: 0.7151\n",
      "Epoch: 969/2000 Train Loss: 0.4698 Train Acc: 0.8188\n",
      "Epoch: 969/2000 Validation Loss: 0.5598 Validation Acc: 0.7486\n",
      "Epoch: 971/2000 Train Loss: 0.4695 Train Acc: 0.8188\n",
      "Epoch: 971/2000 Validation Loss: 0.5595 Validation Acc: 0.7486\n",
      "Epoch: 973/2000 Train Loss: 0.4693 Train Acc: 0.8188\n",
      "Epoch: 973/2000 Validation Loss: 0.5593 Validation Acc: 0.7486\n",
      "Epoch: 975/2000 Train Loss: 0.4690 Train Acc: 0.8188\n",
      "Epoch: 975/2000 Validation Loss: 0.5591 Validation Acc: 0.7486\n",
      "Epoch: 978/2000 Train Loss: 0.4687 Train Acc: 0.8188\n",
      "Epoch: 978/2000 Validation Loss: 0.5588 Validation Acc: 0.7486\n",
      "Epoch: 980/2000 Train Loss: 0.4685 Train Acc: 0.8188\n",
      "Epoch: 980/2000 Validation Loss: 0.5586 Validation Acc: 0.7486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 982/2000 Train Loss: 0.4682 Train Acc: 0.8188\n",
      "Epoch: 982/2000 Validation Loss: 0.5584 Validation Acc: 0.7486\n",
      "Epoch: 985/2000 Train Loss: 0.4679 Train Acc: 0.8188\n",
      "Epoch: 985/2000 Validation Loss: 0.5581 Validation Acc: 0.7486\n",
      "Epoch: 987/2000 Train Loss: 0.4677 Train Acc: 0.8188\n",
      "Epoch: 987/2000 Validation Loss: 0.5579 Validation Acc: 0.7486\n",
      "Epoch: 989/2000 Train Loss: 0.4674 Train Acc: 0.8188\n",
      "Epoch: 989/2000 Validation Loss: 0.5577 Validation Acc: 0.7486\n",
      "Epoch: 991/2000 Train Loss: 0.4672 Train Acc: 0.8188\n",
      "Epoch: 991/2000 Validation Loss: 0.5575 Validation Acc: 0.7486\n",
      "Epoch: 994/2000 Train Loss: 0.4669 Train Acc: 0.8188\n",
      "Epoch: 994/2000 Validation Loss: 0.5573 Validation Acc: 0.7486\n",
      "Epoch: 996/2000 Train Loss: 0.4666 Train Acc: 0.8188\n",
      "Epoch: 996/2000 Validation Loss: 0.5570 Validation Acc: 0.7486\n",
      "Epoch: 998/2000 Train Loss: 0.4664 Train Acc: 0.8188\n",
      "Epoch: 998/2000 Validation Loss: 0.5568 Validation Acc: 0.7486\n",
      "Epoch: 1000/2000 Train Loss: 0.4661 Train Acc: 0.8188\n",
      "Epoch: 1000/2000 Validation Loss: 0.5566 Validation Acc: 0.7486\n",
      "Epoch: 1003/2000 Train Loss: 0.4659 Train Acc: 0.8188\n",
      "Epoch: 1003/2000 Validation Loss: 0.5564 Validation Acc: 0.7486\n",
      "Epoch: 1005/2000 Train Loss: 0.4656 Train Acc: 0.8188\n",
      "Epoch: 1005/2000 Validation Loss: 0.5562 Validation Acc: 0.7486\n",
      "Epoch: 1007/2000 Train Loss: 0.4654 Train Acc: 0.8188\n",
      "Epoch: 1007/2000 Validation Loss: 0.5559 Validation Acc: 0.7486\n",
      "Epoch: 1010/2000 Train Loss: 0.4651 Train Acc: 0.8188\n",
      "Epoch: 1010/2000 Validation Loss: 0.5557 Validation Acc: 0.7486\n",
      "Epoch: 1012/2000 Train Loss: 0.4648 Train Acc: 0.8188\n",
      "Epoch: 1012/2000 Validation Loss: 0.5555 Validation Acc: 0.7486\n",
      "Epoch: 1014/2000 Train Loss: 0.4646 Train Acc: 0.8188\n",
      "Epoch: 1014/2000 Validation Loss: 0.5553 Validation Acc: 0.7486\n",
      "Epoch: 1016/2000 Train Loss: 0.4643 Train Acc: 0.8188\n",
      "Epoch: 1016/2000 Validation Loss: 0.5551 Validation Acc: 0.7486\n",
      "Epoch: 1019/2000 Train Loss: 0.4641 Train Acc: 0.8188\n",
      "Epoch: 1019/2000 Validation Loss: 0.5549 Validation Acc: 0.7486\n",
      "Epoch: 1021/2000 Train Loss: 0.4638 Train Acc: 0.8188\n",
      "Epoch: 1021/2000 Validation Loss: 0.5547 Validation Acc: 0.7486\n",
      "Epoch: 1023/2000 Train Loss: 0.4636 Train Acc: 0.8188\n",
      "Epoch: 1023/2000 Validation Loss: 0.5544 Validation Acc: 0.7486\n",
      "Epoch: 1025/2000 Train Loss: 0.4634 Train Acc: 0.8188\n",
      "Epoch: 1025/2000 Validation Loss: 0.5542 Validation Acc: 0.7486\n",
      "Epoch: 1028/2000 Train Loss: 0.4631 Train Acc: 0.8188\n",
      "Epoch: 1028/2000 Validation Loss: 0.5540 Validation Acc: 0.7486\n",
      "Epoch: 1030/2000 Train Loss: 0.4629 Train Acc: 0.8188\n",
      "Epoch: 1030/2000 Validation Loss: 0.5538 Validation Acc: 0.7486\n",
      "Epoch: 1032/2000 Train Loss: 0.4626 Train Acc: 0.8188\n",
      "Epoch: 1032/2000 Validation Loss: 0.5536 Validation Acc: 0.7486\n",
      "Epoch: 1035/2000 Train Loss: 0.4624 Train Acc: 0.8188\n",
      "Epoch: 1035/2000 Validation Loss: 0.5534 Validation Acc: 0.7486\n",
      "Epoch: 1037/2000 Train Loss: 0.4621 Train Acc: 0.8188\n",
      "Epoch: 1037/2000 Validation Loss: 0.5532 Validation Acc: 0.7486\n",
      "Epoch: 1039/2000 Train Loss: 0.4619 Train Acc: 0.8188\n",
      "Epoch: 1039/2000 Validation Loss: 0.5530 Validation Acc: 0.7486\n",
      "Epoch: 1041/2000 Train Loss: 0.4617 Train Acc: 0.8188\n",
      "Epoch: 1041/2000 Validation Loss: 0.5528 Validation Acc: 0.7486\n",
      "Epoch: 1044/2000 Train Loss: 0.4614 Train Acc: 0.8188\n",
      "Epoch: 1044/2000 Validation Loss: 0.5526 Validation Acc: 0.7486\n",
      "Epoch: 1046/2000 Train Loss: 0.4612 Train Acc: 0.8188\n",
      "Epoch: 1046/2000 Validation Loss: 0.5524 Validation Acc: 0.7486\n",
      "Epoch: 1048/2000 Train Loss: 0.4609 Train Acc: 0.8188\n",
      "Epoch: 1048/2000 Validation Loss: 0.5522 Validation Acc: 0.7486\n",
      "Epoch: 1050/2000 Train Loss: 0.4607 Train Acc: 0.8188\n",
      "Epoch: 1050/2000 Validation Loss: 0.5520 Validation Acc: 0.7486\n",
      "Epoch: 1053/2000 Train Loss: 0.4605 Train Acc: 0.8188\n",
      "Epoch: 1053/2000 Validation Loss: 0.5518 Validation Acc: 0.7486\n",
      "Epoch: 1055/2000 Train Loss: 0.4602 Train Acc: 0.8188\n",
      "Epoch: 1055/2000 Validation Loss: 0.5516 Validation Acc: 0.7486\n",
      "Epoch: 1057/2000 Train Loss: 0.4600 Train Acc: 0.8188\n",
      "Epoch: 1057/2000 Validation Loss: 0.5514 Validation Acc: 0.7486\n",
      "Epoch: 1060/2000 Train Loss: 0.4597 Train Acc: 0.8188\n",
      "Epoch: 1060/2000 Validation Loss: 0.5512 Validation Acc: 0.7486\n",
      "Epoch: 1062/2000 Train Loss: 0.4595 Train Acc: 0.8188\n",
      "Epoch: 1062/2000 Validation Loss: 0.5510 Validation Acc: 0.7486\n",
      "Epoch: 1064/2000 Train Loss: 0.4593 Train Acc: 0.8188\n",
      "Epoch: 1064/2000 Validation Loss: 0.5508 Validation Acc: 0.7486\n",
      "Epoch: 1066/2000 Train Loss: 0.4590 Train Acc: 0.8188\n",
      "Epoch: 1066/2000 Validation Loss: 0.5506 Validation Acc: 0.7486\n",
      "Epoch: 1069/2000 Train Loss: 0.4588 Train Acc: 0.8188\n",
      "Epoch: 1069/2000 Validation Loss: 0.5503 Validation Acc: 0.7486\n",
      "Epoch: 1071/2000 Train Loss: 0.4585 Train Acc: 0.8188\n",
      "Epoch: 1071/2000 Validation Loss: 0.5501 Validation Acc: 0.7486\n",
      "Epoch: 1073/2000 Train Loss: 0.4583 Train Acc: 0.8188\n",
      "Epoch: 1073/2000 Validation Loss: 0.5499 Validation Acc: 0.7486\n",
      "Epoch: 1075/2000 Train Loss: 0.4580 Train Acc: 0.8188\n",
      "Epoch: 1075/2000 Validation Loss: 0.5497 Validation Acc: 0.7486\n",
      "Epoch: 1078/2000 Train Loss: 0.4578 Train Acc: 0.8188\n",
      "Epoch: 1078/2000 Validation Loss: 0.5495 Validation Acc: 0.7486\n",
      "Epoch: 1080/2000 Train Loss: 0.4575 Train Acc: 0.8188\n",
      "Epoch: 1080/2000 Validation Loss: 0.5493 Validation Acc: 0.7486\n",
      "Epoch: 1082/2000 Train Loss: 0.4573 Train Acc: 0.8188\n",
      "Epoch: 1082/2000 Validation Loss: 0.5491 Validation Acc: 0.7486\n",
      "Epoch: 1085/2000 Train Loss: 0.4570 Train Acc: 0.8188\n",
      "Epoch: 1085/2000 Validation Loss: 0.5489 Validation Acc: 0.7486\n",
      "Epoch: 1087/2000 Train Loss: 0.4568 Train Acc: 0.8188\n",
      "Epoch: 1087/2000 Validation Loss: 0.5487 Validation Acc: 0.7486\n",
      "Epoch: 1089/2000 Train Loss: 0.4565 Train Acc: 0.8188\n",
      "Epoch: 1089/2000 Validation Loss: 0.5485 Validation Acc: 0.7486\n",
      "Epoch: 1091/2000 Train Loss: 0.4563 Train Acc: 0.8188\n",
      "Epoch: 1091/2000 Validation Loss: 0.5483 Validation Acc: 0.7486\n",
      "Epoch: 1094/2000 Train Loss: 0.4560 Train Acc: 0.8188\n",
      "Epoch: 1094/2000 Validation Loss: 0.5481 Validation Acc: 0.7486\n",
      "Epoch: 1096/2000 Train Loss: 0.4558 Train Acc: 0.8188\n",
      "Epoch: 1096/2000 Validation Loss: 0.5479 Validation Acc: 0.7486\n",
      "Epoch: 1098/2000 Train Loss: 0.4555 Train Acc: 0.8188\n",
      "Epoch: 1098/2000 Validation Loss: 0.5477 Validation Acc: 0.7430\n",
      "Epoch: 1100/2000 Train Loss: 0.4553 Train Acc: 0.8188\n",
      "Epoch: 1100/2000 Validation Loss: 0.5475 Validation Acc: 0.7430\n",
      "Epoch: 1103/2000 Train Loss: 0.4551 Train Acc: 0.8188\n",
      "Epoch: 1103/2000 Validation Loss: 0.5473 Validation Acc: 0.7430\n",
      "Epoch: 1105/2000 Train Loss: 0.4548 Train Acc: 0.8202\n",
      "Epoch: 1105/2000 Validation Loss: 0.5471 Validation Acc: 0.7430\n",
      "Epoch: 1107/2000 Train Loss: 0.4546 Train Acc: 0.8202\n",
      "Epoch: 1107/2000 Validation Loss: 0.5469 Validation Acc: 0.7430\n",
      "Epoch: 1110/2000 Train Loss: 0.4544 Train Acc: 0.8202\n",
      "Epoch: 1110/2000 Validation Loss: 0.5467 Validation Acc: 0.7430\n",
      "Epoch: 1112/2000 Train Loss: 0.4541 Train Acc: 0.8202\n",
      "Epoch: 1112/2000 Validation Loss: 0.5465 Validation Acc: 0.7430\n",
      "Epoch: 1114/2000 Train Loss: 0.4539 Train Acc: 0.8202\n",
      "Epoch: 1114/2000 Validation Loss: 0.5463 Validation Acc: 0.7486\n",
      "Epoch: 1116/2000 Train Loss: 0.4537 Train Acc: 0.8202\n",
      "Epoch: 1116/2000 Validation Loss: 0.5461 Validation Acc: 0.7486\n",
      "Epoch: 1119/2000 Train Loss: 0.4534 Train Acc: 0.8202\n",
      "Epoch: 1119/2000 Validation Loss: 0.5459 Validation Acc: 0.7486\n",
      "Epoch: 1121/2000 Train Loss: 0.4532 Train Acc: 0.8202\n",
      "Epoch: 1121/2000 Validation Loss: 0.5457 Validation Acc: 0.7486\n",
      "Epoch: 1123/2000 Train Loss: 0.4530 Train Acc: 0.8202\n",
      "Epoch: 1123/2000 Validation Loss: 0.5455 Validation Acc: 0.7486\n",
      "Epoch: 1125/2000 Train Loss: 0.4527 Train Acc: 0.8202\n",
      "Epoch: 1125/2000 Validation Loss: 0.5454 Validation Acc: 0.7486\n",
      "Epoch: 1128/2000 Train Loss: 0.4525 Train Acc: 0.8202\n",
      "Epoch: 1128/2000 Validation Loss: 0.5452 Validation Acc: 0.7486\n",
      "Epoch: 1130/2000 Train Loss: 0.4523 Train Acc: 0.8202\n",
      "Epoch: 1130/2000 Validation Loss: 0.5450 Validation Acc: 0.7486\n",
      "Epoch: 1132/2000 Train Loss: 0.4521 Train Acc: 0.8202\n",
      "Epoch: 1132/2000 Validation Loss: 0.5448 Validation Acc: 0.7486\n",
      "Epoch: 1135/2000 Train Loss: 0.4518 Train Acc: 0.8202\n",
      "Epoch: 1135/2000 Validation Loss: 0.5446 Validation Acc: 0.7486\n",
      "Epoch: 1137/2000 Train Loss: 0.4516 Train Acc: 0.8202\n",
      "Epoch: 1137/2000 Validation Loss: 0.5444 Validation Acc: 0.7486\n",
      "Epoch: 1139/2000 Train Loss: 0.4514 Train Acc: 0.8202\n",
      "Epoch: 1139/2000 Validation Loss: 0.5443 Validation Acc: 0.7486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1141/2000 Train Loss: 0.4512 Train Acc: 0.8202\n",
      "Epoch: 1141/2000 Validation Loss: 0.5441 Validation Acc: 0.7486\n",
      "Epoch: 1144/2000 Train Loss: 0.4510 Train Acc: 0.8202\n",
      "Epoch: 1144/2000 Validation Loss: 0.5439 Validation Acc: 0.7486\n",
      "Epoch: 1146/2000 Train Loss: 0.4508 Train Acc: 0.8202\n",
      "Epoch: 1146/2000 Validation Loss: 0.5437 Validation Acc: 0.7486\n",
      "Epoch: 1148/2000 Train Loss: 0.4505 Train Acc: 0.8202\n",
      "Epoch: 1148/2000 Validation Loss: 0.5435 Validation Acc: 0.7486\n",
      "Epoch: 1150/2000 Train Loss: 0.4503 Train Acc: 0.8202\n",
      "Epoch: 1150/2000 Validation Loss: 0.5434 Validation Acc: 0.7486\n",
      "Epoch: 1153/2000 Train Loss: 0.4501 Train Acc: 0.8188\n",
      "Epoch: 1153/2000 Validation Loss: 0.5432 Validation Acc: 0.7486\n",
      "Epoch: 1155/2000 Train Loss: 0.4499 Train Acc: 0.8188\n",
      "Epoch: 1155/2000 Validation Loss: 0.5430 Validation Acc: 0.7486\n",
      "Epoch: 1157/2000 Train Loss: 0.4497 Train Acc: 0.8188\n",
      "Epoch: 1157/2000 Validation Loss: 0.5428 Validation Acc: 0.7486\n",
      "Epoch: 1160/2000 Train Loss: 0.4495 Train Acc: 0.8188\n",
      "Epoch: 1160/2000 Validation Loss: 0.5427 Validation Acc: 0.7486\n",
      "Epoch: 1162/2000 Train Loss: 0.4493 Train Acc: 0.8188\n",
      "Epoch: 1162/2000 Validation Loss: 0.5425 Validation Acc: 0.7486\n",
      "Epoch: 1164/2000 Train Loss: 0.4491 Train Acc: 0.8188\n",
      "Epoch: 1164/2000 Validation Loss: 0.5423 Validation Acc: 0.7486\n",
      "Epoch: 1166/2000 Train Loss: 0.4489 Train Acc: 0.8188\n",
      "Epoch: 1166/2000 Validation Loss: 0.5422 Validation Acc: 0.7486\n",
      "Epoch: 1169/2000 Train Loss: 0.4487 Train Acc: 0.8188\n",
      "Epoch: 1169/2000 Validation Loss: 0.5420 Validation Acc: 0.7486\n",
      "Epoch: 1171/2000 Train Loss: 0.4485 Train Acc: 0.8188\n",
      "Epoch: 1171/2000 Validation Loss: 0.5418 Validation Acc: 0.7486\n",
      "Epoch: 1173/2000 Train Loss: 0.4483 Train Acc: 0.8188\n",
      "Epoch: 1173/2000 Validation Loss: 0.5417 Validation Acc: 0.7486\n",
      "Epoch: 1175/2000 Train Loss: 0.4481 Train Acc: 0.8188\n",
      "Epoch: 1175/2000 Validation Loss: 0.5415 Validation Acc: 0.7486\n",
      "Epoch: 1178/2000 Train Loss: 0.4479 Train Acc: 0.8188\n",
      "Epoch: 1178/2000 Validation Loss: 0.5413 Validation Acc: 0.7486\n",
      "Epoch: 1180/2000 Train Loss: 0.4477 Train Acc: 0.8188\n",
      "Epoch: 1180/2000 Validation Loss: 0.5412 Validation Acc: 0.7486\n",
      "Epoch: 1182/2000 Train Loss: 0.4475 Train Acc: 0.8188\n",
      "Epoch: 1182/2000 Validation Loss: 0.5410 Validation Acc: 0.7486\n",
      "Epoch: 1185/2000 Train Loss: 0.4473 Train Acc: 0.8188\n",
      "Epoch: 1185/2000 Validation Loss: 0.5408 Validation Acc: 0.7486\n",
      "Epoch: 1187/2000 Train Loss: 0.4471 Train Acc: 0.8188\n",
      "Epoch: 1187/2000 Validation Loss: 0.5407 Validation Acc: 0.7486\n",
      "Epoch: 1189/2000 Train Loss: 0.4469 Train Acc: 0.8188\n",
      "Epoch: 1189/2000 Validation Loss: 0.5405 Validation Acc: 0.7486\n",
      "Epoch: 1191/2000 Train Loss: 0.4467 Train Acc: 0.8188\n",
      "Epoch: 1191/2000 Validation Loss: 0.5404 Validation Acc: 0.7486\n",
      "Epoch: 1194/2000 Train Loss: 0.4465 Train Acc: 0.8174\n",
      "Epoch: 1194/2000 Validation Loss: 0.5402 Validation Acc: 0.7486\n",
      "Epoch: 1196/2000 Train Loss: 0.4463 Train Acc: 0.8174\n",
      "Epoch: 1196/2000 Validation Loss: 0.5400 Validation Acc: 0.7486\n",
      "Epoch: 1198/2000 Train Loss: 0.4461 Train Acc: 0.8174\n",
      "Epoch: 1198/2000 Validation Loss: 0.5399 Validation Acc: 0.7486\n",
      "Epoch: 1200/2000 Train Loss: 0.4459 Train Acc: 0.8174\n",
      "Epoch: 1200/2000 Validation Loss: 0.5397 Validation Acc: 0.7486\n",
      "Epoch: 1203/2000 Train Loss: 0.4457 Train Acc: 0.8174\n",
      "Epoch: 1203/2000 Validation Loss: 0.5396 Validation Acc: 0.7486\n",
      "Epoch: 1205/2000 Train Loss: 0.4455 Train Acc: 0.8174\n",
      "Epoch: 1205/2000 Validation Loss: 0.5394 Validation Acc: 0.7486\n",
      "Epoch: 1207/2000 Train Loss: 0.4453 Train Acc: 0.8174\n",
      "Epoch: 1207/2000 Validation Loss: 0.5393 Validation Acc: 0.7486\n",
      "Epoch: 1210/2000 Train Loss: 0.4452 Train Acc: 0.8174\n",
      "Epoch: 1210/2000 Validation Loss: 0.5391 Validation Acc: 0.7486\n",
      "Epoch: 1212/2000 Train Loss: 0.4450 Train Acc: 0.8174\n",
      "Epoch: 1212/2000 Validation Loss: 0.5390 Validation Acc: 0.7486\n",
      "Epoch: 1214/2000 Train Loss: 0.4448 Train Acc: 0.8174\n",
      "Epoch: 1214/2000 Validation Loss: 0.5388 Validation Acc: 0.7486\n",
      "Epoch: 1216/2000 Train Loss: 0.4446 Train Acc: 0.8174\n",
      "Epoch: 1216/2000 Validation Loss: 0.5387 Validation Acc: 0.7486\n",
      "Epoch: 1219/2000 Train Loss: 0.4444 Train Acc: 0.8174\n",
      "Epoch: 1219/2000 Validation Loss: 0.5385 Validation Acc: 0.7486\n",
      "Epoch: 1221/2000 Train Loss: 0.4442 Train Acc: 0.8174\n",
      "Epoch: 1221/2000 Validation Loss: 0.5384 Validation Acc: 0.7486\n",
      "Epoch: 1223/2000 Train Loss: 0.4440 Train Acc: 0.8174\n",
      "Epoch: 1223/2000 Validation Loss: 0.5382 Validation Acc: 0.7486\n",
      "Epoch: 1225/2000 Train Loss: 0.4439 Train Acc: 0.8174\n",
      "Epoch: 1225/2000 Validation Loss: 0.5381 Validation Acc: 0.7486\n",
      "Epoch: 1228/2000 Train Loss: 0.4437 Train Acc: 0.8174\n",
      "Epoch: 1228/2000 Validation Loss: 0.5379 Validation Acc: 0.7486\n",
      "Epoch: 1230/2000 Train Loss: 0.4435 Train Acc: 0.8188\n",
      "Epoch: 1230/2000 Validation Loss: 0.5378 Validation Acc: 0.7486\n",
      "Epoch: 1232/2000 Train Loss: 0.4433 Train Acc: 0.8188\n",
      "Epoch: 1232/2000 Validation Loss: 0.5376 Validation Acc: 0.7486\n",
      "Epoch: 1235/2000 Train Loss: 0.4431 Train Acc: 0.8188\n",
      "Epoch: 1235/2000 Validation Loss: 0.5375 Validation Acc: 0.7486\n",
      "Epoch: 1237/2000 Train Loss: 0.4429 Train Acc: 0.8188\n",
      "Epoch: 1237/2000 Validation Loss: 0.5373 Validation Acc: 0.7486\n",
      "Epoch: 1239/2000 Train Loss: 0.4428 Train Acc: 0.8188\n",
      "Epoch: 1239/2000 Validation Loss: 0.5372 Validation Acc: 0.7486\n",
      "Epoch: 1241/2000 Train Loss: 0.4426 Train Acc: 0.8188\n",
      "Epoch: 1241/2000 Validation Loss: 0.5370 Validation Acc: 0.7486\n",
      "Epoch: 1244/2000 Train Loss: 0.4424 Train Acc: 0.8188\n",
      "Epoch: 1244/2000 Validation Loss: 0.5369 Validation Acc: 0.7486\n",
      "Epoch: 1246/2000 Train Loss: 0.4422 Train Acc: 0.8188\n",
      "Epoch: 1246/2000 Validation Loss: 0.5368 Validation Acc: 0.7486\n",
      "Epoch: 1248/2000 Train Loss: 0.4421 Train Acc: 0.8188\n",
      "Epoch: 1248/2000 Validation Loss: 0.5366 Validation Acc: 0.7486\n",
      "Epoch: 1250/2000 Train Loss: 0.4419 Train Acc: 0.8188\n",
      "Epoch: 1250/2000 Validation Loss: 0.5365 Validation Acc: 0.7486\n",
      "Epoch: 1253/2000 Train Loss: 0.4417 Train Acc: 0.8188\n",
      "Epoch: 1253/2000 Validation Loss: 0.5363 Validation Acc: 0.7486\n",
      "Epoch: 1255/2000 Train Loss: 0.4415 Train Acc: 0.8188\n",
      "Epoch: 1255/2000 Validation Loss: 0.5362 Validation Acc: 0.7486\n",
      "Epoch: 1257/2000 Train Loss: 0.4414 Train Acc: 0.8188\n",
      "Epoch: 1257/2000 Validation Loss: 0.5361 Validation Acc: 0.7486\n",
      "Epoch: 1260/2000 Train Loss: 0.4412 Train Acc: 0.8188\n",
      "Epoch: 1260/2000 Validation Loss: 0.5359 Validation Acc: 0.7486\n",
      "Epoch: 1262/2000 Train Loss: 0.4410 Train Acc: 0.8188\n",
      "Epoch: 1262/2000 Validation Loss: 0.5358 Validation Acc: 0.7486\n",
      "Epoch: 1264/2000 Train Loss: 0.4408 Train Acc: 0.8188\n",
      "Epoch: 1264/2000 Validation Loss: 0.5357 Validation Acc: 0.7486\n",
      "Epoch: 1266/2000 Train Loss: 0.4407 Train Acc: 0.8188\n",
      "Epoch: 1266/2000 Validation Loss: 0.5355 Validation Acc: 0.7486\n",
      "Epoch: 1269/2000 Train Loss: 0.4405 Train Acc: 0.8188\n",
      "Epoch: 1269/2000 Validation Loss: 0.5354 Validation Acc: 0.7486\n",
      "Epoch: 1271/2000 Train Loss: 0.4403 Train Acc: 0.8188\n",
      "Epoch: 1271/2000 Validation Loss: 0.5353 Validation Acc: 0.7486\n",
      "Epoch: 1273/2000 Train Loss: 0.4402 Train Acc: 0.8188\n",
      "Epoch: 1273/2000 Validation Loss: 0.5351 Validation Acc: 0.7486\n",
      "Epoch: 1275/2000 Train Loss: 0.4400 Train Acc: 0.8188\n",
      "Epoch: 1275/2000 Validation Loss: 0.5350 Validation Acc: 0.7486\n",
      "Epoch: 1278/2000 Train Loss: 0.4398 Train Acc: 0.8188\n",
      "Epoch: 1278/2000 Validation Loss: 0.5349 Validation Acc: 0.7486\n",
      "Epoch: 1280/2000 Train Loss: 0.4397 Train Acc: 0.8188\n",
      "Epoch: 1280/2000 Validation Loss: 0.5347 Validation Acc: 0.7486\n",
      "Epoch: 1282/2000 Train Loss: 0.4395 Train Acc: 0.8188\n",
      "Epoch: 1282/2000 Validation Loss: 0.5346 Validation Acc: 0.7486\n",
      "Epoch: 1285/2000 Train Loss: 0.4393 Train Acc: 0.8188\n",
      "Epoch: 1285/2000 Validation Loss: 0.5345 Validation Acc: 0.7486\n",
      "Epoch: 1287/2000 Train Loss: 0.4392 Train Acc: 0.8188\n",
      "Epoch: 1287/2000 Validation Loss: 0.5343 Validation Acc: 0.7486\n",
      "Epoch: 1289/2000 Train Loss: 0.4390 Train Acc: 0.8188\n",
      "Epoch: 1289/2000 Validation Loss: 0.5342 Validation Acc: 0.7486\n",
      "Epoch: 1291/2000 Train Loss: 0.4388 Train Acc: 0.8188\n",
      "Epoch: 1291/2000 Validation Loss: 0.5341 Validation Acc: 0.7486\n",
      "Epoch: 1294/2000 Train Loss: 0.4387 Train Acc: 0.8188\n",
      "Epoch: 1294/2000 Validation Loss: 0.5339 Validation Acc: 0.7486\n",
      "Epoch: 1296/2000 Train Loss: 0.4385 Train Acc: 0.8188\n",
      "Epoch: 1296/2000 Validation Loss: 0.5338 Validation Acc: 0.7486\n",
      "Epoch: 1298/2000 Train Loss: 0.4383 Train Acc: 0.8188\n",
      "Epoch: 1298/2000 Validation Loss: 0.5337 Validation Acc: 0.7486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1300/2000 Train Loss: 0.4382 Train Acc: 0.8188\n",
      "Epoch: 1300/2000 Validation Loss: 0.5336 Validation Acc: 0.7486\n",
      "Epoch: 1303/2000 Train Loss: 0.4380 Train Acc: 0.8188\n",
      "Epoch: 1303/2000 Validation Loss: 0.5334 Validation Acc: 0.7486\n",
      "Epoch: 1305/2000 Train Loss: 0.4379 Train Acc: 0.8188\n",
      "Epoch: 1305/2000 Validation Loss: 0.5333 Validation Acc: 0.7486\n",
      "Epoch: 1307/2000 Train Loss: 0.4377 Train Acc: 0.8188\n",
      "Epoch: 1307/2000 Validation Loss: 0.5332 Validation Acc: 0.7486\n",
      "Epoch: 1310/2000 Train Loss: 0.4375 Train Acc: 0.8188\n",
      "Epoch: 1310/2000 Validation Loss: 0.5331 Validation Acc: 0.7486\n",
      "Epoch: 1312/2000 Train Loss: 0.4374 Train Acc: 0.8188\n",
      "Epoch: 1312/2000 Validation Loss: 0.5329 Validation Acc: 0.7486\n",
      "Epoch: 1314/2000 Train Loss: 0.4372 Train Acc: 0.8188\n",
      "Epoch: 1314/2000 Validation Loss: 0.5328 Validation Acc: 0.7486\n",
      "Epoch: 1316/2000 Train Loss: 0.4371 Train Acc: 0.8188\n",
      "Epoch: 1316/2000 Validation Loss: 0.5327 Validation Acc: 0.7486\n",
      "Epoch: 1319/2000 Train Loss: 0.4369 Train Acc: 0.8188\n",
      "Epoch: 1319/2000 Validation Loss: 0.5326 Validation Acc: 0.7486\n",
      "Epoch: 1321/2000 Train Loss: 0.4368 Train Acc: 0.8188\n",
      "Epoch: 1321/2000 Validation Loss: 0.5325 Validation Acc: 0.7486\n",
      "Epoch: 1323/2000 Train Loss: 0.4366 Train Acc: 0.8188\n",
      "Epoch: 1323/2000 Validation Loss: 0.5323 Validation Acc: 0.7486\n",
      "Epoch: 1325/2000 Train Loss: 0.4364 Train Acc: 0.8188\n",
      "Epoch: 1325/2000 Validation Loss: 0.5322 Validation Acc: 0.7486\n",
      "Epoch: 1328/2000 Train Loss: 0.4363 Train Acc: 0.8188\n",
      "Epoch: 1328/2000 Validation Loss: 0.5321 Validation Acc: 0.7486\n",
      "Epoch: 1330/2000 Train Loss: 0.4361 Train Acc: 0.8188\n",
      "Epoch: 1330/2000 Validation Loss: 0.5320 Validation Acc: 0.7486\n",
      "Epoch: 1332/2000 Train Loss: 0.4360 Train Acc: 0.8188\n",
      "Epoch: 1332/2000 Validation Loss: 0.5319 Validation Acc: 0.7486\n",
      "Epoch: 1335/2000 Train Loss: 0.4358 Train Acc: 0.8188\n",
      "Epoch: 1335/2000 Validation Loss: 0.5317 Validation Acc: 0.7486\n",
      "Epoch: 1337/2000 Train Loss: 0.4357 Train Acc: 0.8188\n",
      "Epoch: 1337/2000 Validation Loss: 0.5316 Validation Acc: 0.7486\n",
      "Epoch: 1339/2000 Train Loss: 0.4355 Train Acc: 0.8188\n",
      "Epoch: 1339/2000 Validation Loss: 0.5315 Validation Acc: 0.7486\n",
      "Epoch: 1341/2000 Train Loss: 0.4354 Train Acc: 0.8188\n",
      "Epoch: 1341/2000 Validation Loss: 0.5314 Validation Acc: 0.7486\n",
      "Epoch: 1344/2000 Train Loss: 0.4352 Train Acc: 0.8188\n",
      "Epoch: 1344/2000 Validation Loss: 0.5313 Validation Acc: 0.7486\n",
      "Epoch: 1346/2000 Train Loss: 0.4351 Train Acc: 0.8188\n",
      "Epoch: 1346/2000 Validation Loss: 0.5311 Validation Acc: 0.7486\n",
      "Epoch: 1348/2000 Train Loss: 0.4349 Train Acc: 0.8188\n",
      "Epoch: 1348/2000 Validation Loss: 0.5310 Validation Acc: 0.7486\n",
      "Epoch: 1350/2000 Train Loss: 0.4348 Train Acc: 0.8188\n",
      "Epoch: 1350/2000 Validation Loss: 0.5309 Validation Acc: 0.7486\n",
      "Epoch: 1353/2000 Train Loss: 0.4346 Train Acc: 0.8188\n",
      "Epoch: 1353/2000 Validation Loss: 0.5308 Validation Acc: 0.7486\n",
      "Epoch: 1355/2000 Train Loss: 0.4345 Train Acc: 0.8188\n",
      "Epoch: 1355/2000 Validation Loss: 0.5307 Validation Acc: 0.7486\n",
      "Epoch: 1357/2000 Train Loss: 0.4343 Train Acc: 0.8188\n",
      "Epoch: 1357/2000 Validation Loss: 0.5306 Validation Acc: 0.7486\n",
      "Epoch: 1360/2000 Train Loss: 0.4342 Train Acc: 0.8188\n",
      "Epoch: 1360/2000 Validation Loss: 0.5305 Validation Acc: 0.7486\n",
      "Epoch: 1362/2000 Train Loss: 0.4340 Train Acc: 0.8188\n",
      "Epoch: 1362/2000 Validation Loss: 0.5303 Validation Acc: 0.7486\n",
      "Epoch: 1364/2000 Train Loss: 0.4339 Train Acc: 0.8188\n",
      "Epoch: 1364/2000 Validation Loss: 0.5302 Validation Acc: 0.7486\n",
      "Epoch: 1366/2000 Train Loss: 0.4338 Train Acc: 0.8188\n",
      "Epoch: 1366/2000 Validation Loss: 0.5301 Validation Acc: 0.7486\n",
      "Epoch: 1369/2000 Train Loss: 0.4336 Train Acc: 0.8188\n",
      "Epoch: 1369/2000 Validation Loss: 0.5300 Validation Acc: 0.7486\n",
      "Epoch: 1371/2000 Train Loss: 0.4335 Train Acc: 0.8188\n",
      "Epoch: 1371/2000 Validation Loss: 0.5299 Validation Acc: 0.7486\n",
      "Epoch: 1373/2000 Train Loss: 0.4333 Train Acc: 0.8188\n",
      "Epoch: 1373/2000 Validation Loss: 0.5298 Validation Acc: 0.7486\n",
      "Epoch: 1375/2000 Train Loss: 0.4332 Train Acc: 0.8188\n",
      "Epoch: 1375/2000 Validation Loss: 0.5297 Validation Acc: 0.7486\n",
      "Epoch: 1378/2000 Train Loss: 0.4330 Train Acc: 0.8188\n",
      "Epoch: 1378/2000 Validation Loss: 0.5296 Validation Acc: 0.7486\n",
      "Epoch: 1380/2000 Train Loss: 0.4329 Train Acc: 0.8188\n",
      "Epoch: 1380/2000 Validation Loss: 0.5295 Validation Acc: 0.7486\n",
      "Epoch: 1382/2000 Train Loss: 0.4327 Train Acc: 0.8188\n",
      "Epoch: 1382/2000 Validation Loss: 0.5294 Validation Acc: 0.7486\n",
      "Epoch: 1385/2000 Train Loss: 0.4326 Train Acc: 0.8188\n",
      "Epoch: 1385/2000 Validation Loss: 0.5292 Validation Acc: 0.7486\n",
      "Epoch: 1387/2000 Train Loss: 0.4325 Train Acc: 0.8188\n",
      "Epoch: 1387/2000 Validation Loss: 0.5291 Validation Acc: 0.7486\n",
      "Epoch: 1389/2000 Train Loss: 0.4323 Train Acc: 0.8188\n",
      "Epoch: 1389/2000 Validation Loss: 0.5290 Validation Acc: 0.7486\n",
      "Epoch: 1391/2000 Train Loss: 0.4322 Train Acc: 0.8188\n",
      "Epoch: 1391/2000 Validation Loss: 0.5289 Validation Acc: 0.7486\n",
      "Epoch: 1394/2000 Train Loss: 0.4320 Train Acc: 0.8188\n",
      "Epoch: 1394/2000 Validation Loss: 0.5288 Validation Acc: 0.7486\n",
      "Epoch: 1396/2000 Train Loss: 0.4319 Train Acc: 0.8188\n",
      "Epoch: 1396/2000 Validation Loss: 0.5287 Validation Acc: 0.7486\n",
      "Epoch: 1398/2000 Train Loss: 0.4318 Train Acc: 0.8188\n",
      "Epoch: 1398/2000 Validation Loss: 0.5286 Validation Acc: 0.7486\n",
      "Epoch: 1400/2000 Train Loss: 0.4316 Train Acc: 0.8188\n",
      "Epoch: 1400/2000 Validation Loss: 0.5285 Validation Acc: 0.7486\n",
      "Epoch: 1403/2000 Train Loss: 0.4315 Train Acc: 0.8188\n",
      "Epoch: 1403/2000 Validation Loss: 0.5284 Validation Acc: 0.7486\n",
      "Epoch: 1405/2000 Train Loss: 0.4313 Train Acc: 0.8188\n",
      "Epoch: 1405/2000 Validation Loss: 0.5283 Validation Acc: 0.7486\n",
      "Epoch: 1407/2000 Train Loss: 0.4312 Train Acc: 0.8188\n",
      "Epoch: 1407/2000 Validation Loss: 0.5282 Validation Acc: 0.7486\n",
      "Epoch: 1410/2000 Train Loss: 0.4311 Train Acc: 0.8188\n",
      "Epoch: 1410/2000 Validation Loss: 0.5281 Validation Acc: 0.7486\n",
      "Epoch: 1412/2000 Train Loss: 0.4309 Train Acc: 0.8188\n",
      "Epoch: 1412/2000 Validation Loss: 0.5280 Validation Acc: 0.7486\n",
      "Epoch: 1414/2000 Train Loss: 0.4308 Train Acc: 0.8188\n",
      "Epoch: 1414/2000 Validation Loss: 0.5279 Validation Acc: 0.7486\n",
      "Epoch: 1416/2000 Train Loss: 0.4307 Train Acc: 0.8188\n",
      "Epoch: 1416/2000 Validation Loss: 0.5278 Validation Acc: 0.7486\n",
      "Epoch: 1419/2000 Train Loss: 0.4305 Train Acc: 0.8188\n",
      "Epoch: 1419/2000 Validation Loss: 0.5277 Validation Acc: 0.7486\n",
      "Epoch: 1421/2000 Train Loss: 0.4304 Train Acc: 0.8188\n",
      "Epoch: 1421/2000 Validation Loss: 0.5276 Validation Acc: 0.7486\n",
      "Epoch: 1423/2000 Train Loss: 0.4303 Train Acc: 0.8188\n",
      "Epoch: 1423/2000 Validation Loss: 0.5275 Validation Acc: 0.7486\n",
      "Epoch: 1425/2000 Train Loss: 0.4301 Train Acc: 0.8188\n",
      "Epoch: 1425/2000 Validation Loss: 0.5274 Validation Acc: 0.7430\n",
      "Epoch: 1428/2000 Train Loss: 0.4300 Train Acc: 0.8188\n",
      "Epoch: 1428/2000 Validation Loss: 0.5273 Validation Acc: 0.7430\n",
      "Epoch: 1430/2000 Train Loss: 0.4299 Train Acc: 0.8188\n",
      "Epoch: 1430/2000 Validation Loss: 0.5272 Validation Acc: 0.7430\n",
      "Epoch: 1432/2000 Train Loss: 0.4297 Train Acc: 0.8188\n",
      "Epoch: 1432/2000 Validation Loss: 0.5271 Validation Acc: 0.7430\n",
      "Epoch: 1435/2000 Train Loss: 0.4296 Train Acc: 0.8188\n",
      "Epoch: 1435/2000 Validation Loss: 0.5270 Validation Acc: 0.7430\n",
      "Epoch: 1437/2000 Train Loss: 0.4295 Train Acc: 0.8188\n",
      "Epoch: 1437/2000 Validation Loss: 0.5269 Validation Acc: 0.7430\n",
      "Epoch: 1439/2000 Train Loss: 0.4293 Train Acc: 0.8188\n",
      "Epoch: 1439/2000 Validation Loss: 0.5268 Validation Acc: 0.7430\n",
      "Epoch: 1441/2000 Train Loss: 0.4292 Train Acc: 0.8188\n",
      "Epoch: 1441/2000 Validation Loss: 0.5267 Validation Acc: 0.7430\n",
      "Epoch: 1444/2000 Train Loss: 0.4291 Train Acc: 0.8188\n",
      "Epoch: 1444/2000 Validation Loss: 0.5266 Validation Acc: 0.7430\n",
      "Epoch: 1446/2000 Train Loss: 0.4289 Train Acc: 0.8188\n",
      "Epoch: 1446/2000 Validation Loss: 0.5265 Validation Acc: 0.7430\n",
      "Epoch: 1448/2000 Train Loss: 0.4288 Train Acc: 0.8188\n",
      "Epoch: 1448/2000 Validation Loss: 0.5264 Validation Acc: 0.7430\n",
      "Epoch: 1450/2000 Train Loss: 0.4287 Train Acc: 0.8188\n",
      "Epoch: 1450/2000 Validation Loss: 0.5263 Validation Acc: 0.7430\n",
      "Epoch: 1453/2000 Train Loss: 0.4286 Train Acc: 0.8174\n",
      "Epoch: 1453/2000 Validation Loss: 0.5262 Validation Acc: 0.7430\n",
      "Epoch: 1455/2000 Train Loss: 0.4284 Train Acc: 0.8174\n",
      "Epoch: 1455/2000 Validation Loss: 0.5261 Validation Acc: 0.7430\n",
      "Epoch: 1457/2000 Train Loss: 0.4283 Train Acc: 0.8174\n",
      "Epoch: 1457/2000 Validation Loss: 0.5260 Validation Acc: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1460/2000 Train Loss: 0.4282 Train Acc: 0.8174\n",
      "Epoch: 1460/2000 Validation Loss: 0.5260 Validation Acc: 0.7430\n",
      "Epoch: 1462/2000 Train Loss: 0.4281 Train Acc: 0.8174\n",
      "Epoch: 1462/2000 Validation Loss: 0.5259 Validation Acc: 0.7430\n",
      "Epoch: 1464/2000 Train Loss: 0.4279 Train Acc: 0.8174\n",
      "Epoch: 1464/2000 Validation Loss: 0.5258 Validation Acc: 0.7430\n",
      "Epoch: 1466/2000 Train Loss: 0.4278 Train Acc: 0.8174\n",
      "Epoch: 1466/2000 Validation Loss: 0.5257 Validation Acc: 0.7430\n",
      "Epoch: 1469/2000 Train Loss: 0.4277 Train Acc: 0.8174\n",
      "Epoch: 1469/2000 Validation Loss: 0.5256 Validation Acc: 0.7430\n",
      "Epoch: 1471/2000 Train Loss: 0.4276 Train Acc: 0.8174\n",
      "Epoch: 1471/2000 Validation Loss: 0.5255 Validation Acc: 0.7430\n",
      "Epoch: 1473/2000 Train Loss: 0.4274 Train Acc: 0.8174\n",
      "Epoch: 1473/2000 Validation Loss: 0.5254 Validation Acc: 0.7430\n",
      "Epoch: 1475/2000 Train Loss: 0.4273 Train Acc: 0.8174\n",
      "Epoch: 1475/2000 Validation Loss: 0.5253 Validation Acc: 0.7430\n",
      "Epoch: 1478/2000 Train Loss: 0.4272 Train Acc: 0.8174\n",
      "Epoch: 1478/2000 Validation Loss: 0.5252 Validation Acc: 0.7430\n",
      "Epoch: 1480/2000 Train Loss: 0.4271 Train Acc: 0.8174\n",
      "Epoch: 1480/2000 Validation Loss: 0.5251 Validation Acc: 0.7430\n",
      "Epoch: 1482/2000 Train Loss: 0.4270 Train Acc: 0.8174\n",
      "Epoch: 1482/2000 Validation Loss: 0.5251 Validation Acc: 0.7430\n",
      "Epoch: 1485/2000 Train Loss: 0.4268 Train Acc: 0.8174\n",
      "Epoch: 1485/2000 Validation Loss: 0.5250 Validation Acc: 0.7430\n",
      "Epoch: 1487/2000 Train Loss: 0.4267 Train Acc: 0.8174\n",
      "Epoch: 1487/2000 Validation Loss: 0.5249 Validation Acc: 0.7430\n",
      "Epoch: 1489/2000 Train Loss: 0.4266 Train Acc: 0.8174\n",
      "Epoch: 1489/2000 Validation Loss: 0.5248 Validation Acc: 0.7430\n",
      "Epoch: 1491/2000 Train Loss: 0.4265 Train Acc: 0.8174\n",
      "Epoch: 1491/2000 Validation Loss: 0.5247 Validation Acc: 0.7430\n",
      "Epoch: 1494/2000 Train Loss: 0.4264 Train Acc: 0.8174\n",
      "Epoch: 1494/2000 Validation Loss: 0.5246 Validation Acc: 0.7430\n",
      "Epoch: 1496/2000 Train Loss: 0.4262 Train Acc: 0.8174\n",
      "Epoch: 1496/2000 Validation Loss: 0.5245 Validation Acc: 0.7430\n",
      "Epoch: 1498/2000 Train Loss: 0.4261 Train Acc: 0.8174\n",
      "Epoch: 1498/2000 Validation Loss: 0.5244 Validation Acc: 0.7430\n",
      "Epoch: 1500/2000 Train Loss: 0.4260 Train Acc: 0.8174\n",
      "Epoch: 1500/2000 Validation Loss: 0.5244 Validation Acc: 0.7430\n",
      "Epoch: 1503/2000 Train Loss: 0.4259 Train Acc: 0.8174\n",
      "Epoch: 1503/2000 Validation Loss: 0.5243 Validation Acc: 0.7430\n",
      "Epoch: 1505/2000 Train Loss: 0.4258 Train Acc: 0.8174\n",
      "Epoch: 1505/2000 Validation Loss: 0.5242 Validation Acc: 0.7430\n",
      "Epoch: 1507/2000 Train Loss: 0.4257 Train Acc: 0.8174\n",
      "Epoch: 1507/2000 Validation Loss: 0.5241 Validation Acc: 0.7430\n",
      "Epoch: 1510/2000 Train Loss: 0.4255 Train Acc: 0.8174\n",
      "Epoch: 1510/2000 Validation Loss: 0.5240 Validation Acc: 0.7430\n",
      "Epoch: 1512/2000 Train Loss: 0.4254 Train Acc: 0.8174\n",
      "Epoch: 1512/2000 Validation Loss: 0.5239 Validation Acc: 0.7430\n",
      "Epoch: 1514/2000 Train Loss: 0.4253 Train Acc: 0.8174\n",
      "Epoch: 1514/2000 Validation Loss: 0.5239 Validation Acc: 0.7430\n",
      "Epoch: 1516/2000 Train Loss: 0.4252 Train Acc: 0.8174\n",
      "Epoch: 1516/2000 Validation Loss: 0.5238 Validation Acc: 0.7430\n",
      "Epoch: 1519/2000 Train Loss: 0.4251 Train Acc: 0.8174\n",
      "Epoch: 1519/2000 Validation Loss: 0.5237 Validation Acc: 0.7430\n",
      "Epoch: 1521/2000 Train Loss: 0.4250 Train Acc: 0.8174\n",
      "Epoch: 1521/2000 Validation Loss: 0.5236 Validation Acc: 0.7430\n",
      "Epoch: 1523/2000 Train Loss: 0.4249 Train Acc: 0.8202\n",
      "Epoch: 1523/2000 Validation Loss: 0.5235 Validation Acc: 0.7430\n",
      "Epoch: 1525/2000 Train Loss: 0.4247 Train Acc: 0.8202\n",
      "Epoch: 1525/2000 Validation Loss: 0.5234 Validation Acc: 0.7430\n",
      "Epoch: 1528/2000 Train Loss: 0.4246 Train Acc: 0.8202\n",
      "Epoch: 1528/2000 Validation Loss: 0.5234 Validation Acc: 0.7430\n",
      "Epoch: 1530/2000 Train Loss: 0.4245 Train Acc: 0.8202\n",
      "Epoch: 1530/2000 Validation Loss: 0.5233 Validation Acc: 0.7430\n",
      "Epoch: 1532/2000 Train Loss: 0.4244 Train Acc: 0.8202\n",
      "Epoch: 1532/2000 Validation Loss: 0.5232 Validation Acc: 0.7430\n",
      "Epoch: 1535/2000 Train Loss: 0.4243 Train Acc: 0.8202\n",
      "Epoch: 1535/2000 Validation Loss: 0.5231 Validation Acc: 0.7430\n",
      "Epoch: 1537/2000 Train Loss: 0.4242 Train Acc: 0.8202\n",
      "Epoch: 1537/2000 Validation Loss: 0.5230 Validation Acc: 0.7430\n",
      "Epoch: 1539/2000 Train Loss: 0.4241 Train Acc: 0.8202\n",
      "Epoch: 1539/2000 Validation Loss: 0.5230 Validation Acc: 0.7430\n",
      "Epoch: 1541/2000 Train Loss: 0.4240 Train Acc: 0.8202\n",
      "Epoch: 1541/2000 Validation Loss: 0.5229 Validation Acc: 0.7430\n",
      "Epoch: 1544/2000 Train Loss: 0.4238 Train Acc: 0.8202\n",
      "Epoch: 1544/2000 Validation Loss: 0.5228 Validation Acc: 0.7430\n",
      "Epoch: 1546/2000 Train Loss: 0.4237 Train Acc: 0.8216\n",
      "Epoch: 1546/2000 Validation Loss: 0.5227 Validation Acc: 0.7430\n",
      "Epoch: 1548/2000 Train Loss: 0.4236 Train Acc: 0.8216\n",
      "Epoch: 1548/2000 Validation Loss: 0.5226 Validation Acc: 0.7430\n",
      "Epoch: 1550/2000 Train Loss: 0.4235 Train Acc: 0.8216\n",
      "Epoch: 1550/2000 Validation Loss: 0.5226 Validation Acc: 0.7430\n",
      "Epoch: 1553/2000 Train Loss: 0.4234 Train Acc: 0.8216\n",
      "Epoch: 1553/2000 Validation Loss: 0.5225 Validation Acc: 0.7430\n",
      "Epoch: 1555/2000 Train Loss: 0.4233 Train Acc: 0.8216\n",
      "Epoch: 1555/2000 Validation Loss: 0.5224 Validation Acc: 0.7430\n",
      "Epoch: 1557/2000 Train Loss: 0.4232 Train Acc: 0.8216\n",
      "Epoch: 1557/2000 Validation Loss: 0.5223 Validation Acc: 0.7430\n",
      "Epoch: 1560/2000 Train Loss: 0.4231 Train Acc: 0.8216\n",
      "Epoch: 1560/2000 Validation Loss: 0.5222 Validation Acc: 0.7430\n",
      "Epoch: 1562/2000 Train Loss: 0.4230 Train Acc: 0.8216\n",
      "Epoch: 1562/2000 Validation Loss: 0.5222 Validation Acc: 0.7430\n",
      "Epoch: 1564/2000 Train Loss: 0.4229 Train Acc: 0.8216\n",
      "Epoch: 1564/2000 Validation Loss: 0.5221 Validation Acc: 0.7430\n",
      "Epoch: 1566/2000 Train Loss: 0.4228 Train Acc: 0.8216\n",
      "Epoch: 1566/2000 Validation Loss: 0.5220 Validation Acc: 0.7430\n",
      "Epoch: 1569/2000 Train Loss: 0.4227 Train Acc: 0.8216\n",
      "Epoch: 1569/2000 Validation Loss: 0.5219 Validation Acc: 0.7430\n",
      "Epoch: 1571/2000 Train Loss: 0.4225 Train Acc: 0.8216\n",
      "Epoch: 1571/2000 Validation Loss: 0.5219 Validation Acc: 0.7430\n",
      "Epoch: 1573/2000 Train Loss: 0.4224 Train Acc: 0.8216\n",
      "Epoch: 1573/2000 Validation Loss: 0.5218 Validation Acc: 0.7430\n",
      "Epoch: 1575/2000 Train Loss: 0.4223 Train Acc: 0.8216\n",
      "Epoch: 1575/2000 Validation Loss: 0.5217 Validation Acc: 0.7430\n",
      "Epoch: 1578/2000 Train Loss: 0.4222 Train Acc: 0.8216\n",
      "Epoch: 1578/2000 Validation Loss: 0.5216 Validation Acc: 0.7430\n",
      "Epoch: 1580/2000 Train Loss: 0.4221 Train Acc: 0.8216\n",
      "Epoch: 1580/2000 Validation Loss: 0.5215 Validation Acc: 0.7430\n",
      "Epoch: 1582/2000 Train Loss: 0.4220 Train Acc: 0.8216\n",
      "Epoch: 1582/2000 Validation Loss: 0.5215 Validation Acc: 0.7430\n",
      "Epoch: 1585/2000 Train Loss: 0.4219 Train Acc: 0.8216\n",
      "Epoch: 1585/2000 Validation Loss: 0.5214 Validation Acc: 0.7430\n",
      "Epoch: 1587/2000 Train Loss: 0.4218 Train Acc: 0.8216\n",
      "Epoch: 1587/2000 Validation Loss: 0.5213 Validation Acc: 0.7430\n",
      "Epoch: 1589/2000 Train Loss: 0.4217 Train Acc: 0.8216\n",
      "Epoch: 1589/2000 Validation Loss: 0.5212 Validation Acc: 0.7430\n",
      "Epoch: 1591/2000 Train Loss: 0.4216 Train Acc: 0.8216\n",
      "Epoch: 1591/2000 Validation Loss: 0.5212 Validation Acc: 0.7430\n",
      "Epoch: 1594/2000 Train Loss: 0.4215 Train Acc: 0.8216\n",
      "Epoch: 1594/2000 Validation Loss: 0.5211 Validation Acc: 0.7430\n",
      "Epoch: 1596/2000 Train Loss: 0.4214 Train Acc: 0.8216\n",
      "Epoch: 1596/2000 Validation Loss: 0.5210 Validation Acc: 0.7430\n",
      "Epoch: 1598/2000 Train Loss: 0.4213 Train Acc: 0.8216\n",
      "Epoch: 1598/2000 Validation Loss: 0.5210 Validation Acc: 0.7430\n",
      "Epoch: 1600/2000 Train Loss: 0.4212 Train Acc: 0.8216\n",
      "Epoch: 1600/2000 Validation Loss: 0.5209 Validation Acc: 0.7430\n",
      "Epoch: 1603/2000 Train Loss: 0.4211 Train Acc: 0.8216\n",
      "Epoch: 1603/2000 Validation Loss: 0.5208 Validation Acc: 0.7430\n",
      "Epoch: 1605/2000 Train Loss: 0.4210 Train Acc: 0.8216\n",
      "Epoch: 1605/2000 Validation Loss: 0.5207 Validation Acc: 0.7430\n",
      "Epoch: 1607/2000 Train Loss: 0.4209 Train Acc: 0.8216\n",
      "Epoch: 1607/2000 Validation Loss: 0.5207 Validation Acc: 0.7430\n",
      "Epoch: 1610/2000 Train Loss: 0.4208 Train Acc: 0.8216\n",
      "Epoch: 1610/2000 Validation Loss: 0.5206 Validation Acc: 0.7430\n",
      "Epoch: 1612/2000 Train Loss: 0.4207 Train Acc: 0.8216\n",
      "Epoch: 1612/2000 Validation Loss: 0.5205 Validation Acc: 0.7430\n",
      "Epoch: 1614/2000 Train Loss: 0.4206 Train Acc: 0.8216\n",
      "Epoch: 1614/2000 Validation Loss: 0.5204 Validation Acc: 0.7430\n",
      "Epoch: 1616/2000 Train Loss: 0.4205 Train Acc: 0.8216\n",
      "Epoch: 1616/2000 Validation Loss: 0.5204 Validation Acc: 0.7430\n",
      "Epoch: 1619/2000 Train Loss: 0.4204 Train Acc: 0.8216\n",
      "Epoch: 1619/2000 Validation Loss: 0.5203 Validation Acc: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1621/2000 Train Loss: 0.4203 Train Acc: 0.8216\n",
      "Epoch: 1621/2000 Validation Loss: 0.5202 Validation Acc: 0.7430\n",
      "Epoch: 1623/2000 Train Loss: 0.4202 Train Acc: 0.8216\n",
      "Epoch: 1623/2000 Validation Loss: 0.5202 Validation Acc: 0.7430\n",
      "Epoch: 1625/2000 Train Loss: 0.4201 Train Acc: 0.8216\n",
      "Epoch: 1625/2000 Validation Loss: 0.5201 Validation Acc: 0.7430\n",
      "Epoch: 1628/2000 Train Loss: 0.4200 Train Acc: 0.8216\n",
      "Epoch: 1628/2000 Validation Loss: 0.5200 Validation Acc: 0.7430\n",
      "Epoch: 1630/2000 Train Loss: 0.4199 Train Acc: 0.8216\n",
      "Epoch: 1630/2000 Validation Loss: 0.5200 Validation Acc: 0.7430\n",
      "Epoch: 1632/2000 Train Loss: 0.4198 Train Acc: 0.8216\n",
      "Epoch: 1632/2000 Validation Loss: 0.5199 Validation Acc: 0.7486\n",
      "Epoch: 1635/2000 Train Loss: 0.4197 Train Acc: 0.8216\n",
      "Epoch: 1635/2000 Validation Loss: 0.5198 Validation Acc: 0.7486\n",
      "Epoch: 1637/2000 Train Loss: 0.4196 Train Acc: 0.8216\n",
      "Epoch: 1637/2000 Validation Loss: 0.5197 Validation Acc: 0.7486\n",
      "Epoch: 1639/2000 Train Loss: 0.4195 Train Acc: 0.8216\n",
      "Epoch: 1639/2000 Validation Loss: 0.5197 Validation Acc: 0.7486\n",
      "Epoch: 1641/2000 Train Loss: 0.4194 Train Acc: 0.8216\n",
      "Epoch: 1641/2000 Validation Loss: 0.5196 Validation Acc: 0.7486\n",
      "Epoch: 1644/2000 Train Loss: 0.4193 Train Acc: 0.8216\n",
      "Epoch: 1644/2000 Validation Loss: 0.5195 Validation Acc: 0.7486\n",
      "Epoch: 1646/2000 Train Loss: 0.4192 Train Acc: 0.8216\n",
      "Epoch: 1646/2000 Validation Loss: 0.5195 Validation Acc: 0.7486\n",
      "Epoch: 1648/2000 Train Loss: 0.4191 Train Acc: 0.8216\n",
      "Epoch: 1648/2000 Validation Loss: 0.5194 Validation Acc: 0.7486\n",
      "Epoch: 1650/2000 Train Loss: 0.4190 Train Acc: 0.8216\n",
      "Epoch: 1650/2000 Validation Loss: 0.5193 Validation Acc: 0.7486\n",
      "Epoch: 1653/2000 Train Loss: 0.4189 Train Acc: 0.8216\n",
      "Epoch: 1653/2000 Validation Loss: 0.5193 Validation Acc: 0.7486\n",
      "Epoch: 1655/2000 Train Loss: 0.4188 Train Acc: 0.8216\n",
      "Epoch: 1655/2000 Validation Loss: 0.5192 Validation Acc: 0.7486\n",
      "Epoch: 1657/2000 Train Loss: 0.4187 Train Acc: 0.8216\n",
      "Epoch: 1657/2000 Validation Loss: 0.5191 Validation Acc: 0.7486\n",
      "Epoch: 1660/2000 Train Loss: 0.4186 Train Acc: 0.8216\n",
      "Epoch: 1660/2000 Validation Loss: 0.5191 Validation Acc: 0.7486\n",
      "Epoch: 1662/2000 Train Loss: 0.4185 Train Acc: 0.8216\n",
      "Epoch: 1662/2000 Validation Loss: 0.5190 Validation Acc: 0.7486\n",
      "Epoch: 1664/2000 Train Loss: 0.4185 Train Acc: 0.8216\n",
      "Epoch: 1664/2000 Validation Loss: 0.5189 Validation Acc: 0.7486\n",
      "Epoch: 1666/2000 Train Loss: 0.4184 Train Acc: 0.8216\n",
      "Epoch: 1666/2000 Validation Loss: 0.5189 Validation Acc: 0.7486\n",
      "Epoch: 1669/2000 Train Loss: 0.4183 Train Acc: 0.8216\n",
      "Epoch: 1669/2000 Validation Loss: 0.5188 Validation Acc: 0.7486\n",
      "Epoch: 1671/2000 Train Loss: 0.4182 Train Acc: 0.8216\n",
      "Epoch: 1671/2000 Validation Loss: 0.5187 Validation Acc: 0.7486\n",
      "Epoch: 1673/2000 Train Loss: 0.4181 Train Acc: 0.8216\n",
      "Epoch: 1673/2000 Validation Loss: 0.5187 Validation Acc: 0.7486\n",
      "Epoch: 1675/2000 Train Loss: 0.4180 Train Acc: 0.8216\n",
      "Epoch: 1675/2000 Validation Loss: 0.5186 Validation Acc: 0.7486\n",
      "Epoch: 1678/2000 Train Loss: 0.4179 Train Acc: 0.8216\n",
      "Epoch: 1678/2000 Validation Loss: 0.5185 Validation Acc: 0.7486\n",
      "Epoch: 1680/2000 Train Loss: 0.4178 Train Acc: 0.8202\n",
      "Epoch: 1680/2000 Validation Loss: 0.5185 Validation Acc: 0.7486\n",
      "Epoch: 1682/2000 Train Loss: 0.4177 Train Acc: 0.8202\n",
      "Epoch: 1682/2000 Validation Loss: 0.5184 Validation Acc: 0.7486\n",
      "Epoch: 1685/2000 Train Loss: 0.4176 Train Acc: 0.8202\n",
      "Epoch: 1685/2000 Validation Loss: 0.5184 Validation Acc: 0.7486\n",
      "Epoch: 1687/2000 Train Loss: 0.4175 Train Acc: 0.8202\n",
      "Epoch: 1687/2000 Validation Loss: 0.5183 Validation Acc: 0.7486\n",
      "Epoch: 1689/2000 Train Loss: 0.4174 Train Acc: 0.8202\n",
      "Epoch: 1689/2000 Validation Loss: 0.5182 Validation Acc: 0.7486\n",
      "Epoch: 1691/2000 Train Loss: 0.4173 Train Acc: 0.8202\n",
      "Epoch: 1691/2000 Validation Loss: 0.5182 Validation Acc: 0.7486\n",
      "Epoch: 1694/2000 Train Loss: 0.4173 Train Acc: 0.8202\n",
      "Epoch: 1694/2000 Validation Loss: 0.5181 Validation Acc: 0.7486\n",
      "Epoch: 1696/2000 Train Loss: 0.4172 Train Acc: 0.8202\n",
      "Epoch: 1696/2000 Validation Loss: 0.5180 Validation Acc: 0.7486\n",
      "Epoch: 1698/2000 Train Loss: 0.4171 Train Acc: 0.8202\n",
      "Epoch: 1698/2000 Validation Loss: 0.5180 Validation Acc: 0.7486\n",
      "Epoch: 1700/2000 Train Loss: 0.4170 Train Acc: 0.8202\n",
      "Epoch: 1700/2000 Validation Loss: 0.5179 Validation Acc: 0.7486\n",
      "Epoch: 1703/2000 Train Loss: 0.4169 Train Acc: 0.8202\n",
      "Epoch: 1703/2000 Validation Loss: 0.5178 Validation Acc: 0.7486\n",
      "Epoch: 1705/2000 Train Loss: 0.4168 Train Acc: 0.8202\n",
      "Epoch: 1705/2000 Validation Loss: 0.5178 Validation Acc: 0.7486\n",
      "Epoch: 1707/2000 Train Loss: 0.4167 Train Acc: 0.8202\n",
      "Epoch: 1707/2000 Validation Loss: 0.5177 Validation Acc: 0.7486\n",
      "Epoch: 1710/2000 Train Loss: 0.4166 Train Acc: 0.8202\n",
      "Epoch: 1710/2000 Validation Loss: 0.5177 Validation Acc: 0.7486\n",
      "Epoch: 1712/2000 Train Loss: 0.4165 Train Acc: 0.8202\n",
      "Epoch: 1712/2000 Validation Loss: 0.5176 Validation Acc: 0.7486\n",
      "Epoch: 1714/2000 Train Loss: 0.4165 Train Acc: 0.8202\n",
      "Epoch: 1714/2000 Validation Loss: 0.5175 Validation Acc: 0.7486\n",
      "Epoch: 1716/2000 Train Loss: 0.4164 Train Acc: 0.8202\n",
      "Epoch: 1716/2000 Validation Loss: 0.5175 Validation Acc: 0.7486\n",
      "Epoch: 1719/2000 Train Loss: 0.4163 Train Acc: 0.8202\n",
      "Epoch: 1719/2000 Validation Loss: 0.5174 Validation Acc: 0.7486\n",
      "Epoch: 1721/2000 Train Loss: 0.4162 Train Acc: 0.8202\n",
      "Epoch: 1721/2000 Validation Loss: 0.5174 Validation Acc: 0.7486\n",
      "Epoch: 1723/2000 Train Loss: 0.4161 Train Acc: 0.8202\n",
      "Epoch: 1723/2000 Validation Loss: 0.5173 Validation Acc: 0.7486\n",
      "Epoch: 1725/2000 Train Loss: 0.4160 Train Acc: 0.8202\n",
      "Epoch: 1725/2000 Validation Loss: 0.5172 Validation Acc: 0.7486\n",
      "Epoch: 1728/2000 Train Loss: 0.4159 Train Acc: 0.8202\n",
      "Epoch: 1728/2000 Validation Loss: 0.5172 Validation Acc: 0.7486\n",
      "Epoch: 1730/2000 Train Loss: 0.4159 Train Acc: 0.8202\n",
      "Epoch: 1730/2000 Validation Loss: 0.5171 Validation Acc: 0.7486\n",
      "Epoch: 1732/2000 Train Loss: 0.4158 Train Acc: 0.8202\n",
      "Epoch: 1732/2000 Validation Loss: 0.5171 Validation Acc: 0.7486\n",
      "Epoch: 1735/2000 Train Loss: 0.4157 Train Acc: 0.8202\n",
      "Epoch: 1735/2000 Validation Loss: 0.5170 Validation Acc: 0.7486\n",
      "Epoch: 1737/2000 Train Loss: 0.4156 Train Acc: 0.8202\n",
      "Epoch: 1737/2000 Validation Loss: 0.5169 Validation Acc: 0.7486\n",
      "Epoch: 1739/2000 Train Loss: 0.4155 Train Acc: 0.8202\n",
      "Epoch: 1739/2000 Validation Loss: 0.5169 Validation Acc: 0.7486\n",
      "Epoch: 1741/2000 Train Loss: 0.4154 Train Acc: 0.8202\n",
      "Epoch: 1741/2000 Validation Loss: 0.5168 Validation Acc: 0.7430\n",
      "Epoch: 1744/2000 Train Loss: 0.4154 Train Acc: 0.8202\n",
      "Epoch: 1744/2000 Validation Loss: 0.5168 Validation Acc: 0.7430\n",
      "Epoch: 1746/2000 Train Loss: 0.4153 Train Acc: 0.8202\n",
      "Epoch: 1746/2000 Validation Loss: 0.5167 Validation Acc: 0.7430\n",
      "Epoch: 1748/2000 Train Loss: 0.4152 Train Acc: 0.8202\n",
      "Epoch: 1748/2000 Validation Loss: 0.5166 Validation Acc: 0.7430\n",
      "Epoch: 1750/2000 Train Loss: 0.4151 Train Acc: 0.8202\n",
      "Epoch: 1750/2000 Validation Loss: 0.5166 Validation Acc: 0.7430\n",
      "Epoch: 1753/2000 Train Loss: 0.4150 Train Acc: 0.8202\n",
      "Epoch: 1753/2000 Validation Loss: 0.5165 Validation Acc: 0.7430\n",
      "Epoch: 1755/2000 Train Loss: 0.4149 Train Acc: 0.8202\n",
      "Epoch: 1755/2000 Validation Loss: 0.5165 Validation Acc: 0.7430\n",
      "Epoch: 1757/2000 Train Loss: 0.4149 Train Acc: 0.8202\n",
      "Epoch: 1757/2000 Validation Loss: 0.5164 Validation Acc: 0.7430\n",
      "Epoch: 1760/2000 Train Loss: 0.4148 Train Acc: 0.8202\n",
      "Epoch: 1760/2000 Validation Loss: 0.5164 Validation Acc: 0.7430\n",
      "Epoch: 1762/2000 Train Loss: 0.4147 Train Acc: 0.8202\n",
      "Epoch: 1762/2000 Validation Loss: 0.5163 Validation Acc: 0.7430\n",
      "Epoch: 1764/2000 Train Loss: 0.4146 Train Acc: 0.8202\n",
      "Epoch: 1764/2000 Validation Loss: 0.5162 Validation Acc: 0.7430\n",
      "Epoch: 1766/2000 Train Loss: 0.4145 Train Acc: 0.8202\n",
      "Epoch: 1766/2000 Validation Loss: 0.5162 Validation Acc: 0.7430\n",
      "Epoch: 1769/2000 Train Loss: 0.4145 Train Acc: 0.8202\n",
      "Epoch: 1769/2000 Validation Loss: 0.5161 Validation Acc: 0.7430\n",
      "Epoch: 1771/2000 Train Loss: 0.4144 Train Acc: 0.8202\n",
      "Epoch: 1771/2000 Validation Loss: 0.5161 Validation Acc: 0.7430\n",
      "Epoch: 1773/2000 Train Loss: 0.4143 Train Acc: 0.8202\n",
      "Epoch: 1773/2000 Validation Loss: 0.5160 Validation Acc: 0.7430\n",
      "Epoch: 1775/2000 Train Loss: 0.4142 Train Acc: 0.8202\n",
      "Epoch: 1775/2000 Validation Loss: 0.5160 Validation Acc: 0.7430\n",
      "Epoch: 1778/2000 Train Loss: 0.4141 Train Acc: 0.8202\n",
      "Epoch: 1778/2000 Validation Loss: 0.5159 Validation Acc: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1780/2000 Train Loss: 0.4140 Train Acc: 0.8202\n",
      "Epoch: 1780/2000 Validation Loss: 0.5158 Validation Acc: 0.7430\n",
      "Epoch: 1782/2000 Train Loss: 0.4140 Train Acc: 0.8202\n",
      "Epoch: 1782/2000 Validation Loss: 0.5158 Validation Acc: 0.7430\n",
      "Epoch: 1785/2000 Train Loss: 0.4139 Train Acc: 0.8202\n",
      "Epoch: 1785/2000 Validation Loss: 0.5157 Validation Acc: 0.7430\n",
      "Epoch: 1787/2000 Train Loss: 0.4138 Train Acc: 0.8202\n",
      "Epoch: 1787/2000 Validation Loss: 0.5157 Validation Acc: 0.7430\n",
      "Epoch: 1789/2000 Train Loss: 0.4137 Train Acc: 0.8202\n",
      "Epoch: 1789/2000 Validation Loss: 0.5156 Validation Acc: 0.7430\n",
      "Epoch: 1791/2000 Train Loss: 0.4137 Train Acc: 0.8202\n",
      "Epoch: 1791/2000 Validation Loss: 0.5156 Validation Acc: 0.7430\n",
      "Epoch: 1794/2000 Train Loss: 0.4136 Train Acc: 0.8202\n",
      "Epoch: 1794/2000 Validation Loss: 0.5155 Validation Acc: 0.7430\n",
      "Epoch: 1796/2000 Train Loss: 0.4135 Train Acc: 0.8202\n",
      "Epoch: 1796/2000 Validation Loss: 0.5155 Validation Acc: 0.7430\n",
      "Epoch: 1798/2000 Train Loss: 0.4134 Train Acc: 0.8202\n",
      "Epoch: 1798/2000 Validation Loss: 0.5154 Validation Acc: 0.7430\n",
      "Epoch: 1800/2000 Train Loss: 0.4133 Train Acc: 0.8202\n",
      "Epoch: 1800/2000 Validation Loss: 0.5153 Validation Acc: 0.7430\n",
      "Epoch: 1803/2000 Train Loss: 0.4133 Train Acc: 0.8202\n",
      "Epoch: 1803/2000 Validation Loss: 0.5153 Validation Acc: 0.7430\n",
      "Epoch: 1805/2000 Train Loss: 0.4132 Train Acc: 0.8202\n",
      "Epoch: 1805/2000 Validation Loss: 0.5152 Validation Acc: 0.7430\n",
      "Epoch: 1807/2000 Train Loss: 0.4131 Train Acc: 0.8202\n",
      "Epoch: 1807/2000 Validation Loss: 0.5152 Validation Acc: 0.7430\n",
      "Epoch: 1810/2000 Train Loss: 0.4130 Train Acc: 0.8202\n",
      "Epoch: 1810/2000 Validation Loss: 0.5151 Validation Acc: 0.7430\n",
      "Epoch: 1812/2000 Train Loss: 0.4129 Train Acc: 0.8202\n",
      "Epoch: 1812/2000 Validation Loss: 0.5151 Validation Acc: 0.7430\n",
      "Epoch: 1814/2000 Train Loss: 0.4129 Train Acc: 0.8202\n",
      "Epoch: 1814/2000 Validation Loss: 0.5150 Validation Acc: 0.7430\n",
      "Epoch: 1816/2000 Train Loss: 0.4128 Train Acc: 0.8202\n",
      "Epoch: 1816/2000 Validation Loss: 0.5150 Validation Acc: 0.7430\n",
      "Epoch: 1819/2000 Train Loss: 0.4127 Train Acc: 0.8202\n",
      "Epoch: 1819/2000 Validation Loss: 0.5149 Validation Acc: 0.7430\n",
      "Epoch: 1821/2000 Train Loss: 0.4126 Train Acc: 0.8202\n",
      "Epoch: 1821/2000 Validation Loss: 0.5149 Validation Acc: 0.7430\n",
      "Epoch: 1823/2000 Train Loss: 0.4126 Train Acc: 0.8202\n",
      "Epoch: 1823/2000 Validation Loss: 0.5148 Validation Acc: 0.7430\n",
      "Epoch: 1825/2000 Train Loss: 0.4125 Train Acc: 0.8202\n",
      "Epoch: 1825/2000 Validation Loss: 0.5148 Validation Acc: 0.7430\n",
      "Epoch: 1828/2000 Train Loss: 0.4124 Train Acc: 0.8202\n",
      "Epoch: 1828/2000 Validation Loss: 0.5147 Validation Acc: 0.7430\n",
      "Epoch: 1830/2000 Train Loss: 0.4123 Train Acc: 0.8202\n",
      "Epoch: 1830/2000 Validation Loss: 0.5147 Validation Acc: 0.7430\n",
      "Epoch: 1832/2000 Train Loss: 0.4123 Train Acc: 0.8202\n",
      "Epoch: 1832/2000 Validation Loss: 0.5146 Validation Acc: 0.7430\n",
      "Epoch: 1835/2000 Train Loss: 0.4122 Train Acc: 0.8202\n",
      "Epoch: 1835/2000 Validation Loss: 0.5145 Validation Acc: 0.7430\n",
      "Epoch: 1837/2000 Train Loss: 0.4121 Train Acc: 0.8202\n",
      "Epoch: 1837/2000 Validation Loss: 0.5145 Validation Acc: 0.7430\n",
      "Epoch: 1839/2000 Train Loss: 0.4120 Train Acc: 0.8202\n",
      "Epoch: 1839/2000 Validation Loss: 0.5144 Validation Acc: 0.7430\n",
      "Epoch: 1841/2000 Train Loss: 0.4120 Train Acc: 0.8202\n",
      "Epoch: 1841/2000 Validation Loss: 0.5144 Validation Acc: 0.7430\n",
      "Epoch: 1844/2000 Train Loss: 0.4119 Train Acc: 0.8202\n",
      "Epoch: 1844/2000 Validation Loss: 0.5143 Validation Acc: 0.7430\n",
      "Epoch: 1846/2000 Train Loss: 0.4118 Train Acc: 0.8202\n",
      "Epoch: 1846/2000 Validation Loss: 0.5143 Validation Acc: 0.7430\n",
      "Epoch: 1848/2000 Train Loss: 0.4117 Train Acc: 0.8202\n",
      "Epoch: 1848/2000 Validation Loss: 0.5142 Validation Acc: 0.7430\n",
      "Epoch: 1850/2000 Train Loss: 0.4117 Train Acc: 0.8202\n",
      "Epoch: 1850/2000 Validation Loss: 0.5142 Validation Acc: 0.7430\n",
      "Epoch: 1853/2000 Train Loss: 0.4116 Train Acc: 0.8202\n",
      "Epoch: 1853/2000 Validation Loss: 0.5141 Validation Acc: 0.7430\n",
      "Epoch: 1855/2000 Train Loss: 0.4115 Train Acc: 0.8202\n",
      "Epoch: 1855/2000 Validation Loss: 0.5141 Validation Acc: 0.7430\n",
      "Epoch: 1857/2000 Train Loss: 0.4114 Train Acc: 0.8202\n",
      "Epoch: 1857/2000 Validation Loss: 0.5140 Validation Acc: 0.7430\n",
      "Epoch: 1860/2000 Train Loss: 0.4114 Train Acc: 0.8202\n",
      "Epoch: 1860/2000 Validation Loss: 0.5140 Validation Acc: 0.7430\n",
      "Epoch: 1862/2000 Train Loss: 0.4113 Train Acc: 0.8202\n",
      "Epoch: 1862/2000 Validation Loss: 0.5139 Validation Acc: 0.7430\n",
      "Epoch: 1864/2000 Train Loss: 0.4112 Train Acc: 0.8202\n",
      "Epoch: 1864/2000 Validation Loss: 0.5139 Validation Acc: 0.7430\n",
      "Epoch: 1866/2000 Train Loss: 0.4112 Train Acc: 0.8202\n",
      "Epoch: 1866/2000 Validation Loss: 0.5138 Validation Acc: 0.7430\n",
      "Epoch: 1869/2000 Train Loss: 0.4111 Train Acc: 0.8202\n",
      "Epoch: 1869/2000 Validation Loss: 0.5138 Validation Acc: 0.7430\n",
      "Epoch: 1871/2000 Train Loss: 0.4110 Train Acc: 0.8202\n",
      "Epoch: 1871/2000 Validation Loss: 0.5137 Validation Acc: 0.7430\n",
      "Epoch: 1873/2000 Train Loss: 0.4109 Train Acc: 0.8202\n",
      "Epoch: 1873/2000 Validation Loss: 0.5137 Validation Acc: 0.7430\n",
      "Epoch: 1875/2000 Train Loss: 0.4109 Train Acc: 0.8202\n",
      "Epoch: 1875/2000 Validation Loss: 0.5136 Validation Acc: 0.7430\n",
      "Epoch: 1878/2000 Train Loss: 0.4108 Train Acc: 0.8202\n",
      "Epoch: 1878/2000 Validation Loss: 0.5136 Validation Acc: 0.7430\n",
      "Epoch: 1880/2000 Train Loss: 0.4107 Train Acc: 0.8202\n",
      "Epoch: 1880/2000 Validation Loss: 0.5135 Validation Acc: 0.7430\n",
      "Epoch: 1882/2000 Train Loss: 0.4106 Train Acc: 0.8202\n",
      "Epoch: 1882/2000 Validation Loss: 0.5135 Validation Acc: 0.7430\n",
      "Epoch: 1885/2000 Train Loss: 0.4106 Train Acc: 0.8202\n",
      "Epoch: 1885/2000 Validation Loss: 0.5134 Validation Acc: 0.7430\n",
      "Epoch: 1887/2000 Train Loss: 0.4105 Train Acc: 0.8202\n",
      "Epoch: 1887/2000 Validation Loss: 0.5134 Validation Acc: 0.7430\n",
      "Epoch: 1889/2000 Train Loss: 0.4104 Train Acc: 0.8202\n",
      "Epoch: 1889/2000 Validation Loss: 0.5133 Validation Acc: 0.7430\n",
      "Epoch: 1891/2000 Train Loss: 0.4104 Train Acc: 0.8202\n",
      "Epoch: 1891/2000 Validation Loss: 0.5133 Validation Acc: 0.7430\n",
      "Epoch: 1894/2000 Train Loss: 0.4103 Train Acc: 0.8202\n",
      "Epoch: 1894/2000 Validation Loss: 0.5132 Validation Acc: 0.7430\n",
      "Epoch: 1896/2000 Train Loss: 0.4102 Train Acc: 0.8202\n",
      "Epoch: 1896/2000 Validation Loss: 0.5132 Validation Acc: 0.7430\n",
      "Epoch: 1898/2000 Train Loss: 0.4101 Train Acc: 0.8202\n",
      "Epoch: 1898/2000 Validation Loss: 0.5131 Validation Acc: 0.7430\n",
      "Epoch: 1900/2000 Train Loss: 0.4101 Train Acc: 0.8202\n",
      "Epoch: 1900/2000 Validation Loss: 0.5131 Validation Acc: 0.7430\n",
      "Epoch: 1903/2000 Train Loss: 0.4100 Train Acc: 0.8202\n",
      "Epoch: 1903/2000 Validation Loss: 0.5130 Validation Acc: 0.7430\n",
      "Epoch: 1905/2000 Train Loss: 0.4099 Train Acc: 0.8202\n",
      "Epoch: 1905/2000 Validation Loss: 0.5130 Validation Acc: 0.7430\n",
      "Epoch: 1907/2000 Train Loss: 0.4099 Train Acc: 0.8202\n",
      "Epoch: 1907/2000 Validation Loss: 0.5129 Validation Acc: 0.7430\n",
      "Epoch: 1910/2000 Train Loss: 0.4098 Train Acc: 0.8202\n",
      "Epoch: 1910/2000 Validation Loss: 0.5129 Validation Acc: 0.7430\n",
      "Epoch: 1912/2000 Train Loss: 0.4097 Train Acc: 0.8202\n",
      "Epoch: 1912/2000 Validation Loss: 0.5128 Validation Acc: 0.7430\n",
      "Epoch: 1914/2000 Train Loss: 0.4097 Train Acc: 0.8202\n",
      "Epoch: 1914/2000 Validation Loss: 0.5128 Validation Acc: 0.7430\n",
      "Epoch: 1916/2000 Train Loss: 0.4096 Train Acc: 0.8202\n",
      "Epoch: 1916/2000 Validation Loss: 0.5127 Validation Acc: 0.7430\n",
      "Epoch: 1919/2000 Train Loss: 0.4095 Train Acc: 0.8202\n",
      "Epoch: 1919/2000 Validation Loss: 0.5127 Validation Acc: 0.7430\n",
      "Epoch: 1921/2000 Train Loss: 0.4094 Train Acc: 0.8202\n",
      "Epoch: 1921/2000 Validation Loss: 0.5127 Validation Acc: 0.7430\n",
      "Epoch: 1923/2000 Train Loss: 0.4094 Train Acc: 0.8202\n",
      "Epoch: 1923/2000 Validation Loss: 0.5126 Validation Acc: 0.7430\n",
      "Epoch: 1925/2000 Train Loss: 0.4093 Train Acc: 0.8202\n",
      "Epoch: 1925/2000 Validation Loss: 0.5126 Validation Acc: 0.7430\n",
      "Epoch: 1928/2000 Train Loss: 0.4092 Train Acc: 0.8202\n",
      "Epoch: 1928/2000 Validation Loss: 0.5125 Validation Acc: 0.7430\n",
      "Epoch: 1930/2000 Train Loss: 0.4092 Train Acc: 0.8202\n",
      "Epoch: 1930/2000 Validation Loss: 0.5125 Validation Acc: 0.7430\n",
      "Epoch: 1932/2000 Train Loss: 0.4091 Train Acc: 0.8202\n",
      "Epoch: 1932/2000 Validation Loss: 0.5124 Validation Acc: 0.7430\n",
      "Epoch: 1935/2000 Train Loss: 0.4090 Train Acc: 0.8202\n",
      "Epoch: 1935/2000 Validation Loss: 0.5124 Validation Acc: 0.7430\n",
      "Epoch: 1937/2000 Train Loss: 0.4090 Train Acc: 0.8202\n",
      "Epoch: 1937/2000 Validation Loss: 0.5123 Validation Acc: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1939/2000 Train Loss: 0.4089 Train Acc: 0.8202\n",
      "Epoch: 1939/2000 Validation Loss: 0.5123 Validation Acc: 0.7430\n",
      "Epoch: 1941/2000 Train Loss: 0.4088 Train Acc: 0.8202\n",
      "Epoch: 1941/2000 Validation Loss: 0.5122 Validation Acc: 0.7430\n",
      "Epoch: 1944/2000 Train Loss: 0.4088 Train Acc: 0.8202\n",
      "Epoch: 1944/2000 Validation Loss: 0.5122 Validation Acc: 0.7430\n",
      "Epoch: 1946/2000 Train Loss: 0.4087 Train Acc: 0.8202\n",
      "Epoch: 1946/2000 Validation Loss: 0.5121 Validation Acc: 0.7430\n",
      "Epoch: 1948/2000 Train Loss: 0.4086 Train Acc: 0.8202\n",
      "Epoch: 1948/2000 Validation Loss: 0.5121 Validation Acc: 0.7430\n",
      "Epoch: 1950/2000 Train Loss: 0.4086 Train Acc: 0.8202\n",
      "Epoch: 1950/2000 Validation Loss: 0.5120 Validation Acc: 0.7430\n",
      "Epoch: 1953/2000 Train Loss: 0.4085 Train Acc: 0.8202\n",
      "Epoch: 1953/2000 Validation Loss: 0.5120 Validation Acc: 0.7430\n",
      "Epoch: 1955/2000 Train Loss: 0.4084 Train Acc: 0.8202\n",
      "Epoch: 1955/2000 Validation Loss: 0.5119 Validation Acc: 0.7430\n",
      "Epoch: 1957/2000 Train Loss: 0.4083 Train Acc: 0.8202\n",
      "Epoch: 1957/2000 Validation Loss: 0.5119 Validation Acc: 0.7430\n",
      "Epoch: 1960/2000 Train Loss: 0.4083 Train Acc: 0.8202\n",
      "Epoch: 1960/2000 Validation Loss: 0.5119 Validation Acc: 0.7430\n",
      "Epoch: 1962/2000 Train Loss: 0.4082 Train Acc: 0.8202\n",
      "Epoch: 1962/2000 Validation Loss: 0.5118 Validation Acc: 0.7430\n",
      "Epoch: 1964/2000 Train Loss: 0.4081 Train Acc: 0.8202\n",
      "Epoch: 1964/2000 Validation Loss: 0.5118 Validation Acc: 0.7430\n",
      "Epoch: 1966/2000 Train Loss: 0.4081 Train Acc: 0.8202\n",
      "Epoch: 1966/2000 Validation Loss: 0.5117 Validation Acc: 0.7430\n",
      "Epoch: 1969/2000 Train Loss: 0.4080 Train Acc: 0.8202\n",
      "Epoch: 1969/2000 Validation Loss: 0.5117 Validation Acc: 0.7430\n",
      "Epoch: 1971/2000 Train Loss: 0.4079 Train Acc: 0.8202\n",
      "Epoch: 1971/2000 Validation Loss: 0.5116 Validation Acc: 0.7430\n",
      "Epoch: 1973/2000 Train Loss: 0.4079 Train Acc: 0.8202\n",
      "Epoch: 1973/2000 Validation Loss: 0.5116 Validation Acc: 0.7430\n",
      "Epoch: 1975/2000 Train Loss: 0.4078 Train Acc: 0.8202\n",
      "Epoch: 1975/2000 Validation Loss: 0.5115 Validation Acc: 0.7486\n",
      "Epoch: 1978/2000 Train Loss: 0.4077 Train Acc: 0.8202\n",
      "Epoch: 1978/2000 Validation Loss: 0.5115 Validation Acc: 0.7486\n",
      "Epoch: 1980/2000 Train Loss: 0.4077 Train Acc: 0.8202\n",
      "Epoch: 1980/2000 Validation Loss: 0.5115 Validation Acc: 0.7486\n",
      "Epoch: 1982/2000 Train Loss: 0.4076 Train Acc: 0.8202\n",
      "Epoch: 1982/2000 Validation Loss: 0.5114 Validation Acc: 0.7486\n",
      "Epoch: 1985/2000 Train Loss: 0.4076 Train Acc: 0.8202\n",
      "Epoch: 1985/2000 Validation Loss: 0.5114 Validation Acc: 0.7486\n",
      "Epoch: 1987/2000 Train Loss: 0.4075 Train Acc: 0.8202\n",
      "Epoch: 1987/2000 Validation Loss: 0.5113 Validation Acc: 0.7486\n",
      "Epoch: 1989/2000 Train Loss: 0.4074 Train Acc: 0.8202\n",
      "Epoch: 1989/2000 Validation Loss: 0.5113 Validation Acc: 0.7486\n",
      "Epoch: 1991/2000 Train Loss: 0.4074 Train Acc: 0.8202\n",
      "Epoch: 1991/2000 Validation Loss: 0.5112 Validation Acc: 0.7486\n",
      "Epoch: 1994/2000 Train Loss: 0.4073 Train Acc: 0.8202\n",
      "Epoch: 1994/2000 Validation Loss: 0.5112 Validation Acc: 0.7486\n",
      "Epoch: 1996/2000 Train Loss: 0.4072 Train Acc: 0.8202\n",
      "Epoch: 1996/2000 Validation Loss: 0.5111 Validation Acc: 0.7486\n",
      "Epoch: 1998/2000 Train Loss: 0.4072 Train Acc: 0.8202\n",
      "Epoch: 1998/2000 Validation Loss: 0.5111 Validation Acc: 0.7486\n",
      "Epoch: 2000/2000 Train Loss: 0.4071 Train Acc: 0.8202\n",
      "Epoch: 2000/2000 Validation Loss: 0.5111 Validation Acc: 0.7486\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "hidden_units = 10\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "is_training = tf.Variable(True, dtype=tf.bool)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochs = 2000\n",
    "train_collect = 50\n",
    "train_print=train_collect*2\n",
    "\n",
    "learning_rate_value = 0.0001\n",
    "batch_size=16\n",
    "\n",
    "x_collect = []\n",
    "train_loss_collect = []\n",
    "train_acc_collect = []\n",
    "valid_loss_collect = []\n",
    "valid_acc_collect = []\n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "fc = tf.layers.dense(inputs, hidden_units, activation=None, kernel_initializer=initializer)\n",
    "fc = tf.layers.batch_normalization(fc, training=is_training)\n",
    "fc = tf.nn.relu(fc)\n",
    "    \n",
    "logits = tf.layers.dense(fc, 1, activation=None)\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.sigmoid(logits)\n",
    "correct_pred = tf.equal(tf.round(predicted), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration=0\n",
    "    for e in range(epochs):\n",
    "        for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n",
    "            iteration+=1\n",
    "            feed = {inputs: train_x,\n",
    "                    labels: train_y,\n",
    "                    learning_rate: learning_rate_value,\n",
    "                    is_training:True\n",
    "                   }\n",
    "\n",
    "            train_loss, _, train_acc = sess.run([cost, optimizer, accuracy], feed_dict=feed)\n",
    "            \n",
    "            if iteration % train_collect == 0:\n",
    "                x_collect.append(e)\n",
    "                train_loss_collect.append(train_loss)\n",
    "                train_acc_collect.append(train_acc)\n",
    "\n",
    "                if iteration % train_print==0:\n",
    "                     print(\"Epoch: {}/{}\".format(e + 1, epochs),\n",
    "                      \"Train Loss: {:.4f}\".format(train_loss),\n",
    "                      \"Train Acc: {:.4f}\".format(train_acc))\n",
    "                        \n",
    "                feed = {inputs: valid_x,\n",
    "                        labels: valid_y,\n",
    "                        is_training:False\n",
    "                       }\n",
    "                val_loss, val_acc = sess.run([cost, accuracy], feed_dict=feed)\n",
    "                valid_loss_collect.append(val_loss)\n",
    "                valid_acc_collect.append(val_acc)\n",
    "                \n",
    "                if iteration % train_print==0:\n",
    "                    print(\"Epoch: {}/{}\".format(e + 1, epochs),\n",
    "                      \"Validation Loss: {:.4f}\".format(val_loss),\n",
    "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
    "                \n",
    "\n",
    "    saver.save(sess, \"./titanic.ckpt\")\n",
    "    feed={inputs:test_data, is_training:False}\n",
    "    test_predict=sess.run(predicted,feed_dict=feed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restorer=tf.train.Saver()\n",
    "# with tf.Session() as sess:\n",
    "#     restorer.restore(sess,\"./titanic.ckpt\")\n",
    "#     feed={\n",
    "#         inputs:test_data,\n",
    "#         is_training:False\n",
    "#     }\n",
    "#     test_predict=sess.run(predicted,feed_dict=feed)\n",
    "    \n",
    "# test_predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_collect, train_loss_collect, \"r--\")\n",
    "plt.plot(x_collect, valid_loss_collect, \"g^\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_collect, train_acc_collect, \"r--\")\n",
    "plt.plot(x_collect, valid_acc_collect, \"g^\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer=Binarizer(0.5)\n",
    "test_predict_result=binarizer.fit_transform(test_predict)\n",
    "test_predict_result=test_predict_result.astype(np.int32)\n",
    "test_predict_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         1\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1\n",
       "5          897         0\n",
       "6          898         0\n",
       "7          899         0\n",
       "8          900         1\n",
       "9          901         0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_id=test_passenger_id.copy()\n",
    "evaluation=passenger_id.to_frame()\n",
    "evaluation[\"Survived\"]=test_predict_result\n",
    "evaluation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: 0.77990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
